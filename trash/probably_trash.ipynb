{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaxonomyAnalyzer:\n",
    "    def __init__(self, max_retries: int = 3, retry_delay: int = 1):\n",
    "        self.max_retries = max_retries\n",
    "        self.retry_delay = retry_delay\n",
    "        self.uniprot_base_url = \"https://rest.uniprot.org/uniprotkb/\"\n",
    "        \n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "            filename=\"Final_taxonomy_analysis.log\"\n",
    "        )\n",
    "\n",
    "\n",
    "    def fetch_taxonomy_info(self, protein_ids: List[str], output_file: str) -> str:\n",
    "        \"\"\"\n",
    "        Fetch taxonomy information \n",
    "        \"\"\"\n",
    "        taxonomy_data = []\n",
    "        error_counts = {\"success\": 0, \"failed\": 0}\n",
    "\n",
    "        for protein_id in tqdm(protein_ids, desc=\"Fetching taxonomy info\"):\n",
    "            for attempt in range(self.max_retries):\n",
    "                try:\n",
    "                    response = requests.get(f\"{self.uniprot_base_url}{protein_id}.json\")\n",
    "                    response.raise_for_status()\n",
    "                    data = response.json()\n",
    "\n",
    "                    taxonomy = data.get(\"organism\", {})\n",
    "                    scientific_name = taxonomy.get(\"scientificName\", \"N/A\")\n",
    "                    lineage = taxonomy.get(\"lineage\", [])\n",
    "                    taxonomy_data.append([protein_id, scientific_name, \" > \".join(lineage)])\n",
    "\n",
    "                    error_counts[\"success\"] += 1\n",
    "                    break\n",
    "                \n",
    "                except requests.exceptions.RequestException as e:\n",
    "                    if attempt == self.max_retries - 1:\n",
    "                        logging.error(f\"Failed to fetch {protein_id} after {self.max_retries} attempts: {str(e)}\")\n",
    "                        taxonomy_data.append([protein_id, \"Error\", \"\"])\n",
    "                        error_counts[\"failed\"] += 1\n",
    "                    else:\n",
    "                        time.sleep(self.retry_delay)\n",
    "                        \n",
    "        with open(output_file, \"w\", newline=\"\") as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\"Protein ID\", \"Scientific Name\", \"Lineage\"])\n",
    "            writer.writerows(taxonomy_data)\n",
    "        logging.info(f\"Taxonomy data saved to {output_file}\")\n",
    "\n",
    "        return output_file\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhyloTreeVisualizer:\n",
    "    def create_newick_string(self, taxonomy_df):\n",
    "        \"\"\"Convert taxonomy data to Newick format\"\"\"\n",
    "        lineage_counts = {}\n",
    "        for _, row in taxonomy_df.iterrows():\n",
    "            if isinstance(row['Lineage'], str):\n",
    "                taxa = row['Lineage'].split(' > ')\n",
    "                for i in range(len(taxa)):\n",
    "                    lineage = ' > '.join(taxa[:i+1])\n",
    "                    lineage_counts[lineage] = lineage_counts.get(lineage, 0) + 1\n",
    "\n",
    "        def build_newick(taxa, parent=''):\n",
    "            current = taxa[-1] if taxa else ''\n",
    "            current_path = ' > '.join(taxa)\n",
    "            count = lineage_counts.get(current_path, 1)\n",
    "            \n",
    "            children = []\n",
    "            for lineage in lineage_counts.keys():\n",
    "                if lineage.startswith(current_path + ' > '):\n",
    "                    next_level = lineage.split(' > ')[len(taxa)]\n",
    "                    if next_level not in children:\n",
    "                        children.append(next_level)\n",
    "            \n",
    "            if children:\n",
    "                child_strings = [build_newick(taxa + [child]) for child in children]\n",
    "                return f\"({','.join(child_strings)}){current}:{count}\"\n",
    "            else:\n",
    "                return f\"{current}:{count}\"\n",
    "\n",
    "        root_taxa = set()\n",
    "        for lineage in lineage_counts.keys():\n",
    "            root = lineage.split(' > ')[0]\n",
    "            if root not in root_taxa:\n",
    "                root_taxa.add(root)\n",
    "        \n",
    "        newick = f\"({','.join(build_newick([taxa]) for taxa in root_taxa)});\"\n",
    "        return newick\n",
    "\n",
    "    def create_phylogenetic_tree(self, taxonomy_file, output_file):\n",
    "        \"\"\"Create and save a phylogenetic tree visualization\"\"\"\n",
    "        # Read taxonomy data\n",
    "        df = pd.read_csv(taxonomy_file)\n",
    "        \n",
    "        # Create Newick string\n",
    "        newick_str = self.create_newick_string(df)\n",
    "        \n",
    "        # Parse tree from Newick format\n",
    "        handle = StringIO(newick_str)\n",
    "        tree = Phylo.read(handle, \"newick\")\n",
    "\n",
    "        # Check with ASCII representation\n",
    "        print(\"ASCII representation of the tree:\")\n",
    "        Phylo.draw_ascii(tree)\n",
    "        \n",
    "        # Set up the plot with larger figure size and adjusted dimensions\n",
    "        fig = plt.figure(figsize=(20, 30))  # Increased figure size\n",
    "        \n",
    "        # Draw the tree with customized parameters\n",
    "        axes = fig.add_subplot(1, 1, 1)\n",
    "        Phylo.draw(tree, \n",
    "                  axes=axes,\n",
    "                  do_show=False,\n",
    "                  branch_labels=lambda c: str(int(c.branch_length)) if c.branch_length else '')\n",
    "        \n",
    "        # Adjust the plot\n",
    "        axes.set_title(\"Taxonomic Tree with Branch Lengths Showing Relative Abundance\", pad=20, size=16)\n",
    "        axes.set_xlabel(\"Relative Abundance\", size=12)\n",
    "        \n",
    "        # Increase spacing between taxa\n",
    "        axes.set_xticks(axes.get_xticks())\n",
    "        axes.set_yticks(axes.get_yticks())\n",
    "        \n",
    "        # Adjust label sizes and spacing\n",
    "        plt.setp(axes.get_xticklabels(), fontsize=10)\n",
    "        plt.setp(axes.get_yticklabels(), fontsize=10, style='italic')\n",
    "        \n",
    "        # Adjust layout to prevent label cutoff\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the plot with high resolution\n",
    "        plt.savefig(output_file, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        return output_file\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_phylogenetic_tree(taxonomy_file, output_file):\n",
    "    visualizer = PhyloTreeVisualizer()\n",
    "    return visualizer.create_phylogenetic_tree(taxonomy_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # First fetch taxonomy info as before\n",
    "    analyzer = TaxonomyAnalyzer()\n",
    "    taxonomy_file = analyzer.fetch_taxonomy_info(family_sequences, \"Final_taxonomy_info.csv\")\n",
    "    \n",
    "    # Create the phylogenetic tree\n",
    "    tree_file = visualize_phylogenetic_tree(taxonomy_file, \"Final_phylogenetic_tree.png\")\n",
    "    \n",
    "    print(\"\\nFiles created:\")\n",
    "    print(f\"Taxonomy data: {taxonomy_file}\")\n",
    "    print(f\"Phylogenetic tree: {tree_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "from scipy.stats import fisher_exact\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import obonet\n",
    "import networkx as nx\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "# Step 1: Load Protein IDs\n",
    "# TODO : we basically did this above already for taxonomy task and just here neatly written into a function, so we could maybe just do it once in the whole code later on\n",
    "def load_protein_ids(psiblast_file, hmm_file, e_threshold=0.001):\n",
    "    \"\"\"Load protein IDs from PSI-BLAST and HMM search results.\"\"\"\n",
    "    psiblast_df = pd.read_csv(psiblast_file)\n",
    "    hmm_df = pd.read_csv(hmm_file)\n",
    "    \n",
    "    filtered_hmm_proteins = hmm_df[hmm_df['E-value'] <= e_threshold]['uniprot_id']\n",
    "    psiblast_proteins = set(psiblast_df['uniprot_id'])\n",
    "    hmm_proteins = set(filtered_hmm_proteins)\n",
    "    \n",
    "    return list(psiblast_proteins.union(hmm_proteins))\n",
    "\n",
    "\n",
    "def fetch_go_annotations(protein_id):\n",
    "    \"\"\"\n",
    "    Fetch and categorize GO annotations for a given protein ID from the UniProt API.\n",
    "    \n",
    "    Args:\n",
    "        protein_id (str): The UniProt ID of the protein\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary containing:\n",
    "            - Categorized GO terms separated by molecular function, biological process, \n",
    "              and cellular component (new format)\n",
    "    \"\"\"\n",
    "    # Define the UniProt API URL for XML data\n",
    "    url = f\"https://rest.uniprot.org/uniprotkb/{protein_id}.xml\"\n",
    "    \n",
    "    try:\n",
    "        # Fetch the XML data from UniProt\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Initialize our data structures\n",
    "        go_terms = []  # Original format\n",
    "        categorized_terms = {\n",
    "            'molecular_function': [],\n",
    "            'biological_process': [],\n",
    "            'cellular_component': []\n",
    "        }\n",
    "        \n",
    "        # Set up namespace for XML parsing\n",
    "        namespaces = {'ns': 'http://uniprot.org/uniprot'}\n",
    "        root = ET.fromstring(response.content)\n",
    "        \n",
    "        # Find all GO term references in the XML\n",
    "        for db_ref in root.findall(\".//ns:dbReference[@type='GO']\", namespaces):\n",
    "            go_id = db_ref.attrib.get('id')\n",
    "            term = db_ref.find(\"ns:property[@type='term']\", namespaces)\n",
    "            category = db_ref.find(\"ns:property[@type='category']\", namespaces)\n",
    "            \n",
    "            if go_id and term is not None:\n",
    "                # Store in original format\n",
    "                term_value = term.attrib['value']\n",
    "\n",
    "                \n",
    "                # Categorize based on prefix\n",
    "                if term_value.startswith('F:'):\n",
    "                    categorized_terms['molecular_function'].append({\n",
    "                        'id': go_id,\n",
    "                        'term': term_value[2:]  # Remove 'F:' prefix\n",
    "                    })\n",
    "                elif term_value.startswith('P:'):\n",
    "                    categorized_terms['biological_process'].append({\n",
    "                        'id': go_id,\n",
    "                        'term': term_value[2:]  # Remove 'P:' prefix\n",
    "                    })\n",
    "                elif term_value.startswith('C:'):\n",
    "                    categorized_terms['cellular_component'].append({\n",
    "                        'id': go_id,\n",
    "                        'term': term_value[2:]  # Remove 'C:' prefix\n",
    "                    })\n",
    "        \n",
    "        return {\n",
    "            'categorized': categorized_terms  # New categorized format\n",
    "        }\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching GO annotations for {protein_id}: {e}\")\n",
    "        return {\n",
    "            'categorized': {\n",
    "                'molecular_function': [],\n",
    "                'biological_process': [],\n",
    "                'cellular_component': []\n",
    "            }\n",
    "        }\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Step 3: Fetch Random Proteins\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def fetch_random_proteins(batch_size=100, total_proteins=500):\n",
    "    \"\"\"Fetch a list of random reviewed UniProt protein IDs.\"\"\"\n",
    "    url = \"https://rest.uniprot.org/uniprotkb/stream?query=reviewed:true&format=list\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        all_proteins = response.text.splitlines()\n",
    "        selected_proteins = random.sample(all_proteins, min(total_proteins, len(all_proteins)))\n",
    "        return [selected_proteins[i:i + batch_size] for i in range(0, len(selected_proteins), batch_size)]\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching random proteins: {e}\")\n",
    "        return []\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Step 4: Load GO Ontology\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def load_go_ontology():\n",
    "    \"\"\"Load the GO ontology from OBO file.\"\"\"\n",
    "    url = \"http://purl.obolibrary.org/obo/go/go-basic.obo\"\n",
    "    graph = obonet.read_obo(url)\n",
    "    return graph\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Step 5: Flatten Annotations for Analysis\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def flatten_annotations(annotation_dict):\n",
    "    \"\"\"Flatten GO annotations into a list of GO terms.\"\"\"\n",
    "    flat_terms = []\n",
    "    for annotations in annotation_dict.values():\n",
    "        flat_terms.extend([a[\"GO_ID\"] for a in annotations])\n",
    "    return flat_terms\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Step 6: Enrichment Analysis\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def calculate_enrichment(go_term, family_terms, background_terms):\n",
    "    \"\"\"Calculate enrichment of a GO term using Fisher's exact test.\"\"\"\n",
    "    family_count = family_terms.count(go_term)\n",
    "    family_not = len(family_terms) - family_count\n",
    "    background_count = background_terms.count(go_term)\n",
    "    background_not = len(background_terms) - background_count\n",
    "\n",
    "    contingency_table = [[family_count, background_count],\n",
    "                         [family_not, background_not]]\n",
    "    _, p_value = fisher_exact(contingency_table, alternative='greater')\n",
    "    return p_value\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Step 7: Visualize Enriched Terms\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def plot_wordcloud(enrichment_results, annotations):\n",
    "    \"\"\"Generate and save a word cloud of enriched GO terms.\"\"\"\n",
    "    term_names = {a[\"GO_ID\"]: a[\"Term\"] for ann_list in annotations.values() for a in ann_list}\n",
    "    enriched_with_names = {term_names[go_id]: -np.log10(p) for go_id, p in enrichment_results.items() if go_id in term_names}\n",
    "\n",
    "    if not enriched_with_names:\n",
    "        print(\"No enriched terms found. Word cloud will not be generated.\")\n",
    "        return\n",
    "\n",
    "    wordcloud = WordCloud(width=1000, height=600, background_color=\"white\", colormap=\"viridis\").generate_from_frequencies(enriched_with_names)\n",
    "    wordcloud.to_file(\"enriched_terms_wordcloud.png\")\n",
    "    print(\"Word cloud saved as enriched_terms_wordcloud.png\")\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Word Cloud of Enriched GO Terms\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_branch_enrichment(enrichment_results, go_graph):\n",
    "    \"\"\"Plot GO branch enrichment.\"\"\"\n",
    "    branch_scores = {}\n",
    "    for go_id, p_value in enrichment_results.items():\n",
    "        try:\n",
    "            parents = nx.ancestors(go_graph, go_id)\n",
    "            for parent in parents:\n",
    "                if parent not in branch_scores:\n",
    "                    branch_scores[parent] = []\n",
    "                branch_scores[parent].append(p_value)\n",
    "        except nx.NetworkXError:\n",
    "            continue\n",
    "\n",
    "    significant_branches = {branch: np.mean(scores)\n",
    "                            for branch, scores in branch_scores.items() if len(scores) >= 3}\n",
    "\n",
    "    # Limit to top 50 branches by score\n",
    "    sorted_branches = sorted(significant_branches.items(), key=lambda x: x[1], reverse=True)[:50]\n",
    "    branches, scores = zip(*sorted_branches)\n",
    "\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    plt.barh(range(len(branches)), scores, color=\"steelblue\")\n",
    "    plt.yticks(range(len(branches)), [go_graph.nodes[branch]['name'] for branch in branches], fontsize=8)\n",
    "    plt.xlabel('Mean p-value')\n",
    "    plt.title('Top 50 GO Branch Enrichments')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"go_enrichment_branches.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.legend([\"Top 50 Branch Enrichments\"], loc=\"lower right\")\n",
    "    print(\"GO branch enrichment plot saved as go_enrichment_branches.png\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Step 8: Write Summary and Results to File\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def write_summary_and_results(enrichment_results, family_annotations, go_graph):\n",
    "    \"\"\"Write a summary and detailed results to a text file.\"\"\"\n",
    "    with open(\"enrichment_results.txt\", \"w\") as f:\n",
    "        # Write summary\n",
    "        f.write(\"SUMMARY\\n\")\n",
    "        f.write(\"========\\n\")\n",
    "        f.write(f\"Number of enriched GO terms: {len(enrichment_results)}\\n\")\n",
    "        f.write(f\"Top enriched term: {max(enrichment_results, key=enrichment_results.get, default='None')}\\n\")\n",
    "        f.write(\"\\n\\n\")\n",
    "\n",
    "        # Write detailed results\n",
    "        f.write(\"DETAILED RESULTS\\n\")\n",
    "        f.write(\"================\\n\")\n",
    "        for go_id, p_value in enrichment_results.items():\n",
    "            term_name = next((a[\"Term\"] for ann_list in family_annotations.values() \n",
    "                             for a in ann_list if a[\"GO_ID\"] == go_id), go_id)\n",
    "            f.write(f\"{go_id}: {term_name} (p-value: {p_value:.2e})\\n\")\n",
    "\n",
    "        # Write branch scores\n",
    "        f.write(\"\\n\\nSIGNIFICANT GO BRANCHES\\n\")\n",
    "        f.write(\"========================\\n\")\n",
    "        branch_scores = {}\n",
    "        for go_id, p_value in enrichment_results.items():\n",
    "            try:\n",
    "                parents = nx.ancestors(go_graph, go_id)\n",
    "                for parent in parents:\n",
    "                    if parent not in branch_scores:\n",
    "                        branch_scores[parent] = []\n",
    "                    branch_scores[parent].append(p_value)\n",
    "            except nx.NetworkXError:\n",
    "                continue\n",
    "\n",
    "        significant_branches = {branch: np.mean(scores) \n",
    "                                for branch, scores in branch_scores.items() if len(scores) >= 3}\n",
    "        for branch, score in sorted(significant_branches.items(), key=lambda x: x[1]):\n",
    "            branch_name = go_graph.nodes[branch].get('name', branch)\n",
    "            f.write(f\"{branch}: {branch_name} (mean p-value: {score:.2e})\\n\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Main Script\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def main():\n",
    "    print(\"Loading GO ontology...\")\n",
    "    go_graph = load_go_ontology()\n",
    "\n",
    "    psiblast_file = \"psiblast_parsed.csv\"\n",
    "    hmm_file = \"hmmsearch_output.csv\"\n",
    "    protein_ids = load_protein_ids(psiblast_file, hmm_file)\n",
    "\n",
    "    print(\"Fetching GO annotations...\")\n",
    "    family_annotations = {}\n",
    "    for pid in tqdm(protein_ids, desc=\"Fetching GO annotations\"):\n",
    "        family_annotations[pid] = fetch_go_annotations(pid)\n",
    "\n",
    "\n",
    "    print(family_annotations)\n",
    "\n",
    "\n",
    "    print(\"Fetching background annotations...\")\n",
    "    background_annotations = {}\n",
    "    background_batches = fetch_random_proteins(batch_size=50, total_proteins=500)\n",
    "    for batch in tqdm(background_batches, desc=\"Processing background proteins\"):\n",
    "        for pid in batch:\n",
    "            background_annotations[pid] = fetch_go_annotations(pid)\n",
    "\n",
    "    print(\"Calculating enrichment...\")\n",
    "    family_terms = flatten_annotations(family_annotations)\n",
    "    background_terms = flatten_annotations(background_annotations)\n",
    "\n",
    "    unique_go_terms = set(family_terms)\n",
    "    enrichment_results = {}\n",
    "    pvalues = []\n",
    "    terms = []\n",
    "\n",
    "    for term in unique_go_terms:\n",
    "        _, p_value = fisher_exact([\n",
    "            [family_terms.count(term), len(family_terms) - family_terms.count(term)],\n",
    "            [background_terms.count(term), len(background_terms) - background_terms.count(term)]\n",
    "        ], alternative='greater')\n",
    "\n",
    "        pvalues.append(p_value)\n",
    "        terms.append(term)\n",
    "\n",
    "    rejected, p_corrected, _, _ = multipletests(pvalues, method='fdr_bh')\n",
    "\n",
    "    for term, p_value, significant in zip(terms, p_corrected, rejected):\n",
    "        if significant:\n",
    "            enrichment_results[term] = p_value\n",
    "\n",
    "    print(\"Generating visualizations...\")\n",
    "    plot_wordcloud(enrichment_results, family_annotations)\n",
    "    plot_branch_enrichment(enrichment_results, go_graph)\n",
    "\n",
    "    print(\"Writing results to file...\")\n",
    "    write_summary_and_results(enrichment_results, family_annotations, go_graph)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    '''\n",
    "    # Proteins_to_GO terms for our family \n",
    "    print(\"Fetching GO annotations...\")\n",
    "    family_annotations = {}\n",
    "    for pid in tqdm(protein_ids, desc=\"Fetching GO annotations\"):\n",
    "        family_annotations[pid] = fetch_go_annotations(pid)\n",
    "\n",
    "    total_proteins_family = len(family_annotations)\n",
    "\n",
    "    go_id_to_go_term = fetch_go_terms(protein_ids)\n",
    "\n",
    "    \n",
    "    # Proteins_to_GO terms for SwissProt\n",
    "    # Note that we downloaded \"uniprot_sprot.xml\" locally but not onto the GitHub due to its size\n",
    "    swissprot_annotations = parse_swissprot_go_terms(\"uniprot_sprot.xml\", protein_ids) #go_counts_swissprot, num_proteins_swissprot\n",
    "    \n",
    "    total_proteins_swissprot = len(swissprot_annotations)\n",
    "    \n",
    "\n",
    "\n",
    "    # Now Map the GO terms to the proteins ; for the enrichment task, we need to know how many proteins have a certain GO term\n",
    "    go_to_proteins_swissprot = reverse_protein_go_dict(swissprot_annotations)\n",
    "    go_to_proteins_family = reverse_protein_go_dict(family_annotations)\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    # Calculate GO enrichments for both with skipped proteins and without \n",
    "    _ = calculate_go_enrichment(go_to_proteins_family, go_to_proteins_swissprot, total_proteins_family, total_proteins_swissprot, go_id_to_go_term)\n",
    "    \n",
    "    '''å‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "    '''\n",
    "    def calculate_conservation_score(self, pos):\n",
    "        \"\"\"\n",
    "        Calculate conservation score based on frequency of most common amino acid\n",
    "        Ignores gaps in calculation\n",
    "        \"\"\"\n",
    "        freqs = self.calculate_amino_acid_frequencies(pos)\n",
    "        if not freqs:\n",
    "            return 0\n",
    "        return max(freqs.values())\n",
    "    '''\n",
    "\n",
    "\n",
    "    '''\n",
    "    def calculate_group_conservation(self, pos):\n",
    "        \"\"\"\n",
    "        Calculate conservation considering amino acid groups\n",
    "        Basically the same as calculate_conversation_score, just that it calculates based on the groups, not single amino acids !\n",
    "        \"\"\"\n",
    "        column = self.get_column(pos)\n",
    "        groups = self.get_amino_acid_groups()\n",
    "        \n",
    "        # Assign each amino acid to its group\n",
    "        aa_to_group = {}\n",
    "        for group_name, aas in groups.items():\n",
    "            for aa in aas:\n",
    "                aa_to_group[aa] = group_name\n",
    "        \n",
    "        # Count group occurrences\n",
    "        group_counts = Counter(aa_to_group.get(aa, 'other') \n",
    "                             for aa in column if aa != '-')\n",
    "        \n",
    "        if not group_counts:\n",
    "            return 0\n",
    "            \n",
    "        return max(group_counts.values()) / sum(group_counts.values())\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"FOR HMM\"\"\"\n",
    "# File paths\n",
    "input_file_path = \"Model/Evaluation/Predictions/HMM-SEARCH/hmmsearch_output.txt\"\n",
    "output_file_path = \"Model/Evaluation/Predictions/HMM-SEARCH/hmmsearch_output.csv\"\n",
    "\n",
    "# Initialize storage for parsed data\n",
    "parsed_data = []\n",
    "\n",
    "# Regular expressions to capture key information\n",
    "header_regex = r\">> ([^\\s]+)\"\n",
    "domain_regex = r\"\\s+(\\d+) [!?]\\s+[\\d\\.]+\\s+[\\d\\.]+\\s+[\\de\\.\\+\\-]+\\s+([\\de\\.\\+\\-]+)\\s+\\d+\\s+\\d+\\s+(?:\\[\\.|\\.\\.)+\\s+(\\d+)\\s+(\\d+)\"\n",
    "\n",
    "with open(input_file_path, \"r\") as infile:\n",
    "    current_protein = None\n",
    "\n",
    "    for line in infile:\n",
    "        # Match protein header line\n",
    "        header_match = re.match(header_regex, line)\n",
    "        if header_match:\n",
    "            # If we already captured a protein, save its data\n",
    "            if current_protein:\n",
    "                parsed_data.append(current_protein)\n",
    "\n",
    "            # Start a new protein record\n",
    "            protein_id = header_match.groups()[0]\n",
    "            current_protein = {\n",
    "                \"protein_name\": protein_id.split(\"|\")[2],\n",
    "                \"uniprot_id\": protein_id.split(\"|\")[1],\n",
    "                \"domains\": []\n",
    "            }\n",
    "\n",
    "        # Match domain annotation (including both `!` and `?` lines)\n",
    "        domain_match = re.match(domain_regex, line)\n",
    "        if domain_match and current_protein:\n",
    "            _, score, start, end = domain_match.groups()\n",
    "            start, end, score = int(start), int(end), float(score)\n",
    "            length = end - start + 1\n",
    "            current_protein[\"domains\"].append((score, start, end, length))\n",
    "\n",
    "    # Handle the last protein record\n",
    "    if current_protein:\n",
    "        parsed_data.append(current_protein)\n",
    "\n",
    "# Prepare fieldnames dynamically\n",
    "fieldnames = [\"protein_name\", \"uniprot_id\"]\n",
    "max_domains = max(len(protein[\"domains\"]) for protein in parsed_data)\n",
    "for i in range(1, max_domains + 1):\n",
    "    if i == 1:\n",
    "        fieldnames.extend([\n",
    "            f\"E-value\", f\"domain_start\", f\"domain_end\", f\"domain_length\"\n",
    "        ])\n",
    "    else:\n",
    "        fieldnames.extend([\n",
    "        f\"domain_{i}_E-value\", f\"domain_{i}_start\", f\"domain_{i}_end\", f\"domain_{i}_length\"\n",
    "    ])\n",
    "\n",
    "# Write to CSV\n",
    "with open(output_file_path, \"w\", newline=\"\") as outfile:\n",
    "    writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for protein in parsed_data:\n",
    "        row = {\n",
    "            \"protein_name\": protein[\"protein_name\"],\n",
    "            \"uniprot_id\": protein[\"uniprot_id\"]\n",
    "        }\n",
    "        for i, domain in enumerate(protein[\"domains\"], start=1):\n",
    "            if i == 1:\n",
    "                row[f\"E-value\"] = domain[0]\n",
    "                row[f\"domain_start\"] = domain[1]\n",
    "                row[f\"domain_end\"] = domain[2]\n",
    "                row[f\"domain_length\"] = domain[3] \n",
    "            else:\n",
    "                row[f\"domain_{i}_E-value\"] = domain[0]\n",
    "                row[f\"domain_{i}_start\"] = domain[1]\n",
    "                row[f\"domain_{i}_end\"] = domain[2]\n",
    "                row[f\"domain_{i}_length\"] = domain[3]\n",
    "        writer.writerow(row)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
