{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_protein_go_dict(protein_to_go):\n",
    "   \"\"\"\n",
    "   Convert protein id : [GO ids] dict to GO id  : [protein ids] dict.\n",
    "   Args:\n",
    "         protein_to_go (dict): Protein ID to GO id dictionary\n",
    "\n",
    "    Returns:\n",
    "         dict: GO ID to protein IDs dictionary\n",
    "   \n",
    "   \"\"\"\n",
    "   go_to_proteins = defaultdict(list)\n",
    "   for protein, go_terms in protein_to_go.items():\n",
    "       for go_term in go_terms:\n",
    "           go_to_proteins[go_term].append(protein)\n",
    "   return go_to_proteins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_go_terms_with_ancestors(protein_to_go, go_obo):\n",
    "    \"\"\"\n",
    "    Expand GO IDs with their ancestors.\n",
    "    \n",
    "    Args:\n",
    "        protein_to_go (dict): Dictionary mapping GO ids to Protein ID.\n",
    "        go_obo (GODag): Parsed GO DAG for ancestor retrieval.\n",
    "        \n",
    "    Returns:\n",
    "        dict: Updated protein_to_go dictionary with expanded GO IDs.\n",
    "    \"\"\"\n",
    "\n",
    "    expanded_protein_to_go = defaultdict(set)\n",
    "    \n",
    "    for protein, go_terms in protein_to_go.items():\n",
    "        for go_id in go_terms:\n",
    "            # Add the GO ID itself\n",
    "            expanded_protein_to_go[protein].add(go_id)\n",
    "            \n",
    "            # Add all ancestor terms (i.e their IDs) of that GO ID \n",
    "            if go_id in go_obo:\n",
    "                ancestors = go_obo[go_id].get_all_parents()  # Get all ancestors\n",
    "                expanded_protein_to_go[protein].update(ancestors)\n",
    "    \n",
    "   \n",
    "    return {protein: list(go_terms) for protein, go_terms in expanded_protein_to_go.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_go_terms_given_goid(protein_go_dict: Dict[str, List[str]]) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Convert GO IDs to their corresponding terms using the Gene Ontology API.\n",
    "    \n",
    "    Args:\n",
    "        protein_go_dict (dict) : Dictionary mapping protein IDs to lists of GO IDs\n",
    "        \n",
    "    Returns:\n",
    "        dict : Dictionary mapping GO IDs to their terms\n",
    "        \n",
    "\n",
    "    \"\"\"\n",
    "    # Extract unique GO IDs from the dictionary\n",
    "    go_ids = set()\n",
    "    for go_list in protein_go_dict.values():\n",
    "        go_ids.update(go_list)\n",
    "    \n",
    "    # Initialize results dictionary\n",
    "    go_terms_dict = {}\n",
    "    \n",
    "    # Base URL for the Gene Ontology API\n",
    "    base_url = \"http://api.geneontology.org/api/ontology/term/\"\n",
    "    \n",
    "    # Process each GO ID\n",
    "    for go_id in go_ids:\n",
    "        try:\n",
    "            # Add delay \n",
    "            time.sleep(0.1)\n",
    "            \n",
    "            # Make API request using the Gene Ontology API, i.e. search the API based on the current GO ID\n",
    "            response = requests.get(f\"{base_url}{go_id}\")\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # Extract the corresponding GO term\n",
    "            data = response.json()\n",
    "            go_terms_dict[go_id] = data.get('label', 'Term not found')\n",
    "        \n",
    "        # Some error catching \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error fetching term for {go_id}: {str(e)}\")\n",
    "            go_terms_dict[go_id] = 'Error fetching term'\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error processing {go_id}: {str(e)}\")\n",
    "            go_terms_dict[go_id] = 'Error processing term'\n",
    "    \n",
    "    return go_terms_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_go_annotations(protein_id):\n",
    "    \"\"\"\n",
    "    Fetch GO annotations and create GO ID to protein list mapping.\n",
    "    \n",
    "    Args:\n",
    "        protein_id (str): single protein ID of our family \n",
    "        \n",
    "    Returns:\n",
    "        List : List of the GO ids found for that protein\n",
    "    \"\"\"\n",
    "    go_ids = []\n",
    "    \n",
    "\n",
    "    # Base URL for the UniProt API\n",
    "    url = f\"https://rest.uniprot.org/uniprotkb/{protein_id}.xml\"\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        namespaces = {'ns': 'http://uniprot.org/uniprot'}\n",
    "        root = ET.fromstring(response.content)\n",
    "        \n",
    "        # Get all GO IDs for the protein\n",
    "        for db_ref in root.findall(\".//ns:dbReference[@type='GO']\", namespaces):\n",
    "            go_id = db_ref.attrib.get('id')\n",
    "            \n",
    "            if go_id:\n",
    "                go_ids.append(go_id)\n",
    "    \n",
    "    # Some error catching\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching GO annotations for {protein_id}: {e}\")\n",
    "   \n",
    "            \n",
    "    return go_ids\n",
    "\n",
    "\n",
    "# Let's add some debugging to help understand what's happening\n",
    "# here we see that the big .xml file has the same structure as the small ones \n",
    "# we already analyzed ; thus,we can use the same parsing structure, but this time directly\n",
    "# just collect the counts of GO terms, because that is all we need (no diff. categories, would just make our code slower)\n",
    "def print_swissprot_file(swissprot_xml_path, length = 50):\n",
    "    \"\"\"\n",
    "    Just to look at the first few lines to see the structure\n",
    "    \"\"\"\n",
    "\n",
    "    with open(swissprot_xml_path, 'r') as f:\n",
    "        print(\"First length lines of the file:\")\n",
    "        for i, line in enumerate(f):\n",
    "            if i < length:\n",
    "                print(line.strip())\n",
    "            else:\n",
    "                break\n",
    "\n",
    "\n",
    "\n",
    "def parse_swissprot_go_terms(swissprot_xml_path, family_proteins):\n",
    "   \"\"\"\n",
    "   Parse GO IDs from SwissProt XML file for each protein, excluding proteins in the family.\n",
    "   \n",
    "   Args:\n",
    "       swissprot_xml_path (str): Path to SwissProt XML file\n",
    "       family_proteins (set): UniProt IDs in protein family\n",
    "   \n",
    "   Returns:\n",
    "       dict: protein ID : [GO IDs] for that protein\n",
    "   \"\"\"\n",
    "   protein_to_go = defaultdict(list)\n",
    "   total_proteins = 0\n",
    "   skipped_proteins = 0\n",
    "   \n",
    "   namespaces = {'ns': 'http://uniprot.org/uniprot'}\n",
    "   context = ET.iterparse(swissprot_xml_path, events=('end',))\n",
    "   \n",
    "   print(\"Starting to parse SwissProt XML...\")\n",
    "   \n",
    "   for event, elem in context:\n",
    "       if elem.tag.endswith('entry'):\n",
    "           accession = elem.find(\".//ns:accession\", namespaces)\n",
    "           if accession is not None:\n",
    "               uniprot_id = accession.text\n",
    "               \n",
    "               # Exclude family proteins\n",
    "               if uniprot_id in family_proteins:\n",
    "                   skipped_proteins += 1\n",
    "               else:\n",
    "                   # Get all GO IDs for the protein (same structure as in fetch_go_annotations)\n",
    "                   for db_ref in elem.findall(\".//ns:dbReference[@type='GO']\", namespaces):\n",
    "                       go_id = db_ref.attrib.get('id')\n",
    "                       if go_id:\n",
    "                           protein_to_go[uniprot_id].append(go_id)\n",
    "                   total_proteins += 1\n",
    "\n",
    "           elem.clear()\n",
    "           \n",
    "           # Keep track of progress, as it takes some time to parse the whole file\n",
    "           if (total_proteins + skipped_proteins) % 10000 == 0:\n",
    "               print(f\"Processed {total_proteins} proteins \"\n",
    "                     f\"(skipped {skipped_proteins} family proteins)...\")\n",
    "             \n",
    "    \n",
    "               \n",
    "                    \n",
    "               \n",
    "   return protein_to_go\n",
    "\n",
    "def calculate_go_enrichment(go_to_proteins_family, go_to_proteins_swissprot, total_proteins_family, total_proteins_swissprot, go_id_to_go_term):\n",
    "    \"\"\" \n",
    "    Perform Fisher's exact test to calculate GO term enrichment in our protein family.\n",
    "\n",
    "    Args:\n",
    "        go_to_proteins_family (dict): GO ID to list of proteins in family\n",
    "        go_to_proteins_swissprot (dict): GO ID to list of proteins in SwissProt\n",
    "        total_proteins_family (int): Total proteins in family\n",
    "        total_proteins_swissprot (int): Total proteins in SwissProt\n",
    "        go_id_to_go_term (dict): GO ID to GO term mapping\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with GO term enrichment results\n",
    "\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    \n",
    "    for go_id in go_to_proteins_family.keys():\n",
    "   \n",
    "        # Create the 2x2 contingency table for Fisher's exact test\n",
    "        # The table looks like this:\n",
    "        #                   Protein in family    Protein not in family (i.e. all in SwissProt - family proteins)\n",
    "        # Has GO term            a                    b\n",
    "        # No GO term             c                    d\n",
    "        \n",
    "        # Contingency table calculations:\n",
    "        a = len(go_to_proteins_family[go_id])  # Proteins with this GO term in family\n",
    "        \n",
    "      \n",
    "        b = len(go_to_proteins_swissprot.get(go_id, []))  # Proteins with GO term in rest of SwissProt (without family)\n",
    "        \n",
    "        c = total_proteins_family - a  # Proteins without GO term in family\n",
    "        \n",
    "  \n",
    "        d = total_proteins_swissprot - b # Proteins without GO term in rest of SwissProt (without family)\n",
    "        \n",
    "        # Verify all values are non-negative before creating contingency table\n",
    "        if all(x >= 0 for x in [a, b, c, d]):\n",
    "            contingency_table = [[a, b], [c, d]]\n",
    "            \n",
    "            # Perform Fisher's exact test\n",
    "            # We ask : is the GO term appearing more often in our family than we would expect by random chance ?\n",
    "            # The null hypothesis (H0) is: \"The proportion of proteins with this GO term in our family \n",
    "            # is the same as the proportion in the SwissProt dataset (without the protein in the family).\" \n",
    "            # In other words, under H0, getting the GO term is independent of being in our family (so it doesn't represent the family)\n",
    "            # Alternative Hypothesis (H1) using the right-tail and two-tail:\n",
    "            #Right-tail (greater): Our family has a higher proportion of this GO term than SwissProt\n",
    "            #Two-tail (two-sided): The proportion is different (either higher or lower)\n",
    "\n",
    "            #Fisher's exact test calculates the probability of seeing our observed data (or more extreme) under the null hypothesis.\n",
    "            #A very small p-value (like < 0.05) tells us:\n",
    "            #Two-tail: This GO term's frequency is significantly different from SwissProt\n",
    "            #Right-tail: This GO term is significantly enriched in our family(overrepresented)\n",
    "\n",
    "\n",
    "            odds_ratio, pvalue_two_tail = fisher_exact(contingency_table, alternative='two-sided')\n",
    "            _, pvalue_greater = fisher_exact(contingency_table, alternative='greater')\n",
    "      \n",
    "            # Calculate proportions\n",
    "            my_proportion = a / total_proteins_family \n",
    "            # Notice that we add the proteins in the family to the proteins in SwissProt to get the total number of proteins\n",
    "            swissprot_proportion = (a+b) / (total_proteins_swissprot + total_proteins_family)\n",
    "\n",
    "     \n",
    "            \n",
    "            results.append({\n",
    "                'GO_ID': go_id,\n",
    "                'GO_Term': go_id_to_go_term.get(go_id, 'N/A'), # Include GO term name\n",
    "                'Count_Prot_Dataset (a)': a,\n",
    "                'Count_Prot_SwissProt (b)': b,\n",
    "                'c': c,\n",
    "                'd': d,\n",
    "                'Count_Prot_SwissProt_Actual': a+b,\n",
    "                'Percentage_Dataset': round(my_proportion * 100, 2),\n",
    "                'Percentage_SwissProt': round(swissprot_proportion * 100, 10),\n",
    "                'Fold_Enrichment': round(my_proportion/swissprot_proportion,2),\n",
    "                'P_Value_Two_Tail': pvalue_two_tail,\n",
    "                'P_Value_Greater': pvalue_greater,\n",
    "            })\n",
    "    \n",
    "    # Convert to DataFrame and sort by p-value\n",
    "    df_results = pd.DataFrame(results)\n",
    "    if not df_results.empty:\n",
    "        df_results = df_results.sort_values('P_Value_Two_Tail')\n",
    "\n",
    "    df_results.to_csv(\"Function/enrichment_results.csv\")\n",
    "    \n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hierarchical Structure\n",
    "def analyze_go_hierarchy():\n",
    "    \"\"\"\n",
    "    Analyze the hierarchical structure of enriched GO terms.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the Gene Ontology DAG\n",
    "    go_obo = obo_parser.GODag('go.obo')\n",
    "    \n",
    "    # Read our enrichment results\n",
    "    df = pd.read_csv(\"Function/enrichment_results.csv\")\n",
    "    \n",
    "    # Filter for significantly enriched terms\n",
    "    enriched_terms = df[\n",
    "        (df['P_Value_Two_Tail'] < 0.05) &\n",
    "        (df['P_Value_Greater'] < 0.05)\n",
    "    ]\n",
    "    \n",
    "    # Create a dictionary to store branch information\n",
    "    branch_info = {}\n",
    "    \n",
    "    # For each enriched term, traverse up its ancestry\n",
    "    for _, row in enriched_terms.iterrows():\n",
    "        go_id = row['GO_ID']\n",
    "        if go_id in go_obo:\n",
    "            term = go_obo[go_id]\n",
    "            \n",
    "            # Get all ancestors (parents) up to the root of the DAG of the current term (i.e. the current GO ID)\n",
    "            \n",
    "            ancestors = term.get_all_parents()\n",
    "            \n",
    "            for ancestor_id in ancestors:\n",
    "                if ancestor_id not in branch_info:\n",
    "                    ancestor_term = go_obo[ancestor_id]\n",
    "                    # Get the immediate parent as the branch name (if there is one, else take the ancestor_term itself)\n",
    "                    parent_terms = ancestor_term.parents\n",
    "                    branch_name = next(iter(parent_terms)).name if parent_terms else ancestor_term.name\n",
    "                    \n",
    "                    branch_info[ancestor_id] = {\n",
    "                        'term_name': ancestor_term.name,\n",
    "                        'branch_name': branch_name,\n",
    "                        'enriched_children': [],\n",
    "                        'total_significance': 0,\n",
    "                        'depth': ancestor_term.depth,\n",
    "                    }\n",
    "\n",
    "               \n",
    "                # Our go_id in the current iteration is a child to ALL ancestors we found using \"get_all_parents()\"\n",
    "                #  (note that this is not necessarily a direct child, but maybe also much more down in the tree somewhere)\n",
    "                # Thus, add this child into the enriched_children list of the ancestor with its two-tailed p-value \n",
    "                branch_info[ancestor_id]['enriched_children'].append({\n",
    "                    'id': go_id,\n",
    "                    'name': term.name,\n",
    "                    'p_value': row['P_Value_Two_Tail']\n",
    "                })\n",
    "                # Measure significance based on -log value of the p value of all the childs of the ancestor (lower p values have higher scores)\n",
    "                branch_info[ancestor_id]['total_significance'] += -np.log10(row['P_Value_Two_Tail'])\n",
    "    \n",
    "    # Filter for high-level terms (lower depth) with multiple enriched children\n",
    "    significant_branches = {\n",
    "        go_id: info for go_id, info in branch_info.items() # take each key,value of the branch_info dictionary\n",
    "        if len(info['enriched_children']) >= 2  # At least 2 enriched children\n",
    "        and info['depth'] <= 3  # High-level terms having maximum depth of 3 (i.e. only look at GO terms high up in the tree)\n",
    "    } \n",
    "    \n",
    "    # Sort branches by their total significance\n",
    "    sorted_branches = sorted(\n",
    "        significant_branches.items(),\n",
    "        key=lambda x: x[1]['total_significance'],\n",
    "        reverse=True\n",
    "    )\n",
    "    \n",
    "    # Create a list to store the branch information\n",
    "    branch_data = []\n",
    "\n",
    "\n",
    "    for go_id, info in sorted_branches[:20]:  # Top 20 branches\n",
    "        branch_data.append({\n",
    "            'GO_ID': go_id,\n",
    "            'GO_Term': info['term_name'],\n",
    "            'Branch_Name' : info['branch_name'],\n",
    "            'Hierarchy_Depth': info['depth'],\n",
    "            'Number_Enriched_Terms': len(info['enriched_children']),\n",
    "            'Total_Significance_Score': info['total_significance']\n",
    "        })\n",
    "\n",
    "    # Create a DataFrame and save to CSV\n",
    "    branches_df = pd.DataFrame(branch_data)\n",
    "    branches_df.to_csv('Function/enriched_branches.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    psiblast_file = \"Model/Evaluation/Predictions/PSI-BLAST/psiblastsearch_output.csv\"\n",
    "    hmm_file = \"Model/Evaluation/Predictions/HMM-SEARCH/hmmsearch_output.csv\"\n",
    "    protein_ids = load_protein_ids(psiblast_file, hmm_file)\n",
    "\n",
    "\n",
    "    # Proteins_to_GO terms for our family \n",
    "    print(\"Fetching GO annotations...\")\n",
    "    family_annotations = {}\n",
    "    for pid in tqdm(protein_ids, desc=\"Fetching GO annotations\"):\n",
    "        family_annotations[pid] = fetch_go_annotations(pid)\n",
    "\n",
    "    total_proteins_family = len(family_annotations)\n",
    "\n",
    "\n",
    "    \n",
    "    # Proteins_to_GO terms for SwissProt\n",
    "    swissprot_annotations = parse_swissprot_go_terms(\"uniprot_sprot.xml\", protein_ids)\n",
    "\n",
    "    total_proteins_swissprot = len(swissprot_annotations)\n",
    "\n",
    "    # Load the GO DAG for ancestor expansion\n",
    "    # We downloaded the go.obo file so we can parse the whole ontology\n",
    "    # Note that \"go.obo\" we downloaded locally and not to the Git Repository due to its size\n",
    "    print(\"Expanding GO terms to include ancestors...\")\n",
    "    go_obo = obo_parser.GODag('go.obo')\n",
    "    expanded_family_annotations = expand_go_terms_with_ancestors(family_annotations, go_obo)\n",
    "    expanded_swissprot_annotations = expand_go_terms_with_ancestors(swissprot_annotations, go_obo)\n",
    "\n",
    "    # This we didn't directly use, but since it is Task 1 in the Function assignment, we include it \n",
    "    expanded_family_annotations_df = pd.DataFrame(expanded_family_annotations.items(), columns=['Protein ID', 'GO IDs'])\n",
    "    expanded_family_annotations_df.to_csv(\"Function/expanded_family_annotations.csv\", index=False)\n",
    "\n",
    "    # Fetch all GO terms for all found GO IDs in the family after expanding \n",
    "    go_id_to_go_term = get_go_terms_given_goid(expanded_family_annotations)\n",
    "\n",
    "    # Reverse mapping (go_id to proteins mapping) for enrichment\n",
    "    go_to_proteins_family = reverse_protein_go_dict(expanded_family_annotations)\n",
    "    go_to_proteins_swissprot = reverse_protein_go_dict(expanded_swissprot_annotations)\n",
    "\n",
    "    # Calculate GO enrichments\n",
    "    _ = calculate_go_enrichment(go_to_proteins_family, go_to_proteins_swissprot,\n",
    "                                total_proteins_family, total_proteins_swissprot, go_id_to_go_term)\n",
    "\n",
    "    # Analyze hierarchy using the \"cut tree\" approach\n",
    "    print(\"Analyzing GO hierarchy using the 'cut tree' approach...\")\n",
    "    analyze_go_hierarchy_cut_tree(\n",
    "    enrichment_results_path=\"Function/enrichment_results.csv\",\n",
    "    go_obo_path=\"go.obo\",\n",
    "    output_path=\"Function/enriched_branches_cut_tree.csv\"\n",
    "    )\n",
    "    \n",
    "    # Read the enrichment results\n",
    "    df = pd.read_csv(\"Function/enrichment_results.csv\")\n",
    "\n",
    "    # Get the terms to the GO ids from the family data\n",
    "  #  go_id_to_term = create_go_id_to_term_mapping(family_annotations)\n",
    "\n",
    "    # Filter for significantly enriched terms\n",
    "    enriched_terms = df[\n",
    "    (df['P_Value_Two_Tail'] < 0.05) &\n",
    "    (df['P_Value_Greater'] < 0.05)\n",
    "    ]\n",
    "\n",
    "\n",
    "    # Create word frequencies using the actual GO terms instead of IDs\n",
    "    word_frequencies = {}\n",
    "    for _, row in enriched_terms.iterrows():\n",
    "        go_id = row['GO_ID']\n",
    "        if go_id in go_id_to_go_term:  # Make sure we have the term for this ID\n",
    "            term = go_id_to_go_term[go_id]\n",
    "            # Use fold enrichment as weight\n",
    "            weight = row['Fold_Enrichment']\n",
    "            word_frequencies[term] = weight\n",
    "\n",
    "    # Create and display the word cloud\n",
    "    wordcloud = WordCloud(\n",
    "        width=1200, \n",
    "        height=800,\n",
    "        background_color='white',\n",
    "        prefer_horizontal=0.7,\n",
    "        max_words=50,  # Limit to top 50 terms for better readability\n",
    "        min_font_size=10,\n",
    "        max_font_size=60\n",
    "    ).generate_from_frequencies(word_frequencies)\n",
    "\n",
    "    # Plot and save the word cloud\n",
    "    plt.figure(figsize=(20, 12))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title('GO Term Enrichment Word Cloud', fontsize=16, pad=20)\n",
    "    plt.savefig('Function/go_enrichment_wordcloud.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    # Print out the enriched terms for verification\n",
    "    print(\"\\nTop enriched GO terms:\")\n",
    "    sorted_terms = sorted(word_frequencies.items(), key=lambda x: x[1], reverse=True)\n",
    "    for term, weight in sorted_terms[:10]:\n",
    "        print(f\"\\nTerm: {term}\")\n",
    "        print(f\"Weight in word cloud: {weight:.2f}\")\n",
    "\n",
    "    \n",
    "    # Hierarchy\n",
    "\n",
    "    analyze_go_hierarchy()\n",
    "\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
