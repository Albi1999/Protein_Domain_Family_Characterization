{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biological Data Project\n",
    "\n",
    "Group members:\n",
    "\n",
    "- Alberto Calabrese\n",
    "\n",
    "- Marlon Helbing\n",
    "\n",
    "- Lorenzo Baietti"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"A protein domain is a conserved part of a given protein sequence and tertiary structure that can evolve, function, and exist independently of the rest of the protein chain. Each domain forms a compact three-dimensional structure and often can be independently stable and folded.\" ([Wikipedia](https://en.wikipedia.org/wiki/Protein_domain))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The project is about the characterization of a single domain. Each group is provided with a representative domain sequence and the corresponding Pfam identifier (see table below). The objective of the project is to build a sequence model starting from the assigned sequence and to provide a functional characterization of the entire domain family (homologous proteins)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input\n",
    "A representative sequence of the domain family. Columns are: group, UniProt accession, organism, Pfam identifier, Pfam name, domain position in the corresponding UniProt protein, domain sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "UniProt : P54315 \n",
    "PfamID : PF00151 \n",
    "Domain Position : 18-353 \n",
    "Organism : Homo sapiens (Human) \n",
    "Pfam Name : Lipase/vitellogenin \n",
    "Domain Sequence : KEVCYEDLGCFSDTEPWGGTAIRPLKILPWSPEKIGTRFLLYTNENPNNFQILLLSDPSTIEASNFQMDRKTRFIIHGFIDKGDESWVTDMCKKLFEVEEVNCICVDWKKGSQATYTQAANNVRVVGAQVAQMLDILLTEYSYPPSKVHLIGHSLGAHVAGEAGSKTPGLSRITGLDPVEASFESTPEEVRLDPSDADFVDVIHTDAAPLIPFLGFGTNQQMGHLDFFPNGGESMPGCKKNALSQIVDLDGIWAGTRDFVACNHLRSYKYYLESILNPDGFAAYPCTSYKSFESDKCFPCPDQGCPQMGHYADKFAGRTSEEQQKFFLNTGEASNF\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Domain model definition\n",
    "The objective of the first part of the project is to build a PSSM and HMM model representing the assigned domain. The two models will be generated starting from the assigned input sequence. The accuracy of the models will be evaluated against Pfam annotations as provided in the SwissProt database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Building \n",
    "1. Retrieve homologous proteins starting from your input sequence performing a BLAST search\n",
    "against UniProt or UniRef50 or UniRef90, or any other database\n",
    "\n",
    "- We use https://www.uniprot.org/blast \n",
    "    - use the Domain Sequence as Input\n",
    "    Parameters : \n",
    "    - against UniProtKB\n",
    "    - e-value thresh : 0.0001\n",
    "    - 1000 hits\n",
    "\n",
    "- results in 1000 hits \n",
    "- do ID matching to UniProtKB (we need to do that to download the .fasta file)\n",
    "-  results in 'UNIPROTKB_INITIAL_ORIGINAL.FASTA' \n",
    "\n",
    "2. Generate a multiple sequence alignment (MSA) starting from retrieved hits using T-coffee or\n",
    "ClustalOmega or MUSCLE\n",
    "    - We use https://www.ebi.ac.uk/jdispatcher/msa/clustalo\n",
    "    Parameters :\n",
    "    - Output Format : FASTA\n",
    "\n",
    "- results in ClustalOmegaUniPortAlignment_ORIGINAL.fasta\n",
    "\n",
    "3. If necessary, edit the MSA with JalView (or with your custom script or CD-HIT) to remove not\n",
    "conserved positions (columns) and/or redundant information (rows)\n",
    "    - We first used JalView at a 100% threshold to check for redundant rows, which left us with 155 sequences\n",
    "\n",
    "- results in ClustalOmegaUniPortAlignment.fasta\n",
    "\n",
    "    - Then we utilize 'conservation.py' to remove columns based on different kinds of metrics (right now only that, lets check if the model is good enough and then we can clean up the code)\n",
    "        - We have gap_threshold, which removes columns based on how high of a percentage of gaps they have \n",
    "        - We have entropy_threshold, which removes columns based on single amino acid entropy in the column\n",
    "        - We have group_entropy_threshold, which calculates entropies based on predefined groups of amino acids in a column and removes based on the threshold\n",
    "    \n",
    "    - We did.... (gap_threshold, group_entropy_threshold,.....)\n",
    "\n",
    "    - TODO : for the group_entropy thing, I don't know if the MSA clustering already takes that into account and maybe it doesn't help us, but we should try anyway\n",
    "    - TODO : try some different models based on gap_threshold, entropy_threshold and group_entropy_threshold (especially use group_entropy_threshold and entropy_threshold, bc in class we used entropy for getting a better model); our current best model is :\n",
    "        - HMM with first removed redundant rows (I'd say we can always initially start from ClustalOmegaUniProtAlignment.fasta, its in the Model/Building/MSA folder, it has rows with 100% identity removed by using JalView) and here in the notebook only gap threshold of 0.90, nothing else :\n",
    "\n",
    "\n",
    "        Protein-Level Confusion Matrix HMM:\n",
    "        True Positives: 82\n",
    "        False Positives: 1\n",
    "        False Negatives: 0\n",
    "        True Negatives: 0\n",
    "\n",
    "\n",
    "        Protein-Level Metrics HMM:\n",
    "        Precision: 0.9880\n",
    "        Recall: 1.0000\n",
    "        F-score: 0.9939\n",
    "        Balanced Accuracy: 0.5000\n",
    "        MCC: 0.0000\n",
    "\n",
    "\n",
    "\n",
    "        Residue-Level Confusion Matrix HMM:\n",
    "        True Positives: 22810\n",
    "        False Positives: 5440\n",
    "        False Negatives: 2205\n",
    "        True Negatives: 2278\n",
    "\n",
    "\n",
    "        Residue-Level Metrics HMM:\n",
    "        Precision: 0.8074\n",
    "        Recall: 0.9119\n",
    "        F-score: 0.8565\n",
    "        Balanced Accuracy: 0.6035\n",
    "        MCC: 0.2556\n",
    "\n",
    "\n",
    "\n",
    "- results in trimmed_alignment.fasta\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import AlignIO\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from scipy.stats import entropy\n",
    "import math\n",
    "from Bio.Align import MultipleSeqAlignment\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "from Bio.Seq import Seq\n",
    "from Bio import SeqIO\n",
    "import sys\n",
    "import csv\n",
    "import re \n",
    "import json\n",
    "import requests\n",
    "import time\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, balanced_accuracy_score, matthews_corrcoef\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from scipy.spatial.distance import pdist\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "from Bio import Phylo\n",
    "from io import StringIO\n",
    "#from ete3 import Tree, TreeStyle, NodeStyle, TextFace\n",
    "import xml.etree.ElementTree as ET\n",
    "from scipy.stats import fisher_exact\n",
    "from wordcloud import WordCloud\n",
    "import random\n",
    "import obonet\n",
    "import networkx as nx\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "from goatools import obo_parser\n",
    "import ast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConservationAnalyzer:\n",
    "    def __init__(self, alignment_file):\n",
    "        \"\"\"\n",
    "        Initialize with an alignment file\n",
    "            alignment_file (str): Path to the alignment file\n",
    "        \"\"\"\n",
    "        self.alignment = AlignIO.read(alignment_file, 'fasta')\n",
    "        self.num_sequences = len(self.alignment)\n",
    "        self.alignment_length = self.alignment.get_alignment_length()\n",
    "        \n",
    "    def get_column(self, pos):\n",
    "        \"\"\"Extract a column from the alignment\"\"\"\n",
    "        return [record.seq[pos] for record in self.alignment]\n",
    "    \n",
    "    def calculate_gap_frequency(self, pos):\n",
    "        \"\"\"Calculate frequency of gaps in a column\"\"\"\n",
    "        column = self.get_column(pos)\n",
    "        return column.count('-') / len(column)\n",
    "    \n",
    "    def calculate_amino_acid_frequencies(self, pos):\n",
    "        \"\"\"Calculate frequencies of each amino acid in a column\"\"\"\n",
    "        column = self.get_column(pos)\n",
    "        total = len(column) - column.count('-')  # Don't count gaps, such that when we calculate conservation scores the gaps don't mess it up \n",
    "        if total == 0:\n",
    "            return {}\n",
    "        \n",
    "        counts = Counter(aa for aa in column if aa != '-')\n",
    "        return {aa: count/total for aa, count in counts.items()}\n",
    "\n",
    "    def calculate_entropy(self, pos):\n",
    "        \"\"\"\n",
    "        Calculate Shannon entropy for a column\n",
    "        Lower entropy means higher conservation\n",
    "        \"\"\"\n",
    "        freqs = self.calculate_amino_acid_frequencies(pos)\n",
    "        if not freqs:\n",
    "            return float('inf')  \n",
    "        \n",
    "        return -sum(p * math.log2(p) for p in freqs.values())\n",
    "    \n",
    "    def get_amino_acid_groups(self):\n",
    "        \"\"\"Define groups of similar amino acids \n",
    "           Based on : https://en.wikipedia.org/wiki/Conservative_replacement#:~:text=There%20are%2020%20naturally%20occurring,both%20small%2C%20negatively%20charged%20residues.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'aliphatic': set('GAVLI'),\n",
    "            'hydroxyl': set('SCUTM'),\n",
    "            'cyclic': set('P'),\n",
    "            'aromatic': set('FYW'),\n",
    "            'basic': set('HKR'),\n",
    "            'acidic': set('DENQ')\n",
    "        }\n",
    "    \n",
    "\n",
    "    def calculate_group_entropy(self, pos):\n",
    "        \"\"\"\n",
    "        Calculate Shannon entropy based on amino acid groups for a column\n",
    "        Lower entropy means higher conservation of amino acid groups\n",
    "        \n",
    "        Returns:\n",
    "            float: Group entropy value (lower means more conserved groups)\n",
    "                Returns float('inf') if column contains only gaps\n",
    "        \"\"\"\n",
    "        column = self.get_column(pos)\n",
    "        groups = self.get_amino_acid_groups()\n",
    "        \n",
    "        # Create mapping from amino acid to group\n",
    "        aa_to_group = {}\n",
    "        for group_name, aas in groups.items():\n",
    "            for aa in aas:\n",
    "                aa_to_group[aa] = group_name\n",
    "                \n",
    "        # Count group occurrences (excluding gaps)\n",
    "        group_counts = Counter(aa_to_group.get(aa, 'other') \n",
    "                            for aa in column if aa != '-')\n",
    "        \n",
    "        # If column has only gaps, return infinity\n",
    "        if not group_counts:\n",
    "            return float('inf')\n",
    "            \n",
    "        # Calculate group frequencies\n",
    "        total = sum(group_counts.values())\n",
    "        group_freqs = {group: count/total for group, count in group_counts.items()}\n",
    "        \n",
    "        # Calculate entropy\n",
    "        return -sum(p * math.log2(p) for p in group_freqs.values())\n",
    "\n",
    "\n",
    "    def analyze_columns(self, gap_threshold=0.90, group_entropy_threshold=0.4, entropy_threshold=0.5):\n",
    "        \"\"\"\n",
    "        Analyze all columns and return comprehensive metrics\n",
    "        Returns DataFrame with various conservation metrics for each position\n",
    "        \"\"\"\n",
    "        data = []\n",
    "        \n",
    "        for i in range(self.alignment_length):\n",
    "            gap_freq = self.calculate_gap_frequency(i)\n",
    "          #  cons_score = self.calculate_conservation_score(i)\n",
    "            info_content = self.calculate_entropy(i)\n",
    "         #   group_cons = self.calculate_group_conservation(i)\n",
    "            group_entropy = self.calculate_group_entropy(i)\n",
    "            \n",
    "            data.append({\n",
    "                'position': i + 1,\n",
    "                'gap_frequency': gap_freq,\n",
    "             #   'single_conservation': cons_score,\n",
    "                'entropy': info_content,\n",
    "             #   'group_conservation': group_cons,\n",
    "                'group_entropy': group_entropy,\n",
    "                # Here we should look possibly for better ideas\n",
    "                # Check gap frequency not too high (i.e. not nearly all elements in the columns gaps (-))\n",
    "                # Check that the group conservation is high enough (i.e. the amino acids are not too different\n",
    "                # ; right now we do with groups and not single amino acid sequence since I'd say the groups\n",
    "                # are more representative (if we do single amino acids, we'd delete more stuff))\n",
    "                'suggested_remove': (gap_freq > gap_threshold) #or      \n",
    "                                 # or group_entropy < group_entropy_threshold)\n",
    "                                 # or info_content < entropy_threshold)\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "def remove_columns_from_alignment(input_file, output_file, columns_to_remove, format=\"fasta\"):\n",
    "    \"\"\"\n",
    "    Remove specified columns from a multiple sequence alignment and save to new file\n",
    "    \n",
    "    Args:\n",
    "        input_file (str): Path to input alignment file\n",
    "        output_file (str): Path where to save trimmed alignment\n",
    "        columns_to_remove (list): List of column indices to remove (0-based)\n",
    "        format (str): File format (default: \"fasta\")\n",
    "    \"\"\"\n",
    "    # Read the alignment\n",
    "    alignment = AlignIO.read(input_file, format)\n",
    "    \n",
    "    # Sort columns to remove in descending order\n",
    "    # (so removing them doesn't affect the indices of remaining columns)\n",
    "    columns_to_remove = sorted(columns_to_remove, reverse=True)\n",
    "    \n",
    "    # Create new alignment records\n",
    "    new_records = []\n",
    "    \n",
    "    # Process each sequence\n",
    "    for record in alignment:\n",
    "        # Convert sequence to list for easier manipulation\n",
    "        seq_list = list(record.seq)\n",
    "        \n",
    "        # Remove specified columns\n",
    "        for col in columns_to_remove:\n",
    "            del seq_list[col]\n",
    "        \n",
    "        # Create new sequence record\n",
    "        new_seq = Seq(''.join(seq_list)) # Join the list element to a string again (i.e. after removal of amino acids out of sequence represented as list, turn into one string again) and turn into Seq object\n",
    "        new_record = SeqRecord(new_seq,\n",
    "                            id=record.id,\n",
    "                            name=record.name,\n",
    "                            description=record.description)\n",
    "        new_records.append(new_record)\n",
    "    \n",
    "    # Create new alignment\n",
    "    # TODO : Maybe we have to add some variables here (i.e. how to do the MSA)!\n",
    "    new_alignment = MultipleSeqAlignment(new_records)\n",
    "    \n",
    "    # Write to file\n",
    "    AlignIO.write(new_alignment, output_file, format)\n",
    "    \n",
    "    return new_alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Alignment Summary:\n",
      "Number of sequences: 155\n",
      "Alignment length: 3254\n",
      "With the current removal tactic, we would remove 0.68 percent of columns ; we keep 1043 of 3254 columns\n",
      "Original alignment length: 3254\n",
      "Number of columns removed: 2211\n",
      "New alignment length: 1043\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Initialize analyzer \n",
    "    analyzer = ConservationAnalyzer(\"Model/Building/MSA/ClustalOmegaUniProtAlignment.fasta\")\n",
    "    \n",
    "    # Get comprehensive analysis\n",
    "    analysis = analyzer.analyze_columns(gap_threshold=0.90)\n",
    "   # analysis_2 = analyzer.analyze_rows()\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\nAlignment Summary:\")\n",
    "    print(f\"Number of sequences: {analyzer.num_sequences}\")\n",
    "    print(f\"Alignment length: {analyzer.alignment_length}\")\n",
    "\n",
    "\n",
    "    # Print number of True/False\n",
    "    counts = analysis['suggested_remove'].value_counts()\n",
    "\n",
    "    counts_true = counts[True]  # To be removed\n",
    "    counts_false = counts[False] # To be kept\n",
    "\n",
    "    print(f\"With the current removal tactic, we would remove {(counts_true / (counts_true + counts_false)):.2f} percent of columns ; we keep {counts_false} of {counts_false + counts_true} columns\")\n",
    "    \n",
    "\n",
    "    # Save detailed analysis to CSV\n",
    "    analysis.to_csv(\"Model/Building/MSA_processed/conservation_analysis.csv\", index=False)\n",
    "\n",
    "\n",
    "    # Get indices of columns marked for removal\n",
    "    columns_to_remove = analysis[analysis['suggested_remove']]['position'].values.tolist()\n",
    "    # Convert to 0-based indices (if positions were 1-based)\n",
    "    columns_to_remove = [x-1 for x in columns_to_remove]\n",
    "    \n",
    "    # Remove columns and save new alignment\n",
    "    new_alignment = remove_columns_from_alignment(\n",
    "        \"Model/Building/MSA/ClustalOmegaUniProtAlignment.fasta\",\n",
    "        \"Model/Building/MSA_processed/trimmed_alignment.fasta\",\n",
    "        columns_to_remove\n",
    "    )\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    print(f\"Original alignment length: {analyzer.alignment_length}\")\n",
    "    print(f\"Number of columns removed: {len(columns_to_remove)}\")\n",
    "    print(f\"New alignment length: {new_alignment.get_alignment_length()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file trimmed_alignement.fasta gives some problem during the costruction of pssm so is necessary to cut some columns and some sequenceses in the alignement to avoid to much consecuitives gaps: the new alignement is filtered_trimmed_alignement.fasta and it must be used with PSIBLAST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered and trimmed alignment saved to Model/Building/MSA_processed/filtered_trimmed_alignment.fasta\n"
     ]
    }
   ],
   "source": [
    "def filter_sequences_and_columns(input_file, output_file, alpha):\n",
    "    \"\"\"\n",
    "    Removes sequences with a gap percentage above a certain threshold\n",
    "    \"\"\"\t\n",
    "    # Load the alignment\n",
    "    alignment = AlignIO.read(input_file, \"fasta\")\n",
    "    \n",
    "    # Step 1: Filter sequences based on gap percentage\n",
    "    filtered_sequences = []\n",
    "    for record in alignment:\n",
    "        gap_count = record.seq.count('-')\n",
    "        gap_percentage = (gap_count / len(record.seq))\n",
    "        if gap_percentage <= alpha:\n",
    "            filtered_sequences.append(record)\n",
    "    \n",
    "    if not filtered_sequences:\n",
    "        print(\"No sequences left after filtering based on gap percentage.\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    filtered_alignment = MultipleSeqAlignment(filtered_sequences)\n",
    "\n",
    "     # Step 2: Trim columns from both ends based on gap thresholds\n",
    "    def trim_columns_based_on_gaps(alignment, gap_threshold):\n",
    "        \"\"\"\n",
    "        Removes columns from the left and right ends of the alignment\n",
    "        until no sequence has more than the gap threshold in a row\n",
    "        \"\"\"\n",
    "        alignment_length = alignment.get_alignment_length()\n",
    "        start, end = 0, alignment_length - 1\n",
    "\n",
    "        # Remove columns from the left until no sequence has more than the gap threshold in a row\n",
    "        while start < alignment_length:\n",
    "            # Check if any sequence exceeds the gap threshold\n",
    "            if any(record.seq[start:start + gap_threshold].count('-') == gap_threshold for record in alignment):\n",
    "                start += 1\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        # Remove columns from the right until no sequence has more than the gap threshold in a row\n",
    "        while end >= 0: \n",
    "            # Check if any sequence exceeds the gap threshold\n",
    "            if any(record.seq[end - gap_threshold + 1:end + 1].count('-') == gap_threshold for record in alignment):\n",
    "                end -= 1\n",
    "            else:\n",
    "                break\n",
    "    \n",
    "        # Trim the alignment\n",
    "        valid_columns = range(start, end + 1)\n",
    "        trimmed_sequences = []\n",
    "        for record in alignment:\n",
    "            new_seq = Seq(\"\".join(record.seq[i] for i in valid_columns))\n",
    "            trimmed_sequences.append(SeqRecord(new_seq, id=record.id, description=record.description))\n",
    "        \n",
    "        return MultipleSeqAlignment(trimmed_sequences)\n",
    "    \n",
    "    final_alignment = trim_columns_based_on_gaps(filtered_alignment, gap_threshold)\n",
    "\n",
    "    # Save the filtered and trimmed alignment\n",
    "    AlignIO.write(final_alignment, output_file, \"fasta\")\n",
    "    print(f\"Filtered and trimmed alignment saved to {output_file}\")\n",
    "\n",
    "# Parameters\n",
    "input_file = \"Model/Building/MSA_processed/trimmed_alignment.fasta\"\n",
    "output_file = \"Model/Building/MSA_processed/filtered_trimmed_alignment.fasta\"\n",
    "gap_threshold = 50  # Maximum consecutive gaps allowed from the beginning or end\n",
    "alpha = 0.79  # Change this threshold as needed (percentage of gaps)\n",
    "# I choose 0.79 because in trimmed_alignement.fasta there are few sequences that have a lot of gaps, but other sequences have at most 60% of gaps\n",
    "\n",
    "filter_sequences_and_columns(input_file, output_file, alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Build a PSSM model starting from the MSA\n",
    "\n",
    "- First direct in the folder where ncbi-blast-2.16.0+ was installed\n",
    "\n",
    "- Then use this terminal command : \n",
    "```bash\n",
    "ncbi-blast-2.16.0+/bin/psiblast -subject data/protein_family/filtered_trimmed_alignment.fasta -in_msa data/protein_family/filtered_trimmed_alignment.fasta -out_ascii_pssm data/protein_family/trimmed_alignment.pssm_ascii -out_pssm data/protein_family/trimmed_alignment.pssm\n",
    "```\n",
    "\n",
    "    Note that the trimmed_alignment.fasta needs to be in the described folder (data/protein_family/)\n",
    "\n",
    "\n",
    "5. Build a HMM model starting from the MSA\n",
    "\n",
    "- First direct in the folder where hmmer-3.4 was installed\n",
    "\n",
    "- Then use this terminal command :\n",
    "```bash\n",
    "hmmer-3.4/src/hmmbuild data/protein_family/trimmed_alignment.hmm data/protein_family/trimmed_alignment.fasta\n",
    "```\n",
    "    Note that the trimmed_alignment.fasta needs to be in the described folder (data/protein_family/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models evaluation\n",
    "1. Generate predictions. Run HMM-SEARCH and PSI-BLAST with your models against\n",
    "SwissProt.\n",
    "\n",
    "    - Collect the list of retrieved hits\n",
    "\n",
    "    - Collect matching positions of your models in the retrieved hits\n",
    "\n",
    "PSI-BLAST :\n",
    "-\tWhen working on MAC : First have to change settings so we have access to use psiblast & makeblastdb (in terminal, go to folder where psiblast/makeblastdb located and run this)\n",
    "```bash\n",
    "    xattr -c psiblast\n",
    "    chmod +x psiblast\n",
    "    xattr -c makeblastdb\n",
    "    chmod +x makeblastdb\n",
    "```\n",
    "- Then use this command to create a \"formatted swissprot database\" (we need to check what that exactly means)\n",
    "```bash\n",
    "      ./ncbi-blast-2.16.0+/bin/makeblastdb -in uniprot_sprot.fasta -dbtype prot -out swissprot\n",
    "```\n",
    "    Notice that uniprot_sprot.fasta needs to be installed and be located in the current folder that we are in (in the terminal)\n",
    "\n",
    "\n",
    "- Finally, the actual predictions can be obtained with the following command\n",
    "```bash\n",
    "        ./ncbi-blast-2.16.0+/bin/psiblast -in_pssm trimmed_alignment.pssm \\\n",
    "            -db swissprot \\\n",
    "            -out psiblastsearch_output.txt \\\n",
    "            -outfmt \"6 qseqid sseqid qstart qend sstart send pident evalue\" \\\n",
    "```\n",
    "where \n",
    "```bash\n",
    "    qseqid: Query sequence identifier (your domain)\n",
    "    sseqid: Subject sequence identifier (matched protein)\n",
    "    qstart: Start position in your query domain\n",
    "    qend: End position in your query domain\n",
    "    sstart: Start position in the matched sequence\n",
    "    send: End position in the matched sequence\n",
    "    pident: Percentage of identical matches\n",
    "    evalue: Expectation value (statistical significance)\n",
    "```\n",
    "\n",
    "HMMER :\n",
    "\n",
    "- Simply use \n",
    "```bash\n",
    "    ./hmmer-3.4/src/hmmsearch trimmed_alignment.hmm uniprot_sprot.fasta > hmmsearch_output.txt\n",
    "```\n",
    "\n",
    "- To make both of the output files more accessible, we parsed them by writing two scripts \n",
    "    - in that way we create two .csv files that are easier to be compared, also directly by eye\n",
    "    - these .csv files then contained the matching positions of our two models in the retrieved hits\n",
    "```bash\n",
    "Returns psiblastsearch_output.csv\n",
    "Returns hmmsearch_output.csv\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the PSIBLAST output was easy to parse, since there was always only one domain hit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"FOR PSIBLAST\"\"\"\n",
    "def parse_psiblast_output(input_file):\n",
    "    results = []\n",
    "    \n",
    "    with open(input_file, 'r') as f:\n",
    "        for line in f:\n",
    "            # Skip empty lines\n",
    "            if not line.strip():\n",
    "                continue\n",
    "                \n",
    "            # Split the line by tabs or multiple spaces\n",
    "            parts = re.split(r'\\s+', line.strip())\n",
    "            \n",
    "            if len(parts) >= 8:  # Make sure we have all required fields\n",
    "                query_id = parts[0]\n",
    "                subject_id = parts[1]\n",
    "                \n",
    "                # Extract UniProt ID and organism from subject_id\n",
    "                # Format is usually sp|UniprotID|Name\n",
    "                subject_parts = subject_id.split('|')\n",
    "                if len(subject_parts) >= 2:\n",
    "                    uniprot_id = subject_parts[1]\n",
    "                    \n",
    "                    # Create result dictionary\n",
    "                    result = {\n",
    "                        'protein_name': subject_id,\n",
    "                        'uniprot_id': uniprot_id,\n",
    "                        'organism': 'N/A',  # PSIBLAST output doesn't include organism\n",
    "                        'domain_start': int(parts[4]),  # sstart\n",
    "                        'domain_end': int(parts[5]),    # send\n",
    "                        'domain_length': int(parts[5]) - int(parts[4]) + 1,\n",
    "                        'E-value': float(parts[7])  \n",
    "                    }\n",
    "                    results.append(result)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def write_csv(results, output_file):\n",
    "    if not results:\n",
    "        return\n",
    "    # Notice that we skip the start & end positions in the query domain (i.e. the PSSM here), as we are only interested in where we found matches in the sequence of SwissProt we looked through\n",
    "    fieldnames = ['protein_name', 'uniprot_id', 'organism', 'domain_start', \n",
    "                 'domain_end', 'domain_length', 'E-value']\n",
    "    \n",
    "    with open(output_file, 'w', newline='') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(results)\n",
    "\n",
    "\n",
    "input_file = 'Model/Evaluation/Predictions/PSI-BLAST/psiblastsearch_output.txt'\n",
    "output_file = 'Model/Evaluation/Predictions/PSI-BLAST/psiblastsearch_output.csv'\n",
    "\n",
    "results = parse_psiblast_output(input_file)\n",
    "write_csv(results, output_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The HMM one was a bit more complicated in the sense that for some predicted proteins we had multiple domain hits. We decided to select only the domain hits with the lowest e-value, i.e. the ones with the highest significance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"FOR HMM\"\"\"\n",
    "# File paths\n",
    "input_file_path = \"Model/Evaluation/Predictions/HMM-SEARCH/hmmsearch_output.txt\"\n",
    "output_file_path = \"Model/Evaluation/Predictions/HMM-SEARCH/hmmsearch_output.csv\"\n",
    "\n",
    "# Initialize storage for parsed data\n",
    "parsed_data = []\n",
    "\n",
    "# Regular expressions to capture key information\n",
    "header_regex = r\">> ([^\\s]+)\"\n",
    "domain_regex = r\"\\s+(\\d+) [!?]\\s+[\\d\\.]+\\s+[\\d\\.]+\\s+[\\de\\.\\+\\-]+\\s+([\\de\\.\\+\\-]+)\\s+\\d+\\s+\\d+\\s+(?:\\[\\.|\\.\\.)+\\s+(\\d+)\\s+(\\d+)\"\n",
    "\n",
    "with open(input_file_path, \"r\") as infile:\n",
    "    current_protein = None\n",
    "\n",
    "    for line in infile:\n",
    "        # Match protein header line\n",
    "        header_match = re.match(header_regex, line)\n",
    "        if header_match:\n",
    "            # If we already captured a protein, save its data\n",
    "            if current_protein:\n",
    "                # Sort domains by E-value and keep only the best one (!)\n",
    "                if current_protein[\"domains\"]:\n",
    "                    current_protein[\"domains\"] = [min(current_protein[\"domains\"], key=lambda x: x[0])]\n",
    "                parsed_data.append(current_protein)\n",
    "\n",
    "            # Start a new protein record\n",
    "            protein_id = header_match.groups()[0]\n",
    "            current_protein = {\n",
    "                \"protein_name\": protein_id.split(\"|\")[2],\n",
    "                \"uniprot_id\": protein_id.split(\"|\")[1],\n",
    "                \"domains\": []\n",
    "            }\n",
    "\n",
    "        # Match domain annotation \n",
    "        domain_match = re.match(domain_regex, line)\n",
    "        if domain_match and current_protein:\n",
    "            _, score, start, end = domain_match.groups()\n",
    "            start, end, score = int(start), int(end), float(score)\n",
    "            length = end - start + 1\n",
    "            current_protein[\"domains\"].append((score, start, end, length))\n",
    "\n",
    "    # Handle the last protein record\n",
    "    if current_protein:\n",
    "        if current_protein[\"domains\"]:\n",
    "            current_protein[\"domains\"] = [min(current_protein[\"domains\"], key=lambda x: x[0])]\n",
    "        parsed_data.append(current_protein)\n",
    "\n",
    "# Define fixed fieldnames\n",
    "fieldnames = [\"protein_name\", \"uniprot_id\", \"E-value\", \"domain_start\", \"domain_end\", \"domain_length\"]\n",
    "\n",
    "# Write to CSV\n",
    "with open(output_file_path, \"w\", newline=\"\") as outfile:\n",
    "    writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for protein in parsed_data:\n",
    "        row = {\n",
    "            \"protein_name\": protein[\"protein_name\"],\n",
    "            \"uniprot_id\": protein[\"uniprot_id\"]\n",
    "        }\n",
    "        if protein[\"domains\"]:  # Check if there are any domains\n",
    "            # If there is, since we only keep the best domain, we can just take the first one\n",
    "            best_domain = protein[\"domains\"][0]  \n",
    "            row[\"E-value\"] = best_domain[0]\n",
    "            row[\"domain_start\"] = best_domain[1]\n",
    "            row[\"domain_end\"] = best_domain[2]\n",
    "            row[\"domain_length\"] = best_domain[3]\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Define your ground truth. Find all proteins in SwissProt annotated (and not annotated) with the assigned Pfam domain\n",
    "\n",
    "    - Collect the list of proteins matching the assigned Pfam domain\n",
    "\n",
    "    - Collect matching positions of the Pfam domain in the retrieved sequences. Domain positions are available [here](ftp://ftp.ebi.ac.uk/pub/databases/interpro/current/protein2ipr.dat.gz) (large tsv file) or using the [InterPro API](https://github.com/ProteinsWebTeam/interpro7-api/tree/master/docs) or align the Pfam domain yourself against SwissProt (HMMSEARCH)\n",
    "\n",
    "        - For this, we decided to use the InterPro API : \n",
    "\n",
    "        https://www.ebi.ac.uk/interpro/api/protein/reviewed/entry/pfam/PF00151/\n",
    "\n",
    "        which contains the reviewed proteins that match our assigned PFAM domain, PF00151\n",
    "\n",
    "        - to effectively scrape this website, we wrote a parser\n",
    "\n",
    "```bash\n",
    "    Returns pfam_domain_positions.json \n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching page 1...\n",
      "URL: https://www.ebi.ac.uk/interpro/api/protein/reviewed/entry/pfam/PF00151/\n",
      "API reports total count: 82\n",
      "Page 1 stats:\n",
      "- Proteins in response: 20\n",
      "- New unique proteins: 20\n",
      "- Duplicates found: 0\n",
      "\n",
      "Fetching page 2...\n",
      "URL: https://www.ebi.ac.uk/interpro/api/protein/reviewed/entry/pfam/PF00151/?cursor=source%3As%3Ap0dmb4\n",
      "Page 2 stats:\n",
      "- Proteins in response: 20\n",
      "- New unique proteins: 20\n",
      "- Duplicates found: 0\n",
      "\n",
      "Fetching page 3...\n",
      "URL: https://www.ebi.ac.uk/interpro/api/protein/reviewed/entry/pfam/PF00151/?cursor=source%3As%3Ap51528\n",
      "Page 3 stats:\n",
      "- Proteins in response: 20\n",
      "- New unique proteins: 20\n",
      "- Duplicates found: 0\n",
      "\n",
      "Fetching page 4...\n",
      "URL: https://www.ebi.ac.uk/interpro/api/protein/reviewed/entry/pfam/PF00151/?cursor=source%3As%3Aq5rbq5\n",
      "Page 4 stats:\n",
      "- Proteins in response: 20\n",
      "- New unique proteins: 20\n",
      "- Duplicates found: 0\n",
      "\n",
      "Fetching page 5...\n",
      "URL: https://www.ebi.ac.uk/interpro/api/protein/reviewed/entry/pfam/PF00151/?cursor=source%3As%3Aq9u6w0\n",
      "Page 5 stats:\n",
      "- Proteins in response: 2\n",
      "- New unique proteins: 2\n",
      "- Duplicates found: 0\n",
      "\n",
      "Final Statistics:\n",
      "Total unique proteins: 82\n",
      "Total duplicates found: 0\n",
      "Total processed entries: 82\n",
      "\n",
      "Results saved to Model/Evaluation/Ground Truth/pfam_domain_positions.json\n",
      "File contains 82 unique proteins\n"
     ]
    }
   ],
   "source": [
    "class InterProAPIFetcher:\n",
    "    def __init__(self, base_url: str):\n",
    "        self.base_url = base_url\n",
    "        self.processed_count = 0\n",
    "        self.all_results = []\n",
    "        self.seen_accessions = set()  # Track unique protein accessions\n",
    "        self.duplicate_count = 0\n",
    "\n",
    "    def fetch_page(self, url: str) -> Optional[Dict]:\n",
    "        max_retries = 3\n",
    "        retry_delay = 2\n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                response = requests.get(url)\n",
    "                response.raise_for_status()\n",
    "                return response.json()\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"Attempt {attempt + 1} failed: {str(e)}\")\n",
    "                if attempt < max_retries - 1:\n",
    "                    print(f\"Waiting {retry_delay} seconds before retrying...\")\n",
    "                    time.sleep(retry_delay)\n",
    "                    retry_delay *= 2\n",
    "                else:\n",
    "                    print(\"Max retries reached. Moving on...\")\n",
    "                    return None\n",
    "\n",
    "    def fetch_all_pages(self) -> List[Dict]:\n",
    "        next_url = self.base_url\n",
    "        total_count = None\n",
    "        page_number = 1\n",
    "        \n",
    "        while next_url:\n",
    "            print(f\"\\nFetching page {page_number}...\")\n",
    "            print(f\"URL: {next_url}\")\n",
    "            \n",
    "            page_data = self.fetch_page(next_url)\n",
    "            \n",
    "            if page_data is None:\n",
    "                print(\"Failed to fetch page. Stopping pagination.\")\n",
    "                break\n",
    "            \n",
    "            if total_count is None:\n",
    "                total_count = page_data['count']\n",
    "                print(f\"API reports total count: {total_count}\")\n",
    "            \n",
    "            # Check for duplicates in this page\n",
    "            new_proteins = []\n",
    "            page_duplicates = 0\n",
    "            \n",
    "            for protein in page_data['results']:\n",
    "                accession = protein['metadata']['accession']\n",
    "                if accession in self.seen_accessions:\n",
    "                    page_duplicates += 1\n",
    "                    self.duplicate_count += 1\n",
    "                else:\n",
    "                    self.seen_accessions.add(accession)\n",
    "                    new_proteins.append(protein)\n",
    "            \n",
    "            print(f\"Page {page_number} stats:\")\n",
    "            print(f\"- Proteins in response: {len(page_data['results'])}\")\n",
    "            print(f\"- New unique proteins: {len(new_proteins)}\")\n",
    "            print(f\"- Duplicates found: {page_duplicates}\")\n",
    "            \n",
    "            self.all_results.extend(new_proteins)\n",
    "            self.processed_count = len(self.all_results)\n",
    "            \n",
    "            next_url = page_data.get('next')\n",
    "            page_number += 1\n",
    "            \n",
    "            time.sleep(1)\n",
    "        \n",
    "        print(f\"\\nFinal Statistics:\")\n",
    "        print(f\"Total unique proteins: {len(self.all_results)}\")\n",
    "        print(f\"Total duplicates found: {self.duplicate_count}\")\n",
    "        print(f\"Total processed entries: {self.processed_count + self.duplicate_count}\")\n",
    "        \n",
    "        return self.all_results\n",
    "\n",
    "    def save_results(self, filename: str):\n",
    "        output_data = {\n",
    "            'count': len(self.all_results),\n",
    "            'results': self.all_results\n",
    "        }\n",
    "        \n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(output_data, f, indent=2)\n",
    "        print(f\"\\nResults saved to {filename}\")\n",
    "        print(f\"File contains {len(self.all_results)} unique proteins\")\n",
    "\n",
    "def main():\n",
    "    base_url = \"https://www.ebi.ac.uk/interpro/api/protein/reviewed/entry/pfam/PF00151/\"\n",
    "    \n",
    "    fetcher = InterProAPIFetcher(base_url)\n",
    "    fetcher.fetch_all_pages()\n",
    "    fetcher.save_results('Model/Evaluation/Ground Truth/pfam_domain_positions.json')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- After the successful parsing, we also turn the .json file into .csv for further processing (and again, easier interpretation by eye)\n",
    "\n",
    "For this we wrote another script \n",
    "```bash\n",
    "Returns pfam_domain_positions.csv\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results have been saved to 'Model/Evaluation/Ground Truth/pfam_domain_positions.csv'\n"
     ]
    }
   ],
   "source": [
    "def extract_pfam_info(json_file):\n",
    "    # Read the JSON file\n",
    "    with open(json_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # List to store the extracted information\n",
    "    pfam_matches = []\n",
    "    \n",
    "    # Iterate through each result in the JSON data\n",
    "    for result in data['results']:\n",
    "        # Extract protein metadata\n",
    "        protein_info = {\n",
    "            'protein_name': result['metadata']['name'],\n",
    "            'uniprot_id': result['metadata']['accession'],\n",
    "            'organism': result['metadata']['source_organism']['scientificName']\n",
    "        }\n",
    "        \n",
    "        # Extract PFAM domain information\n",
    "        # We know there's only one entry because we queried for a specific PFAM domain\n",
    "        pfam_entry = result['entries'][0]\n",
    "        \n",
    "        # Get the domain fragments (start and end positions)\n",
    "        for location in pfam_entry['entry_protein_locations']:\n",
    "            for fragment in location['fragments']:\n",
    "                domain_info = {\n",
    "                    **protein_info,  # Include all protein information\n",
    "                    'domain_start': fragment['start'],\n",
    "                    'domain_end': fragment['end'],\n",
    "                    'domain_length': fragment['end'] - fragment['start'] + 1,\n",
    "                    'protein_length': pfam_entry['protein_length'],\n",
    "                    'score': location['score']\n",
    "                }\n",
    "                pfam_matches.append(domain_info)\n",
    "    \n",
    "    return pfam_matches\n",
    "\n",
    "# Use the function to extract information\n",
    "json_file = 'Model/Evaluation/Ground Truth/pfam_domain_positions.json'\n",
    "matches = extract_pfam_info(json_file)\n",
    "\n",
    "\n",
    "# Convert the matches to a DataFrame\n",
    "df = pd.DataFrame(matches)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv('Model/Evaluation/Ground Truth/pfam_domain_positions.csv', index=False)\n",
    "print(\"\\nResults have been saved to 'Model/Evaluation/Ground Truth/pfam_domain_positions.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Finally, we now have 3 similar .csv files , 1 depicting the Ground Truth and 2 depicting the predictions from both the PSSM and HMM model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "3. Compare your model with the assigned Pfam. Calculate the precision, recall, F-score, balanced accuracy, MCC\n",
    "\n",
    "    - Comparison at the protein level. Measure the ability of your model to retrieve the same proteins matched by Pfam\n",
    "\n",
    "    - Comparison at the residue level. Measure the ability of your model to match the same position matched by Pfam\n",
    "\n",
    "\n",
    "- To this extent , we created a script for evaluation purposes\n",
    "\n",
    "    - The comparison on protein level is more or less trivial, as we just had to compare whether the protein was in the PFAM ones or not\n",
    "    - The comparison on residual level was a bit more complicated : We decided to create binary residue vector for the matched domain area in both the prediction and ground truth and then compared them to see where the vectors predicted the same areas and where not (see the code for more details)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===============================\n",
      "Evaluation only on found proteins in both PSSM/HMM:\n",
      "\n",
      "Evaluating all proteins:\n",
      "Number of proteins predicted by PSIBLAST: 83\n",
      "Number of proteins predicted by HMM: 83\n",
      "Number of proteins in Pfam ground truth: 82\n",
      "Number of proteins being evaluated for PSIBLAST: 83\n",
      "Number of proteins being evaluated for HMM: 83\n",
      "\n",
      "=== Protein-Level Evaluation ===\n",
      "\n",
      "Protein-Level Confusion Matrix PSIBLAST:\n",
      "True Positives: 82\n",
      "False Positives: 1\n",
      "False Negatives: 0\n",
      "True Negatives: 0\n",
      "\n",
      "Protein-Level Confusion Matrix HMM:\n",
      "True Positives: 82\n",
      "False Positives: 1\n",
      "False Negatives: 0\n",
      "True Negatives: 0\n",
      "\n",
      "Protein-Level Metrics PSIBLAST:\n",
      "Precision: 0.9880\n",
      "Recall: 1.0000\n",
      "F-score: 0.9939\n",
      "Balanced Accuracy: 0.5000\n",
      "MCC: 0.0000\n",
      "\n",
      "Protein-Level Metrics HMM:\n",
      "Precision: 0.9880\n",
      "Recall: 1.0000\n",
      "F-score: 0.9939\n",
      "Balanced Accuracy: 0.5000\n",
      "MCC: 0.0000\n",
      "\n",
      "=== Residue-Level Evaluation ===\n",
      "Number of proteins for residue-level evaluation PSIBLAST: 82\n",
      "Number of proteins for residue-level evaluation HMM: 82\n",
      "\n",
      "Residue-Level Confusion Matrix PSIBLAST:\n",
      "True Positives: 23341\n",
      "False Positives: 2157\n",
      "False Negatives: 1674\n",
      "True Negatives: 2357\n",
      "\n",
      "Residue-Level Metrics PSIBLAST:\n",
      "Precision: 0.9154\n",
      "Recall: 0.9331\n",
      "F-score: 0.9242\n",
      "Balanced Accuracy: 0.7276\n",
      "MCC: 0.4772\n",
      "\n",
      "Residue-Level Confusion Matrix HMM:\n",
      "True Positives: 22810\n",
      "False Positives: 5440\n",
      "False Negatives: 2205\n",
      "True Negatives: 2278\n",
      "\n",
      "Residue-Level Metrics HMM:\n",
      "Precision: 0.8074\n",
      "Recall: 0.9119\n",
      "F-score: 0.8565\n",
      "Balanced Accuracy: 0.6035\n",
      "MCC: 0.2556\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Based on the .csv files that closely resemble each other, do the calculations\"\"\"\n",
    "\n",
    "\n",
    "def create_residue_vectors(pred_df, pfam_df, protein_id):\n",
    "    \"\"\"\n",
    "    For a single protein (based on its protein_id), create residue vectors for the ground truth and the prediction to compare them\n",
    "    in their matching positions.\n",
    "    \"\"\"\n",
    " \n",
    "    # Find both the ground truth and the prediction for the given protein\n",
    "    pred_match = pred_df[pred_df['uniprot_id'] == protein_id]\n",
    "    pfam_match = pfam_df[pfam_df['uniprot_id'] == protein_id]\n",
    "    \n",
    "\n",
    "\n",
    "    # Get the max of the lengths of the domains of prediction and ground truth (i.e. the longest domain)\n",
    "    max_length = max(\n",
    "        pred_match['domain_end'].iloc[0],\n",
    "        pfam_match['domain_end'].iloc[0]\n",
    "    )\n",
    "    \n",
    "    # With that, we can create vectors of the same size\n",
    "    # Create binary vectors for each position from 0 to max_length\n",
    "    # Using this, we can directly compare the vectors for evaluation\n",
    "    true_positions = np.zeros(int(max_length))\n",
    "    pred_positions = np.zeros(int(max_length))\n",
    "    \n",
    "\n",
    "    # Fill in PFAM (true) positions\n",
    "    pfam_row = pfam_match.iloc[0]\n",
    "    start = pfam_row['domain_start'] - 1  # Convert to 0-based indexing\n",
    "    end = pfam_row['domain_end']\n",
    "    true_positions[start:end] = 1\n",
    "        \n",
    "    # Fill in PSSM/HMM (predicted) positions\n",
    "    pred_row = pred_match.iloc[0]\n",
    "    start = int(pred_row['domain_start'] - 1)  # Convert to 0-based indexing\n",
    "    end = int(pred_row['domain_end'])\n",
    "    pred_positions[start:end] = 1\n",
    "    \n",
    "    return true_positions, pred_positions\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_model(psiblast_file, hmm_file, pfam_file, only_found=False, e_threshold=0.0001):\n",
    "    \"\"\"\n",
    "    Evaluate PSIBLAST model performance against Pfam annotations\n",
    "    \n",
    "    Parameters:\n",
    "    - psiblast_file: Path to PSIBLAST results CSV\n",
    "    - hmm_file : Path to HMM results CSV\n",
    "    - pfam_file: Path to Pfam ground truth CSV\n",
    "    - only_found: If True, only evaluate proteins found by PSIBLAST\n",
    "    \"\"\"\n",
    "    # Step 1: Load both CSV files\n",
    "    psiblast_df = pd.read_csv(psiblast_file)\n",
    "    hmm_df = pd.read_csv(hmm_file)\n",
    "    pfam_df = pd.read_csv(pfam_file)\n",
    "\n",
    "    # HMM finds a lot of hits, a lot with extremely high e-values : Take only the ones that are above some threshold (score for now, later e-value)\n",
    "    # We filter based on the first domain hit \n",
    "    filtered_hmm_proteins = hmm_df[hmm_df['E-value'] <= e_threshold]['uniprot_id']\n",
    "\n",
    "    # Also PSIBLAST finds some hits with an high e-values so we filter them out\n",
    "    filtered_psiblast_proteins = psiblast_df[psiblast_df['E-value'] <= e_threshold]['uniprot_id']\n",
    "    \n",
    "    # Step 2: Get unique list of proteins from both files\n",
    "    psiblast_proteins = set(filtered_psiblast_proteins)\n",
    "    hmm_proteins = set(filtered_hmm_proteins)\n",
    "    pfam_proteins = set(pfam_df['uniprot_id'])\n",
    "\n",
    "    \n",
    "    if only_found:\n",
    "        # Only consider proteins that PSIBLAST/HMM found\n",
    "        all_proteins_psiblast = psiblast_proteins\n",
    "        all_proteins_hmm = hmm_proteins\n",
    "        print(\"\\nEvaluating only PSIBLAST-found proteins:\")\n",
    "    else:\n",
    "        # Consider all proteins from both sets\n",
    "        all_proteins_psiblast = psiblast_proteins.union(pfam_proteins)\n",
    "        all_proteins_hmm = hmm_proteins.union(pfam_proteins)\n",
    "        print(\"\\nEvaluating all proteins:\")\n",
    "    \n",
    "    print(f\"Number of proteins predicted by PSIBLAST: {len(psiblast_proteins)}\")\n",
    "    print(f\"Number of proteins predicted by HMM: {len(hmm_proteins)}\")\n",
    "    print(f\"Number of proteins in Pfam ground truth: {len(pfam_proteins)}\")\n",
    "    print(f\"Number of proteins being evaluated for PSIBLAST: {len(all_proteins_psiblast)}\")\n",
    "    print(f\"Number of proteins being evaluated for HMM: {len(all_proteins_hmm)}\")\n",
    "    \n",
    "\n",
    "    print(\"\\n=== Protein-Level Evaluation ===\")\n",
    "    # Step 3: Create binary vectors for true and predicted labels\n",
    "    y_true_psiblast = []  # Ground truth from Pfam\n",
    "    y_pred_psiblast = []  # Predictions from PSIBLAST\n",
    "    y_true_hmm = []\n",
    "    y_pred_hmm = []\n",
    "\n",
    "\n",
    "    \n",
    "    for protein in all_proteins_psiblast:\n",
    "        y_true_psiblast.append(1 if protein in pfam_proteins else 0)\n",
    "        y_pred_psiblast.append(1 if protein in psiblast_proteins else 0)\n",
    "\n",
    "\n",
    "    for protein in all_proteins_hmm:\n",
    "        y_true_hmm.append(1 if protein in pfam_proteins else 0)\n",
    "        y_pred_hmm.append(1 if protein in hmm_proteins else 0)\n",
    "\n",
    "    # So we have something like\n",
    "    # y_true_psiblast 0 0 1 0 1 ...\n",
    "    # y_pred_psiblast 0 1 1 0 1 ...\n",
    "    \n",
    "    # Step 4: Calculate performance metrics\n",
    "    protein_results_psiblast = {\n",
    "        'Precision': precision_score(y_true_psiblast, y_pred_psiblast),\n",
    "        'Recall': recall_score(y_true_psiblast, y_pred_psiblast),\n",
    "        'F-score': f1_score(y_true_psiblast, y_pred_psiblast),\n",
    "        'Balanced Accuracy': balanced_accuracy_score(y_true_psiblast, y_pred_psiblast),\n",
    "        'MCC': matthews_corrcoef(y_true_psiblast, y_pred_psiblast)\n",
    "    }\n",
    "\n",
    "\n",
    "    protein_results_hmm = {\n",
    "        'Precision': precision_score(y_true_hmm, y_pred_hmm),\n",
    "        'Recall': recall_score(y_true_hmm, y_pred_hmm),\n",
    "        'F-score': f1_score(y_true_hmm, y_pred_hmm),\n",
    "        'Balanced Accuracy': balanced_accuracy_score(y_true_hmm, y_pred_hmm),\n",
    "        'MCC': matthews_corrcoef(y_true_hmm, y_pred_hmm)\n",
    "    }\n",
    "\n",
    "\n",
    "    # Step 5: Calculate confusion matrix components\n",
    "    tp_psiblast = sum(1 for t, p in zip(y_true_psiblast, y_pred_psiblast) if t == 1 and p == 1)\n",
    "    fp_psiblast = sum(1 for t, p in zip(y_true_psiblast, y_pred_psiblast) if t == 0 and p == 1)\n",
    "    fn_psiblast = sum(1 for t, p in zip(y_true_psiblast, y_pred_psiblast) if t == 1 and p == 0)\n",
    "    tn_psiblast = sum(1 for t, p in zip(y_true_psiblast, y_pred_psiblast) if t == 0 and p == 0)\n",
    "\n",
    "\n",
    "    tp_hmm = sum(1 for t, p in zip(y_true_hmm, y_pred_hmm) if t == 1 and p == 1)\n",
    "    fp_hmm = sum(1 for t, p in zip(y_true_hmm, y_pred_hmm) if t == 0 and p == 1)\n",
    "    fn_hmm = sum(1 for t, p in zip(y_true_hmm, y_pred_hmm) if t == 1 and p == 0)\n",
    "    tn_hmm = sum(1 for t, p in zip(y_true_hmm, y_pred_hmm) if t == 0 and p == 0)\n",
    "    \n",
    "    # Print detailed results\n",
    "    print(\"\\nProtein-Level Confusion Matrix PSIBLAST:\")\n",
    "    print(f\"True Positives: {tp_psiblast}\")\n",
    "    print(f\"False Positives: {fp_psiblast}\")\n",
    "    print(f\"False Negatives: {fn_psiblast}\")\n",
    "    print(f\"True Negatives: {tn_psiblast}\")\n",
    "\n",
    "\n",
    "\n",
    "    print(\"\\nProtein-Level Confusion Matrix HMM:\")\n",
    "    print(f\"True Positives: {tp_hmm}\")\n",
    "    print(f\"False Positives: {fp_hmm}\")\n",
    "    print(f\"False Negatives: {fn_hmm}\")\n",
    "    print(f\"True Negatives: {tn_hmm}\")\n",
    "\n",
    "\n",
    "    print(\"\\nProtein-Level Metrics PSIBLAST:\")\n",
    "    for metric, value in protein_results_psiblast.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "\n",
    "    print(\"\\nProtein-Level Metrics HMM:\")\n",
    "    for metric, value in protein_results_hmm.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "    # Residue-level evaluation\n",
    "   \n",
    "    print(\"\\n=== Residue-Level Evaluation ===\")\n",
    "    # Only evaluate residues for proteins found in both sets \n",
    "    common_proteins_psiblast = psiblast_proteins.intersection(pfam_proteins)\n",
    "    common_proteins_hmm = hmm_proteins.intersection(pfam_proteins)\n",
    "    print(f\"Number of proteins for residue-level evaluation PSIBLAST: {len(common_proteins_psiblast)}\")\n",
    "    print(f\"Number of proteins for residue-level evaluation HMM: {len(common_proteins_hmm)}\")\n",
    "    \n",
    "    # Collect all residue-level predictions\n",
    "    all_true_residues_psiblast = []\n",
    "    all_pred_residues_psiblast = []\n",
    "\n",
    "    all_true_residues_hmm = []\n",
    "    all_pred_residues_hmm = []\n",
    "\n",
    "    \n",
    "    for protein in common_proteins_psiblast:\n",
    "        result = create_residue_vectors(psiblast_df, pfam_df, protein)\n",
    "        if result is not None:\n",
    "            true_pos, pred_pos = result\n",
    "            all_true_residues_psiblast.extend(true_pos)\n",
    "            all_pred_residues_psiblast.extend(pred_pos)\n",
    "\n",
    "\n",
    "    for protein in common_proteins_hmm:\n",
    "        result = create_residue_vectors(hmm_df, pfam_df, protein)\n",
    "        if result is not None:\n",
    "            true_pos, pred_pos = result\n",
    "            all_true_residues_hmm.extend(true_pos)\n",
    "            all_pred_residues_hmm.extend(pred_pos)\n",
    "    \n",
    "    # Calculate residue-level metrics\n",
    "    residue_results_psiblast = {\n",
    "        'Precision': precision_score(all_true_residues_psiblast, all_pred_residues_psiblast),\n",
    "        'Recall': recall_score(all_true_residues_psiblast, all_pred_residues_psiblast),\n",
    "        'F-score': f1_score(all_true_residues_psiblast, all_pred_residues_psiblast),\n",
    "        'Balanced Accuracy': balanced_accuracy_score(all_true_residues_psiblast, all_pred_residues_psiblast),\n",
    "        'MCC': matthews_corrcoef(all_true_residues_psiblast, all_pred_residues_psiblast)\n",
    "    }\n",
    "    \n",
    "    # Calculate residue-level confusion matrix\n",
    "    tp = sum(1 for t, p in zip(all_true_residues_psiblast, all_pred_residues_psiblast) if t == 1 and p == 1)\n",
    "    fp = sum(1 for t, p in zip(all_true_residues_psiblast, all_pred_residues_psiblast) if t == 0 and p == 1)\n",
    "    fn = sum(1 for t, p in zip(all_true_residues_psiblast, all_pred_residues_psiblast) if t == 1 and p == 0)\n",
    "    tn = sum(1 for t, p in zip(all_true_residues_psiblast, all_pred_residues_psiblast) if t == 0 and p == 0)\n",
    "    \n",
    "    print(\"\\nResidue-Level Confusion Matrix PSIBLAST:\")\n",
    "    print(f\"True Positives: {tp}\")\n",
    "    print(f\"False Positives: {fp}\")\n",
    "    print(f\"False Negatives: {fn}\")\n",
    "    print(f\"True Negatives: {tn}\")\n",
    "    \n",
    "    print(\"\\nResidue-Level Metrics PSIBLAST:\")\n",
    "    for metric, value in residue_results_psiblast.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "    # Calculate residue-level metrics\n",
    "    residue_results_hmm = {\n",
    "        'Precision': precision_score(all_true_residues_hmm, all_pred_residues_hmm),\n",
    "        'Recall': recall_score(all_true_residues_hmm, all_pred_residues_hmm),\n",
    "        'F-score': f1_score(all_true_residues_hmm, all_pred_residues_hmm),\n",
    "        'Balanced Accuracy': balanced_accuracy_score(all_true_residues_hmm, all_pred_residues_hmm),\n",
    "        'MCC': matthews_corrcoef(all_true_residues_hmm, all_pred_residues_hmm)\n",
    "    }\n",
    "    \n",
    "    # Calculate residue-level confusion matrix\n",
    "    tp = sum(1 for t, p in zip(all_true_residues_hmm, all_pred_residues_hmm) if t == 1 and p == 1)\n",
    "    fp = sum(1 for t, p in zip(all_true_residues_hmm, all_pred_residues_hmm) if t == 0 and p == 1)\n",
    "    fn = sum(1 for t, p in zip(all_true_residues_hmm, all_pred_residues_hmm) if t == 1 and p == 0)\n",
    "    tn = sum(1 for t, p in zip(all_true_residues_hmm, all_pred_residues_hmm) if t == 0 and p == 0)\n",
    "\n",
    "\n",
    "    print(\"\\nResidue-Level Confusion Matrix HMM:\")\n",
    "    print(f\"True Positives: {tp}\")\n",
    "    print(f\"False Positives: {fp}\")\n",
    "    print(f\"False Negatives: {fn}\")\n",
    "    print(f\"True Negatives: {tn}\")\n",
    "    \n",
    "    print(\"\\nResidue-Level Metrics HMM:\")\n",
    "    for metric, value in residue_results_hmm.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "    \n",
    "\n",
    "psiblast_file = 'Model/Evaluation/Predictions/PSI-BLAST/psiblastsearch_output.csv'\n",
    "hmm_file = 'Model/Evaluation/Predictions/HMM-SEARCH/hmmsearch_output.csv'\n",
    "pfam_file = 'Model/Evaluation/Ground Truth/pfam_domain_positions.csv'\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n===============================\")\n",
    "print(\"Evaluation only on found proteins in both PSSM/HMM:\")\n",
    "evaluate_model(psiblast_file,hmm_file, pfam_file, only_found=False, e_threshold= 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Consider refining your models to improve their performance\n",
    "\n",
    "- Multiple Steps that we took\n",
    "\n",
    "    - remove redundancy at different thresholds in JalView\n",
    "        - but we saw that at 100% already removal of around 80% most of the times, so we just removed at 100% threshold (which is the lowest in this case) to not have too little sequences to build the model\n",
    "    - use other starting database\n",
    "        - we tried using UniRef90 first, but got much less initial homologous sequences (with the same settings, so e-value 0.0001 etc.), as this resulted in weaker model performance, we wanted to change to a bigger database and therefore chose UniProtKB for the final model\n",
    "    - when removing columns, tweak the two parameters that we had differently to see what gives better results\n",
    "        - tried different things (still TODO) ; using the 3 parameters (3 thresholds) we have : aim was to remove a good amount of columns that basically were only gaps (thats why we use the gap threshold) and in the other columns that were aligned already, to not have completely random amino acids (i.e. tweaking of entropy)\n",
    "    - and here then say we found our best model using\n",
    "        - TODO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Domain family characterization\n",
    "Once the family model is defined (previous step), you will look at functional (and structural) aspects/properties of the entire protein family. The objective is to provide insights about the main function of the family."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taxonomy\n",
    "\n",
    "1. Collect the taxonomic lineage (tree branch) for each protein of the family_sequences dataset\n",
    "from UniProt (entity/organism/lineage in the UniProt XML)\n",
    "\n",
    "- TODO : it says here we have to do it in the UniProt XML and we did it by parsing the website, ask in email if that gives the same result\n",
    "\n",
    "2. Plot the taxonomic tree of the family with nodes size proportional to their relative abundance \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_protein_ids(psiblast_file, hmm_file, e_threshold=0.001):\n",
    "    \"\"\"Load protein IDs from PSI-BLAST and HMM search results.\"\"\"\n",
    "    psiblast_df = pd.read_csv(psiblast_file)\n",
    "    hmm_df = pd.read_csv(hmm_file)\n",
    "    \n",
    "    filtered_hmm_proteins = hmm_df[hmm_df['E-value'] <= e_threshold]['uniprot_id']\n",
    "    psiblast_proteins = set(psiblast_df['uniprot_id'])\n",
    "    hmm_proteins = set(filtered_hmm_proteins)\n",
    "    \n",
    "    return list(psiblast_proteins.union(hmm_proteins))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     P06857\n",
      "1     P54316\n",
      "2     Q5BKQ4\n",
      "3     P16233\n",
      "4     P54315\n",
      "       ...  \n",
      "78    P02844\n",
      "79    P27587\n",
      "80    P0DSI2\n",
      "81    Q68KK0\n",
      "82    P83629\n",
      "Name: uniprot_id, Length: 83, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Q02157: 100%|| 83/83 [00:52<00:00,  1.59it/s]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taxonomy file saved to: Taxonomy/taxonomy_info.csv\n",
      "Tree saved to: Taxonomy/phylogenetic_tree_freq.png\n"
     ]
    }
   ],
   "source": [
    "# TaxonomyAnalyzer Class for fetching taxonomy information\n",
    "class TaxonomyAnalyzer:\n",
    "    def __init__(self, max_retries: int = 3, retry_delay: int = 1):\n",
    "        self.max_retries = max_retries\n",
    "        self.retry_delay = retry_delay\n",
    "        self.uniprot_base_url = \"https://rest.uniprot.org/uniprotkb/\"\n",
    "\n",
    "    def fetch_taxonomy_info(self, protein_ids: list, output_file: str):\n",
    "        \"\"\" Get the lineage data by parsing UniProt for a list of protein IDs (our family sequences) and save to a CSV file \"\"\"\n",
    "        taxonomy_data = []\n",
    "\n",
    "        pbar = tqdm(protein_ids, desc=\"Fetching taxonomy data\")\n",
    "        \n",
    "        # Iterate through each protein of our family\n",
    "        for protein_id in pbar:\n",
    "            pbar.set_description(f\"Processing {protein_id}\")\n",
    "\n",
    "            for attempt in range(self.max_retries):\n",
    "                try:\n",
    "                    # Fetch data from UniProt API\n",
    "                    response = requests.get(f\"{self.uniprot_base_url}{protein_id}.json\")\n",
    "                    response.raise_for_status()\n",
    "                    data = response.json()\n",
    "\n",
    "                    # Get lineage information \n",
    "                    taxonomy = data.get(\"organism\", {})\n",
    "                    scientific_name = taxonomy.get(\"scientificName\", \"N/A\")\n",
    "                    lineage = taxonomy.get(\"lineage\", [])\n",
    "\n",
    "                    taxonomy_data.append([protein_id, scientific_name, \" > \".join(lineage)])\n",
    "                    break\n",
    "\n",
    "                # Error Catching\n",
    "                except requests.exceptions.RequestException as e:\n",
    "                    print(f\"Error fetching data for {protein_id}: {e}\")\n",
    "                    if attempt == self.max_retries - 1:\n",
    "                        taxonomy_data.append([protein_id, \"Error\", \"\"])\n",
    "                    else:\n",
    "                        time.sleep(self.retry_delay)\n",
    "\n",
    "        # Save to CSV\n",
    "        taxonomy_df = pd.DataFrame(taxonomy_data, columns=[\"Protein ID\", \"Scientific Name\", \"Lineage\"])\n",
    "        taxonomy_df.to_csv(output_file, index=False)\n",
    "        return taxonomy_df\n",
    "\n",
    "# Process taxonomy data\n",
    "def process_taxonomy(data):\n",
    "    \"\"\" Process lineage data into a nested dictionary and frequency counts \"\"\"\n",
    "    taxonomy_dict = {}\n",
    "    frequency_counts = {}\n",
    "    \n",
    "    for _, row in data.iterrows():\n",
    "        # Split the lineage by \" > \" to get each level\n",
    "        lineage = row[\"Lineage\"].split(\" > \")\n",
    "        current = taxonomy_dict\n",
    "        # Keep track of the full path as we traverse the lineage\n",
    "        current_path = [] # such that we count occurences of terms in the correct \"level\" where they appear (i.e. always count just in the \"column\" of the linage)\n",
    "        \n",
    "        # Iterate through each level in the lineage\n",
    "        for level in lineage:\n",
    "            current_path.append(level)\n",
    "            path_key = \" > \".join(current_path)\n",
    "            \n",
    "            # Count frequencies using the full path as key\n",
    "            if path_key not in frequency_counts:\n",
    "                frequency_counts[path_key] = 0\n",
    "            frequency_counts[path_key] += 1\n",
    "            \n",
    "            if level not in current:\n",
    "                current[level] = {}\n",
    "            # Set current to be at the next level\n",
    "            current = current[level]\n",
    "    \n",
    "    return taxonomy_dict, frequency_counts\n",
    "\n",
    "# Create a Newick string for the taxonomy tree for representation purposes\n",
    "def dict_to_newick(d):\n",
    "    \"\"\" Convert dictionary to Newick format string \"\"\"\n",
    "    newick = \"\"\n",
    "    for key, sub_dict in d.items():\n",
    "        sub_tree = dict_to_newick(sub_dict)\n",
    "        newick += f\"({sub_tree}){key},\" if sub_tree else f\"{key},\"\n",
    "    return newick.rstrip(\",\")\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    psiblast_file = \"Model/Evaluation/Predictions/PSI-BLAST/psiblastsearch_output.csv\"\n",
    "    hmm_file = \"Model/Evaluation/Predictions/HMM-SEARCH/hmmsearch_output.csv\"\n",
    "    protein_ids = load_protein_ids(psiblast_file, hmm_file)\n",
    "\n",
    "    analyzer = TaxonomyAnalyzer()\n",
    "    taxonomy_data = analyzer.fetch_taxonomy_info(protein_ids, \"Taxonomy/taxonomy_info.csv\")\n",
    "\n",
    "    print(\"Taxonomy file saved to: Taxonomy/taxonomy_info.csv\")\n",
    "\n",
    "\n",
    "    # Create a nested dictionary of lineage data and frequency counts (of occurences of each term in the Lineage)\n",
    "    taxonomy_dict, frequency_counts = process_taxonomy(taxonomy_data)\n",
    "\n",
    "    # Create the newick tree based on the lineage data \n",
    "    newick_tree = f\"({dict_to_newick(taxonomy_dict)});\"\n",
    "\n",
    "    # Plot using ETE Toolkit\n",
    "    phylo_tree = Tree(newick_tree, format=1)\n",
    "    tree_style = TreeStyle()\n",
    "    tree_style.show_leaf_name = False\n",
    "\n",
    "\n",
    "    # Adjust node sizes \n",
    "    max_size = 12  # Increase max size for nodes for visibility\n",
    "    for node in phylo_tree.traverse():\n",
    "        # Get the full path from root to this node\n",
    "        path = []\n",
    "        current = node\n",
    "        while current:\n",
    "            if current.name:  # Skip empty names\n",
    "                path.insert(0, current.name)\n",
    "            current = current.up\n",
    "        \n",
    "        path_key = \" > \".join(path)\n",
    "        count = frequency_counts.get(path_key, 1)\n",
    "        nstyle = NodeStyle()\n",
    "        # Set node sizes based on relative abundance\n",
    "        nstyle[\"size\"] = min(count, max_size)  \n",
    "        node.set_style(nstyle)\n",
    "        # Add label with name and count\n",
    "        node.add_face(TextFace(f\"{node.name} ({count})\", fsize=10), column=0)\n",
    "\n",
    "    # Improve tree spacing\n",
    "    tree_style.branch_vertical_margin = 30  # Increase spacing for better visibility\n",
    "\n",
    "    # Save the tree to PNG file\n",
    "    output_file = \"Taxonomy/phylogenetic_tree_freq.png\"\n",
    "    phylo_tree.render(output_file, w=3000, h=2000, tree_style=tree_style)\n",
    "\n",
    "    print(f\"Tree saved to: {output_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function\n",
    "\n",
    "1. Collect GO annotations for each protein of the family_sequences dataset (entity/dbReference type=\"GO\" in the UniProt XML)\n",
    "\n",
    "- For this, we did two steps\n",
    "    - First, find the \"direct\" GO annotations of the proteins (i.e. no ancestors, just the ones found in the UniProt XML directly for the current protein id) using fetch_go_annotations()\n",
    "    - Then, we expanded the found GO terms with their ancestors using expand_go_terms_with_ancestors(), which takes all the found GO terms (i.e. their IDs), and based on that parses up the whole ontology tree to find the ancestors of that GO term aswell \n",
    "- TODO : maybe create .csv file here too so we also have the results of this stored\n",
    "\n",
    "2. Calculate the enrichment of each term in the dataset compared to GO annotations available in the SwissProt database (you can download the entire SwissProt XML [here](ftp://ftp.uniprot.org/pub/databases/uniprot/current_release/knowledgebase/complete/uniprot_sprot.xml.gz)). You can use Fisher exact test and verify that both two-tails and right-tail P-values (or left-tail depending on how you build the confusion matrix) are close to zero\n",
    "\n",
    "- For this, also multiple steps\n",
    "    - First again parsing, just this time on the entire SwissProtXML using parse_swissprot_go_terms()\n",
    "        - Notice that in this step, we skip over the family proteins such that later on it will be easier to calculate the contingency table\n",
    "          for Fishers exact Test ; for calculating the metrics, we can then simply add these family proteins again \n",
    "    - Then again, expand the GO-terms using expand_go_terms_with_ancestors()\n",
    "    - Now, we could calculate the actual enrichment\n",
    "        - Note that we also use a function to get the actual GO terms from the GO ids using the helper function get_go_terms_given_goid()\n",
    "        - Furthermore, as both our family and full SwissProt Parser created dictionaries of the form protein_id : [go_ids], we also employ a helper function to reverse this dictionary into go_id : [protein_ids], since this structure simplifies the calculations of Fishers Exact Test\n",
    "    - We calculate some additional metrics for the enrichment, see the code (i.e all the things in the .csv)\n",
    "\n",
    "Results in enrichment_results.csv\n",
    "\n",
    "3. Plot enriched terms in a word cloud \n",
    "    - Plot a word cloud based on the enriched terms, i.e. the ones with two-tail & right tail P-value <0.05\n",
    "    - To weight the terms in the word cloud, we use the proportion of the percentage that GO term had in our dataset to the percentage the GO term had in the entirety of SwissProt  \n",
    "\n",
    "Results in go_enrichment_wordcloud.png\n",
    "\n",
    "4. Take into consideration the hierarchical structure of the GO ontology and report most significantly enriched branches, i.e. high level terms\n",
    "    - For each GO-Id in the family, we parsed up the whole tree such that we found all ancestors of that GO-Id and added an \"enriched child\" to each of these ancestors, i.e. the initial GO-term for the search itself was the enriched child (the \"enrichment\" was weighted by the two-sided p-value we obtained earlier).\n",
    "    - After that, we then printed out the top 20 branches that had atleast 2 enriched children and were somewhere high up in the tree (i.e. not deeper than depth 3)\n",
    "\n",
    "5. Always report the full name of the terms and not only the GO ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_protein_go_dict(protein_to_go):\n",
    "   \"\"\"\n",
    "   Convert protein id : [GO ids] dict to GO id  : [protein ids] dict.\n",
    "   Args:\n",
    "         protein_to_go (dict): Protein ID to GO id dictionary\n",
    "\n",
    "    Returns:\n",
    "         dict: GO ID to protein IDs dictionary\n",
    "   \n",
    "   \"\"\"\n",
    "   go_to_proteins = defaultdict(list)\n",
    "   for protein, go_terms in protein_to_go.items():\n",
    "       for go_term in go_terms:\n",
    "           go_to_proteins[go_term].append(protein)\n",
    "   return go_to_proteins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_go_terms_with_ancestors(protein_to_go, go_obo):\n",
    "    \"\"\"\n",
    "    Expand GO IDs with their ancestors.\n",
    "    \n",
    "    Args:\n",
    "        protein_to_go (dict): Dictionary mapping GO ids to Protein ID.\n",
    "        go_obo (GODag): Parsed GO DAG for ancestor retrieval.\n",
    "        \n",
    "    Returns:\n",
    "        dict: Updated protein_to_go dictionary with expanded GO IDs.\n",
    "    \"\"\"\n",
    "\n",
    "    expanded_protein_to_go = defaultdict(set)\n",
    "    \n",
    "    for protein, go_terms in protein_to_go.items():\n",
    "        for go_id in go_terms:\n",
    "            # Add the GO ID itself\n",
    "            expanded_protein_to_go[protein].add(go_id)\n",
    "            \n",
    "            # Add all ancestor terms (i.e their IDs) of that GO ID \n",
    "            if go_id in go_obo:\n",
    "                ancestors = go_obo[go_id].get_all_parents()  # Get all ancestors\n",
    "                expanded_protein_to_go[protein].update(ancestors)\n",
    "    \n",
    "   \n",
    "    return {protein: list(go_terms) for protein, go_terms in expanded_protein_to_go.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_go_terms_given_goid(protein_go_dict: Dict[str, List[str]]) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Convert GO IDs to their corresponding terms using the Gene Ontology API.\n",
    "    \n",
    "    Args:\n",
    "        protein_go_dict (dict) : Dictionary mapping protein IDs to lists of GO IDs\n",
    "        \n",
    "    Returns:\n",
    "        dict : Dictionary mapping GO IDs to their terms\n",
    "        \n",
    "\n",
    "    \"\"\"\n",
    "    # Extract unique GO IDs from the dictionary\n",
    "    go_ids = set()\n",
    "    for go_list in protein_go_dict.values():\n",
    "        go_ids.update(go_list)\n",
    "    \n",
    "    # Initialize results dictionary\n",
    "    go_terms_dict = {}\n",
    "    \n",
    "    # Base URL for the Gene Ontology API\n",
    "    base_url = \"http://api.geneontology.org/api/ontology/term/\"\n",
    "    \n",
    "    # Process each GO ID\n",
    "    for go_id in go_ids:\n",
    "        try:\n",
    "            # Add delay \n",
    "            time.sleep(0.1)\n",
    "            \n",
    "            # Make API request using the Gene Ontology API, i.e. search the API based on the current GO ID\n",
    "            response = requests.get(f\"{base_url}{go_id}\")\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # Extract the corresponding GO term\n",
    "            data = response.json()\n",
    "            go_terms_dict[go_id] = data.get('label', 'Term not found')\n",
    "        \n",
    "        # Some error catching \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error fetching term for {go_id}: {str(e)}\")\n",
    "            go_terms_dict[go_id] = 'Error fetching term'\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error processing {go_id}: {str(e)}\")\n",
    "            go_terms_dict[go_id] = 'Error processing term'\n",
    "    \n",
    "    return go_terms_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_go_annotations(protein_id):\n",
    "    \"\"\"\n",
    "    Fetch GO annotations and create GO ID to protein list mapping.\n",
    "    \n",
    "    Args:\n",
    "        protein_id (str): single protein ID of our family \n",
    "        \n",
    "    Returns:\n",
    "        List : List of the GO ids found for that protein\n",
    "    \"\"\"\n",
    "    go_ids = []\n",
    "    \n",
    "\n",
    "    # Base URL for the UniProt API\n",
    "    url = f\"https://rest.uniprot.org/uniprotkb/{protein_id}.xml\"\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        namespaces = {'ns': 'http://uniprot.org/uniprot'}\n",
    "        root = ET.fromstring(response.content)\n",
    "        \n",
    "        # Get all GO IDs for the protein\n",
    "        for db_ref in root.findall(\".//ns:dbReference[@type='GO']\", namespaces):\n",
    "            go_id = db_ref.attrib.get('id')\n",
    "            \n",
    "            if go_id:\n",
    "                go_ids.append(go_id)\n",
    "    \n",
    "    # Some error catching\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching GO annotations for {protein_id}: {e}\")\n",
    "   \n",
    "            \n",
    "    return go_ids\n",
    "\n",
    "\n",
    "# Let's add some debugging to help understand what's happening\n",
    "# here we see that the big .xml file has the same structure as the small ones \n",
    "# we already analyzed ; thus,we can use the same parsing structure, but this time directly\n",
    "# just collect the counts of GO terms, because that is all we need (no diff. categories, would just make our code slower)\n",
    "def print_swissprot_file(swissprot_xml_path, length = 50):\n",
    "    \"\"\"\n",
    "    Just to look at the first few lines to see the structure\n",
    "    \"\"\"\n",
    "\n",
    "    with open(swissprot_xml_path, 'r') as f:\n",
    "        print(\"First length lines of the file:\")\n",
    "        for i, line in enumerate(f):\n",
    "            if i < length:\n",
    "                print(line.strip())\n",
    "            else:\n",
    "                break\n",
    "\n",
    "\n",
    "\n",
    "def parse_swissprot_go_terms(swissprot_xml_path, family_proteins):\n",
    "   \"\"\"\n",
    "   Parse GO IDs from SwissProt XML file for each protein, excluding proteins in the family.\n",
    "   \n",
    "   Args:\n",
    "       swissprot_xml_path (str): Path to SwissProt XML file\n",
    "       family_proteins (set): UniProt IDs in protein family\n",
    "   \n",
    "   Returns:\n",
    "       dict: protein ID : [GO IDs] for that protein\n",
    "   \"\"\"\n",
    "   protein_to_go = defaultdict(list)\n",
    "   total_proteins = 0\n",
    "   skipped_proteins = 0\n",
    "   \n",
    "   namespaces = {'ns': 'http://uniprot.org/uniprot'}\n",
    "   context = ET.iterparse(swissprot_xml_path, events=('end',))\n",
    "   \n",
    "   print(\"Starting to parse SwissProt XML...\")\n",
    "   \n",
    "   for event, elem in context:\n",
    "       if elem.tag.endswith('entry'):\n",
    "           accession = elem.find(\".//ns:accession\", namespaces)\n",
    "           if accession is not None:\n",
    "               uniprot_id = accession.text\n",
    "               \n",
    "               # Exclude family proteins\n",
    "               if uniprot_id in family_proteins:\n",
    "                   skipped_proteins += 1\n",
    "               else:\n",
    "                   # Get all GO IDs for the protein (same structure as in fetch_go_annotations)\n",
    "                   for db_ref in elem.findall(\".//ns:dbReference[@type='GO']\", namespaces):\n",
    "                       go_id = db_ref.attrib.get('id')\n",
    "                       if go_id:\n",
    "                           protein_to_go[uniprot_id].append(go_id)\n",
    "                   total_proteins += 1\n",
    "\n",
    "           elem.clear()\n",
    "           \n",
    "           # Keep track of progress, as it takes some time to parse the whole file\n",
    "           if (total_proteins + skipped_proteins) % 10000 == 0:\n",
    "               print(f\"Processed {total_proteins} proteins \"\n",
    "                     f\"(skipped {skipped_proteins} family proteins)...\")\n",
    "             \n",
    "    \n",
    "               \n",
    "                    \n",
    "               \n",
    "   return protein_to_go\n",
    "\n",
    "def calculate_go_enrichment(go_to_proteins_family, go_to_proteins_swissprot, total_proteins_family, total_proteins_swissprot, go_id_to_go_term):\n",
    "    \"\"\" \n",
    "    Perform Fisher's exact test to calculate GO term enrichment in our protein family.\n",
    "\n",
    "    Args:\n",
    "        go_to_proteins_family (dict): GO ID to list of proteins in family\n",
    "        go_to_proteins_swissprot (dict): GO ID to list of proteins in SwissProt\n",
    "        total_proteins_family (int): Total proteins in family\n",
    "        total_proteins_swissprot (int): Total proteins in SwissProt\n",
    "        go_id_to_go_term (dict): GO ID to GO term mapping\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with GO term enrichment results\n",
    "\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    \n",
    "    for go_id in go_to_proteins_family.keys():\n",
    "   \n",
    "        # Create the 2x2 contingency table for Fisher's exact test\n",
    "        # The table looks like this:\n",
    "        #                   Protein in family    Protein not in family (i.e. all in SwissProt - family proteins)\n",
    "        # Has GO term            a                    b\n",
    "        # No GO term             c                    d\n",
    "        \n",
    "        # Contingency table calculations:\n",
    "        a = len(go_to_proteins_family[go_id])  # Proteins with this GO term in family\n",
    "        \n",
    "      \n",
    "        b = len(go_to_proteins_swissprot.get(go_id, []))  # Proteins with GO term in rest of SwissProt (without family)\n",
    "        \n",
    "        c = total_proteins_family - a  # Proteins without GO term in family\n",
    "        \n",
    "  \n",
    "        d = total_proteins_swissprot - b # Proteins without GO term in rest of SwissProt (without family)\n",
    "        \n",
    "        # Verify all values are non-negative before creating contingency table\n",
    "        if all(x >= 0 for x in [a, b, c, d]):\n",
    "            contingency_table = [[a, b], [c, d]]\n",
    "            \n",
    "            # Perform Fisher's exact test\n",
    "            # We ask : is the GO term appearing more often in our family than we would expect by random chance ?\n",
    "            # The null hypothesis (H0) is: \"The proportion of proteins with this GO term in our family \n",
    "            # is the same as the proportion in the SwissProt dataset (without the protein in the family).\" \n",
    "            # In other words, under H0, getting the GO term is independent of being in our family (so it doesn't represent the family)\n",
    "            # Alternative Hypothesis (H1) using the right-tail and two-tail:\n",
    "            #Right-tail (greater): Our family has a higher proportion of this GO term than SwissProt\n",
    "            #Two-tail (two-sided): The proportion is different (either higher or lower)\n",
    "\n",
    "            # ((((Left-tail (less): Our family has a lower proportion of this GO term than SwissProt)))))\n",
    "\n",
    "            #Fisher's exact test calculates the probability of seeing our observed data (or more extreme) under the null hypothesis.\n",
    "            #A very small p-value (like < 0.05) tells us:\n",
    "            #Two-tail: This GO term's frequency is significantly different from SwissProt\n",
    "            #Right-tail: This GO term is significantly enriched in our family(overrepresented)\n",
    "\n",
    "\n",
    "            #(((((Left-tail: This GO term is significantly depleted in our family(underrepresented)))))))\n",
    "\n",
    "            odds_ratio, pvalue_two_tail = fisher_exact(contingency_table, alternative='two-sided')\n",
    "            _, pvalue_greater = fisher_exact(contingency_table, alternative='greater')\n",
    "          #  _, pvalue_less = fisher_exact(contingency_table, alternative='less')\n",
    "            \n",
    "            # Calculate proportions\n",
    "            my_proportion = a / total_proteins_family \n",
    "            swissprot_proportion = (a+b) / (total_proteins_swissprot + total_proteins_family)\n",
    "\n",
    "     \n",
    "            \n",
    "            results.append({\n",
    "                'GO_ID': go_id,\n",
    "                'GO_Term': go_id_to_go_term.get(go_id, 'N/A'), # Include GO term name\n",
    "                'Count_Prot_Dataset': a,\n",
    "                'Count_Prot_SwissProt': b,\n",
    "                'Count_Prot_SwissProt_Actual': a+b,\n",
    "                'Percentage_Dataset': round(my_proportion * 100, 2),\n",
    "                'Percentage_SwissProt': round(swissprot_proportion * 100, 10),\n",
    "                'Fold_Enrichment': round(my_proportion/swissprot_proportion,2),\n",
    "                'P_Value_Two_Tail': pvalue_two_tail,\n",
    "                'P_Value_Greater': pvalue_greater,\n",
    "            })\n",
    "    \n",
    "    # Convert to DataFrame and sort by p-value\n",
    "    df_results = pd.DataFrame(results)\n",
    "    if not df_results.empty:\n",
    "        df_results = df_results.sort_values('P_Value_Two_Tail')\n",
    "\n",
    "    df_results.to_csv(\"Function/enrichment_results.csv\")\n",
    "    \n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hierarchical Structure\n",
    "def analyze_go_hierarchy(go_id_to_go_term):\n",
    "    \"\"\"\n",
    "    Analyze the hierarchical structure of enriched GO terms.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the Gene Ontology DAG\n",
    "    go_obo = obo_parser.GODag('go.obo')\n",
    "    \n",
    "    # Read our enrichment results\n",
    "    df = pd.read_csv(\"Function/enrichment_results.csv\")\n",
    "    \n",
    "    # Filter for significantly enriched terms\n",
    "    enriched_terms = df[\n",
    "        (df['P_Value_Two_Tail'] < 0.05) &\n",
    "        (df['P_Value_Greater'] < 0.05)\n",
    "    ]\n",
    "    \n",
    "    # Create a dictionary to store branch information\n",
    "    branch_info = {}\n",
    "    \n",
    "    # For each enriched term, traverse up its ancestry\n",
    "    for _, row in enriched_terms.iterrows():\n",
    "        go_id = row['GO_ID']\n",
    "        if go_id in go_obo:\n",
    "            term = go_obo[go_id]\n",
    "            \n",
    "            # Get all ancestors (parents) up to the root of the DAG of the current term (i.e. the current GO ID)\n",
    "            \n",
    "            ancestors = term.get_all_parents()\n",
    "            \n",
    "            # Add information about this term to all its ancestor branches\n",
    "            for ancestor_id in ancestors:\n",
    "                if ancestor_id not in branch_info:\n",
    "                    branch_info[ancestor_id] = {\n",
    "                        'term_name': go_obo[ancestor_id].name,\n",
    "                        'enriched_children': [], # initialize empty\n",
    "                        'total_significance': 0, # initialize empty \n",
    "                        'depth': go_obo[ancestor_id].depth, # depth based on depth in tree (i.e root has depth 0)\n",
    "                    }\n",
    "\n",
    "               \n",
    "                # Our go_id in the current iteration is a child to ALL ancestors we found using \"get_all_parents()\"\n",
    "                #  (note that this is not necessarily a direct child, but maybe also much more down in the tree somewhere)\n",
    "                # Thus, add this child into the enriched_children list of the ancestor with its two-tailed p-value \n",
    "                branch_info[ancestor_id]['enriched_children'].append({\n",
    "                    'id': go_id,\n",
    "                    'name': term.name,\n",
    "                    'p_value': row['P_Value_Two_Tail']\n",
    "                })\n",
    "                # Measure significance based on -log value of the p value of all the childs of the ancestor (lower p values have higher scores)\n",
    "                branch_info[ancestor_id]['total_significance'] += -np.log10(row['P_Value_Two_Tail'])\n",
    "    \n",
    "    # Filter for high-level terms (lower depth) with multiple enriched children\n",
    "    significant_branches = {\n",
    "        go_id: info for go_id, info in branch_info.items() # take each key,value of the branch_info dictionary\n",
    "        if len(info['enriched_children']) >= 2  # At least 2 enriched children\n",
    "        and info['depth'] <= 3  # High-level terms having maximum depth of 3 (i.e. only look at GO terms high up in the tree)\n",
    "    } \n",
    "    \n",
    "    # Sort branches by their total significance\n",
    "    sorted_branches = sorted(\n",
    "        significant_branches.items(),\n",
    "        key=lambda x: x[1]['total_significance'],\n",
    "        reverse=True\n",
    "    )\n",
    "    \n",
    "    # Create a list to store the branch information\n",
    "    branch_data = []\n",
    "\n",
    "\n",
    "    for go_id, info in sorted_branches[:20]:  # Top 20 branches\n",
    "        branch_data.append({\n",
    "            'GO_ID': go_id,\n",
    "            'GO_Term': info['term_name'],\n",
    "            'Hierarchy_Depth': info['depth'],\n",
    "            'Number_Enriched_Terms': len(info['enriched_children']),\n",
    "            'Total_Significance_Score': info['total_significance']\n",
    "        })\n",
    "\n",
    "    # Create a DataFrame and save to CSV\n",
    "    branches_df = pd.DataFrame(branch_data)\n",
    "    branches_df.to_csv('Function/enriched_branches.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching GO annotations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching GO annotations: 100%|| 92/92 [00:25<00:00,  3.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to parse SwissProt XML...\n",
      "Processed 10000 proteins (skipped 0 family proteins)...\n",
      "Processed 19999 proteins (skipped 1 family proteins)...\n",
      "Processed 29983 proteins (skipped 17 family proteins)...\n",
      "Processed 39982 proteins (skipped 18 family proteins)...\n",
      "Processed 49982 proteins (skipped 18 family proteins)...\n",
      "Processed 59979 proteins (skipped 21 family proteins)...\n",
      "Processed 69979 proteins (skipped 21 family proteins)...\n",
      "Processed 79978 proteins (skipped 22 family proteins)...\n",
      "Processed 89958 proteins (skipped 42 family proteins)...\n",
      "Processed 99958 proteins (skipped 42 family proteins)...\n",
      "Processed 109958 proteins (skipped 42 family proteins)...\n",
      "Processed 119958 proteins (skipped 42 family proteins)...\n",
      "Processed 129958 proteins (skipped 42 family proteins)...\n",
      "Processed 139958 proteins (skipped 42 family proteins)...\n",
      "Processed 149958 proteins (skipped 42 family proteins)...\n",
      "Processed 159958 proteins (skipped 42 family proteins)...\n",
      "Processed 169958 proteins (skipped 42 family proteins)...\n",
      "Processed 179958 proteins (skipped 42 family proteins)...\n",
      "Processed 189958 proteins (skipped 42 family proteins)...\n",
      "Processed 199953 proteins (skipped 47 family proteins)...\n",
      "Processed 209953 proteins (skipped 47 family proteins)...\n",
      "Processed 219953 proteins (skipped 47 family proteins)...\n",
      "Processed 229953 proteins (skipped 47 family proteins)...\n",
      "Processed 239952 proteins (skipped 48 family proteins)...\n",
      "Processed 249952 proteins (skipped 48 family proteins)...\n",
      "Processed 259942 proteins (skipped 58 family proteins)...\n",
      "Processed 269938 proteins (skipped 62 family proteins)...\n",
      "Processed 279938 proteins (skipped 62 family proteins)...\n",
      "Processed 289936 proteins (skipped 64 family proteins)...\n",
      "Processed 299936 proteins (skipped 64 family proteins)...\n",
      "Processed 309929 proteins (skipped 71 family proteins)...\n",
      "Processed 319918 proteins (skipped 82 family proteins)...\n",
      "Processed 329918 proteins (skipped 82 family proteins)...\n",
      "Processed 339917 proteins (skipped 83 family proteins)...\n",
      "Processed 349917 proteins (skipped 83 family proteins)...\n",
      "Processed 359917 proteins (skipped 83 family proteins)...\n",
      "Processed 369917 proteins (skipped 83 family proteins)...\n",
      "Processed 379917 proteins (skipped 83 family proteins)...\n",
      "Processed 389917 proteins (skipped 83 family proteins)...\n",
      "Processed 399917 proteins (skipped 83 family proteins)...\n",
      "Processed 409917 proteins (skipped 83 family proteins)...\n",
      "Processed 419917 proteins (skipped 83 family proteins)...\n",
      "Processed 429917 proteins (skipped 83 family proteins)...\n",
      "Processed 439917 proteins (skipped 83 family proteins)...\n",
      "Processed 449917 proteins (skipped 83 family proteins)...\n",
      "Processed 459917 proteins (skipped 83 family proteins)...\n",
      "Processed 469917 proteins (skipped 83 family proteins)...\n",
      "Processed 479917 proteins (skipped 83 family proteins)...\n",
      "Processed 489913 proteins (skipped 87 family proteins)...\n",
      "Processed 499913 proteins (skipped 87 family proteins)...\n",
      "Processed 509913 proteins (skipped 87 family proteins)...\n",
      "Processed 519912 proteins (skipped 88 family proteins)...\n",
      "Processed 529912 proteins (skipped 88 family proteins)...\n",
      "Processed 539912 proteins (skipped 88 family proteins)...\n",
      "Processed 549909 proteins (skipped 91 family proteins)...\n",
      "Processed 559909 proteins (skipped 91 family proteins)...\n",
      "Processed 569908 proteins (skipped 92 family proteins)...\n",
      "Expanding GO terms to include ancestors...\n",
      "go.obo: fmt(1.2) rel(2024-11-03) 43,983 Terms\n",
      "\n",
      "Top enriched GO terms:\n",
      "\n",
      "Term: intermediate-density lipoprotein particle remodeling\n",
      "Weight in word cloud: 5996.03\n",
      "\n",
      "Term: chylomicron remodeling\n",
      "Weight in word cloud: 5073.57\n",
      "\n",
      "Term: intestinal lipid catabolic process\n",
      "Weight in word cloud: 4497.02\n",
      "\n",
      "Term: lipoprotein lipase activity\n",
      "Weight in word cloud: 4441.51\n",
      "\n",
      "Term: obsolete 1-18:1-2-16:0-monogalactosyldiacylglycerol lipase activity\n",
      "Weight in word cloud: 3815.66\n",
      "\n",
      "Term: lipoprotein particle mediated signaling\n",
      "Weight in word cloud: 3270.56\n",
      "\n",
      "Term: low-density lipoprotein particle mediated signaling\n",
      "Weight in word cloud: 3270.56\n",
      "\n",
      "Term: phosphatidylserine 1-acylhydrolase activity\n",
      "Weight in word cloud: 2892.20\n",
      "\n",
      "Term: obsolete 1-acyl-2-lysophosphatidylserine acylhydrolase activity\n",
      "Weight in word cloud: 2855.25\n",
      "\n",
      "Term: galactolipase activity\n",
      "Weight in word cloud: 2821.66\n",
      "go.obo: fmt(1.2) rel(2024-11-03) 43,983 Terms\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "\n",
    "    psiblast_file = \"Model/Evaluation/Predictions/PSI-BLAST/psiblastsearch_output.csv\"\n",
    "    hmm_file = \"Model/Evaluation/Predictions/HMM-SEARCH/hmmsearch_output.csv\"\n",
    "    protein_ids = load_protein_ids(psiblast_file, hmm_file)\n",
    "\n",
    "\n",
    "    # Proteins_to_GO terms for our family \n",
    "    print(\"Fetching GO annotations...\")\n",
    "    family_annotations = {}\n",
    "    for pid in tqdm(protein_ids, desc=\"Fetching GO annotations\"):\n",
    "        family_annotations[pid] = fetch_go_annotations(pid)\n",
    "\n",
    "    total_proteins_family = len(family_annotations)\n",
    "\n",
    "\n",
    "    \n",
    "    # Proteins_to_GO terms for SwissProt\n",
    "    swissprot_annotations = parse_swissprot_go_terms(\"uniprot_sprot.xml\", protein_ids)\n",
    "\n",
    "    total_proteins_swissprot = len(swissprot_annotations)\n",
    "\n",
    "    # Load the GO DAG for ancestor expansion\n",
    "    # We downloaded the go.obo file so we can parse the whole ontology\n",
    "    # Note that \"go.obo\" we downloaded locally and not to the Git Repository due to its size\n",
    "    print(\"Expanding GO terms to include ancestors...\")\n",
    "    go_obo = obo_parser.GODag('go.obo')\n",
    "    expanded_family_annotations = expand_go_terms_with_ancestors(family_annotations, go_obo)\n",
    "    expanded_swissprot_annotations = expand_go_terms_with_ancestors(swissprot_annotations, go_obo)\n",
    "\n",
    "    # Fetch all GO terms for all found GO IDs in the family after expanding \n",
    "    go_id_to_go_term = get_go_terms_given_goid(expanded_family_annotations)\n",
    "\n",
    "    # Reverse mapping (go_id to proteins mapping) for enrichment\n",
    "    go_to_proteins_family = reverse_protein_go_dict(expanded_family_annotations)\n",
    "    go_to_proteins_swissprot = reverse_protein_go_dict(expanded_swissprot_annotations)\n",
    "\n",
    "    # Calculate GO enrichments\n",
    "    _ = calculate_go_enrichment(go_to_proteins_family, go_to_proteins_swissprot,\n",
    "                                total_proteins_family, total_proteins_swissprot, go_id_to_go_term)\n",
    "\n",
    "    \n",
    "    \n",
    "    # Read the enrichment results\n",
    "    df = pd.read_csv(\"Function/enrichment_results.csv\")\n",
    "\n",
    "    # Get the terms to the GO ids from the family data\n",
    "  #  go_id_to_term = create_go_id_to_term_mapping(family_annotations)\n",
    "\n",
    "    # Filter for significantly enriched terms\n",
    "    enriched_terms = df[\n",
    "    (df['P_Value_Two_Tail'] < 0.05) &\n",
    "    (df['P_Value_Greater'] < 0.05)\n",
    "    ]\n",
    "\n",
    "\n",
    "    # Create word frequencies using the actual GO terms instead of IDs\n",
    "    word_frequencies = {}\n",
    "    for _, row in enriched_terms.iterrows():\n",
    "        go_id = row['GO_ID']\n",
    "        if go_id in go_id_to_go_term:  # Make sure we have the term for this ID\n",
    "            term = go_id_to_go_term[go_id]\n",
    "            # Use fold enrichment as weight\n",
    "            weight = row['Fold_Enrichment']\n",
    "            word_frequencies[term] = weight\n",
    "\n",
    "    # Create and display the word cloud\n",
    "    wordcloud = WordCloud(\n",
    "        width=1200, \n",
    "        height=800,\n",
    "        background_color='white',\n",
    "        prefer_horizontal=0.7,\n",
    "        max_words=50,  # Limit to top 50 terms for better readability\n",
    "        min_font_size=10,\n",
    "        max_font_size=60\n",
    "    ).generate_from_frequencies(word_frequencies)\n",
    "\n",
    "    # Plot and save the word cloud\n",
    "    plt.figure(figsize=(20, 12))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title('GO Term Enrichment Word Cloud', fontsize=16, pad=20)\n",
    "    plt.savefig('Function/go_enrichment_wordcloud.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    # Print out the enriched terms for verification\n",
    "    print(\"\\nTop enriched GO terms:\")\n",
    "    sorted_terms = sorted(word_frequencies.items(), key=lambda x: x[1], reverse=True)\n",
    "    for term, weight in sorted_terms[:10]:\n",
    "        print(f\"\\nTerm: {term}\")\n",
    "        print(f\"Weight in word cloud: {weight:.2f}\")\n",
    "\n",
    "    \n",
    "    # Hierarchy\n",
    "\n",
    "    analyze_go_hierarchy(go_id_to_go_term)\n",
    "\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motifs\n",
    "1. Search significantly conserved short motifs inside your family. Use [ELM classes](http://elm.eu.org/elms) and [ProSite patterns](https://ftp.expasy.org/databases/prosite/prosite.dat) (for ProSite consider only patterns PA lines, not the profiles). Make sure to consider as true matches only those that are found inside disordered regions. Disordered regions for the entire SwissProt (as defined by MobiDB-lite) are available [here](https://drive.google.com/file/d/1m7rdFvQiCRizOx54YPk1eMw4qF1iskbz/view?usp=sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following script come from https://github.com/stevin-wilson/PrositePatternsToPythonRegex/tree/master and is useful for converts prosite patterns into regex patters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PS00001 ['N[ACDEFGHIKLMNQRSTVWY][ST][ACDEFGHIKLMNQRSTVWY]']\n",
      "PS00004 ['[RK]{2}.[ST]']\n",
      "PS00005 ['[ST].[RK]']\n",
      "PS00006 ['[ST].{2}[DE]']\n",
      "PS00007 ['[RK].{2}[DE].{3}Y']\n",
      "PS00008 ['G[ACGILMNQSTV].{2}[STAGCN][ACDEFGHIKLMNQRSTVWY]']\n",
      "PS00009 ['.G[RK][RK]']\n",
      "PS00010 ['C.[DN].{4}[FY].C.C']\n",
      "PS00011 ['E.{2}[ERK]E.C.{6}[EDR].{10,11}[FYA][YW]']\n",
      "PS00012 ['[DEQGSTALMKRH][LIVMFYSTAC][GNQ][LIVMFYAG][DNEKHS]S[LIVMST][ADEGHIKLMNQRSTVW][STAGCPQLIVMF][LIVMATN][DENQGTAKRHLM][LIVMWSTA][LIVGSTACR][ACDEFGHKMNQRSTVW][ACDEFGHIKLMNPQRSTW][LIVMFA]']\n",
      "PS00014 ['[KRHQSA][DENQ]EL$']\n",
      "PS00016 ['RGD']\n",
      "PS00017 ['[AG].{4}GK[ST]']\n",
      "PS00018 ['D[ACDEFGHIKLMNPQRSTVY][DNS][ACDEGHKMNPQRST][DENSTG][DNQGHRK][ACDEFHIKLMNQRSTVWY][LIVMC][DENQSTAGC].{2}[DE][LIVMFYW]']\n",
      "PS00019 ['[EQ][ACDEFGIKMPQRSTVW].[ATV][FY][CEFGHIKNPQRSTVWY][ACDEFGHIKLMNPQRSVWY]W[ACDEFHIKLMNQRSTVWY]N']\n",
      "PS00020 ['[LIVM].[SGNL][LIVMN][DAGHENRS][SAGPNVT].[DNEAG][LIVM].[DEAGQ].{4}[LIVM].[LM][SAG][LIVM][LIVMT][WS].{0,1}[LIVM]{2}']\n",
      "PS00021 ['[FY]C[RH][NS].{7,8}[WY]C']\n",
      "PS00022 ['C.C.{2}[ACDEFGHIKLMNPQRSTWY].{2}G[ADEFGHIKLMNPQRSTVWY].C']\n",
      "PS00023 ['C.{2}PF.[FYWIV].{7}C.{8,10}WC.{4}[DNSR][FYW].{3,5}[FYW].[FYWI]C']\n",
      "PS00024 ['[LIFAT][ACDEFGHKMNPQRSTVWY].{2}W.{2,3}[PE].[ACDEGHIKLMNPQRSTWY][LIVMFY][DENQS][STA][AV][LIVMFY]']\n",
      "PS00025 ['[KRH].{2}C.[FYPSTV].{3,4}[ST].{3}C.{4}CC[FYWH]']\n",
      "PS00026 ['C.{4,5}CCS.{2}G.CG.{3,4}[FYW]C']\n",
      "PS00027 ['[LIVMFYG][ASLVR].{2}[LIVMSTACN].[LIVM][ACDEFGHIKLMNPQRSTVW].{2}[ACDEFGHIKMNPQRSTVWY][LIV][RKNQESTAIY][LIVFSTNKH]W[FYVC].[NDQTAH].{5}[RKNAIMW]']\n",
      "PS00028 ['C.{2,4}C.{3}[LIVMFYWC].{8}H.{3,5}H']\n",
      "PS00029 ['L.{6}L.{6}L.{6}L']\n",
      "PS00031 ['C.{2}C.{1,2}[DENAVSPHKQT].{5,6}[HNY][FY].{4}C.{2}C.{2}F{2}.R']\n",
      "PS00032 ['[LIVMFE][FY]PWM[KRQTA]']\n",
      "PS00033 ['LMA[EQ]GLYN']\n",
      "PS00034 ['RPC.{11}CVS']\n",
      "PS00035 ['[RKQ]R[LIM].[LF]G[LIVMFY].Q.[DNQ]VG']\n",
      "PS00036 ['[KR].{1,3}[RKSAQ]N[ACDEFGHIKMNPQRSTWY].[SAQ]{2}[ACDEFGHIKMNPQRSTVWY][RKTAENQ].R[ACDEFGHIKLMNPQRTVWY][RK]']\n",
      "PS00039 ['[LIVMF]{2}DEAD[RKEN].[LIVMFYGSTN]']\n",
      "PS00041 ['[KRQ][LIVMA].{2}[GSTALIV][ACEHIKLMQRSTV].{2}[LIVMSA].{4,9}[LIVMF].[ACDEFGIKMNQRSTVWY][LIVMSTA][GSTACIL][ACDEFHILMNQRSTVWY][ACDEGHIKLMNPQRSTVWY].[GANQRF][LIVMFY].{4,5}[LFY].{3}[FYIVA][ADEGIKLNPQRSTV][ACDEFHKLMNQRSTWY].{2}[GSADENQKR].[NSTAPKL][PARL]']\n",
      "PS00042 ['[LIVM][STAG][RHNWM].{2}[LIM][GA].[LIVMFYAS][LIVSC][GA].[STACN].{2}[MST].{1,2}[GSTN]R.[LIVMF].{2}[LIVMF]']\n",
      "PS00045 ['[GSK]F.{2}[LIVMF].{4}[RKEQA].{2}[RST].{1,2}[GA].[KN]P.[TN]']\n",
      "PS00046 ['[AC]GL.FPV']\n",
      "PS00047 ['GAKRH']\n",
      "PS00048 ['[AV]R[NFY]R.{2,3}[ST][ACDEFGHIKLMNPQRTVWY]S[ACDEFGHIKLMPQRTVWY]S']\n",
      "PS00049 ['[GA][LIV]{3}.{9,10}[DNS]G.{4}[FY].{2}[NT].{2}V[LIV]']\n",
      "PS00050 ['[RK]{2}[AM][IVFYT][IV][RKT]L[STANEQK].{7}[LIVMFT]']\n",
      "PS00051 ['[KRM][PTKS].{3}[LIVMFG].{2}[NHS].{3}R[DNHY]WR[RS]']\n",
      "PS00052 ['[DENSK].[LIVMDET].{3}[LIVMFTA]{2}.{6}GK[KR].{5}[LIVMF][LIVMFC].{2}[STAC]']\n",
      "PS00053 ['[GE].{2}[LIV]{2}[STY][ST][CDEFGHIKLMNPQRSTVWY].G[LIVM]{2}.{4}[AG][KRHAYIL]']\n",
      "PS00054 ['[LIVMFR].[GSTACQI][LIVMF].{1,2}[GSTALVM].{0,1}[GSN][LIVMFY].[LIVM].{4}[DEN].[TS][PS].[PA][STCHF][DN]']\n",
      "PS00055 ['[RK].PNS[AR].R']\n",
      "PS00056 ['GD.[LIV].[LIVA].[QEK].[RK]P[LIV]S']\n",
      "PS00057 ['[IVRLP][DYN][YLF].{2,3}[LIVMTPFS].{2}[LIVM].{2}[FYTS][LIVMT][STNQG][DERPN].{1,2}[GYAH][KCR][LIVM].{3}[RHG][LIVMASR]']\n",
      "PS00058 ['GFRGE[AG]L']\n",
      "PS00059 ['GHE.[ACDFGHIKMNPQRSTVWY]G[CDEFGHIKLMNQRSTVWY].{4}[GA].{2}[IVSAC]']\n",
      "PS00060 ['[GSW].[LIVTSACD][GH].{2}[GSAE][GSHYQ].[LIVTP][GAST][GAS].{3}[LIVMT].[HNS][GA].[GTAC]']\n",
      "PS00061 ['[LIVSPADNK].{9}[ACDEFGHIKLMNQRSTVWY].{2}Y[PSTAGNCV][STAGNQCIVM][STAGC]K[ADEFGHIKLMNQRSTVWY][SAGFYR][LIVMSTAGD].[ACDEFGHILMNPQRSTVWY][LIVMFYW][ACEFGHIKLMNPQRSTVWY].[ACDEFGHIKLMNPQSTVW][LIVMFYWGAPTHQ][GSACQRHM]']\n",
      "PS00062 ['[LIVMFY].{8}[ACDEFGHIKMNPQRSTVWY][KREQ][ACDEFGHILMNPQRSTVWY][LIVM]G[LIVM][SC]N[FY]']\n",
      "PS00063 ['[LIVM][PAIV][KR][ST][ACDFHIKLMNRSTVWY][ACDEGHKLMNPQSTVWY].{2}R[CDEGHIKLMNPQRTWY].[GSTAEQK][NSL].[ACDEFGHKMNPQSTWY][LIVMFA]']\n",
      "PS00064 ['[LIVMA]G[EQ]HG[DN][ST]']\n",
      "PS00065 ['[LIVMA][AG][IVT][LIVMFY][AG].G[NHKRQGSAC][LIV]G.{13,14}[LIVMFT][CDEFGHIKLMNPQRSTVWY].[FYWCTH][DNSTK]']\n",
      "PS00066 ['[RKH].[ACDEFGHIKLMNPQRSTVW][ACDEFGHKLMNPQRSTVWY].[ACDEFGHKLMNPQRSTVWY][ACDEFGHIKMNPQRSTVWY]D.MG.N.[LIVMA]']\n",
      "PS00067 ['[DNES].{2}[GA]F[LIVMFYA].[NT]R.{3}[PA][LIVMFY][LIVMFYST].{5,6}[LIVMFYCT][LIVMFYEAH].{2}[GVE]']\n",
      "PS00068 ['[LIVM]T[TRKMN]LD.{2}R[STA].{3}[LIVMFY]']\n",
      "PS00069 ['DH[YF]LGK[EQK]']\n",
      "PS00070 ['[FYLVA].[ACDFHIKLMNQRSTWY][ACEFGHKMNPQRSTWY]G[QE][ACDEFHIKMNQRSTVW]C[LIVMGSTANC][AGCN][ACDFGIKLMNPQRSTVWY][GSTADNEKR]']\n",
      "PS00071 ['[ASV]SC[NT]T[ACDEFGHIKLMNPQRTVWY].[LIM]']\n",
      "PS00072 ['[GAC][LIVM][ST]E.{2}[GSAN]G[ST]D.{2}[GSA]']\n",
      "PS00073 ['[QDE].[ACDEFGHIKLMNQRSTVWY]G[GS].G[LIVMFY].{2}[DEN].{4}[KR].{3}[DEN]']\n",
      "PS00074 ['[LIV].{2}GG[SAG]K.[GV].{3}[DNST][PL]']\n",
      "PS00075 ['[LVAGC][LIF]G.{4}[LIVMF]PW.{4,5}[DE].{3}[FYIV].{3}[STIQ]']\n",
      "PS00076 ['GG.C[LIVA].{2}GC[LIVM]P']\n",
      "PS00077 ['[YWG][LIVFYWTA]{2}[VGS]H[LNP].V.{44,47}HH']\n",
      "PS00078 ['V.H.{33,40}C.{3}C.{3}H.{2}M']\n",
      "PS00079 ['G.[FYW].[LIVMFYW].[CST].[ACDEFGHIKLMNQSTVWY][ACDEFGHILMNPQRSTVWY].{2}[ACDEFGHIKLMNPQRTVWY].[ACDEGIKMNPQRSTVWY]G[LM].{3}[LIVMFYW]']\n",
      "PS00080 ['HCH.{3}H.{3}[AG][LM]']\n",
      "PS00081 ['[LIVMACST]HP[LIVM].[KRQV][LIVMF]{2}.[AP]H']\n",
      "PS00082 ['[GNTIV].H.{5,7}[LIVMF]Y.{2}[DENTA]P.[GP].{2,3}E']\n",
      "PS00083 ['[LIVMF].G.[LIVM].{4}[GS].{2}[LIVMA].{4}[LIVM][DE][LIVMFYC].{6}G.[FY]']\n",
      "PS00084 ['HHM.{2}F.C']\n",
      "PS00085 ['H.F.{4}HTH.{2}G']\n",
      "PS00086 ['[FW][SGNH].[GD][ACDEGHIKLMNPQRSTVWY][RKHPT][ACDEFGHIKLMNQRSTVWY]C[LIVMFAP][GAD]']\n",
      "PS00087 ['[GA][IMFAT]H[LIVF]H[ACDEFGHIKLMNPQRTVWY].[GP][SDG].[STAGDE]']\n",
      "PS00088 ['D.[WF]EH[STA][FY]{2}']\n",
      "PS00089 ['W.{2}[LIVF].{6,7}G[LIVM][FYRA][NH].{3}[STAQLIVM][ASC].{2}[PA]']\n",
      "PS00090 ['[STANQ][ET]C.{5}GD[DN][LIVMT].[STAGR][LIVMFYST]']\n",
      "PS00091 ['R.{2}[LIVMT].{2,3}[FWY][QNYDI].{8,13}[LVESI].PC[HAVMLC].{3}[QMTLHD][FYWL].{0,1}[LV]']\n",
      "PS00092 ['[LIVMAC][LIVFYWA][ACEFGHIKLMNQRSTVW][DN]PP[FYW]']\n",
      "PS00093 ['[LIVMF]TSPP[FY]']\n",
      "PS00094 ['[DENKS].[FLIV].{2}[GSTC].PC.[ACDEFGHIKLMNPQRSTWY][FYWLIM]S']\n",
      "PS00095 ['[RKQGTF].{2}GN[SA][LIVF].[VIP].[LVMT].{3}[LIVM].{3}[LIVM]']\n",
      "PS00096 ['[DEQHY][LIVMFYA].[GSTMVA][GSTAV][ST][STVM][HQ]K[STG][LFMI].[GAS][PGAC][RQ][GSARH][GA]']\n",
      "PS00097 ['F.[EK].S[GT]RT']\n",
      "PS00098 ['[LIVM][NST][ACDEFGHIKLMNPQRSVWY].C[SAGLI][ST][SAG][LIVMFYNS].[STAG][LIVM].{6}[LIVM]']\n",
      "PS00099 ['[AG][LIVMA][STAGCLIVM][STAG][LIVMA]C[ACDEFGHIKLMNPRSTVWY][AG].[AG].[AG].[SAG]']\n",
      "PS00100 ['Q[LIV]HH[SA].{2}DG[FY]H']\n",
      "PS00101 ['[LIV][GAED].{2}[STAV].[LIV].{3}[LIVAC].[LIV][GAED].{2}[STAVR].[LIV][GAED].{2}[STAV].[LIV].{3}[LIV]']\n",
      "PS00102 ['EA[SC]G.[GS].MK.{2}[LM]N']\n",
      "PS00103 ['[LIVMFYWCTA][LIVM][LIVMA][LIVMFC][DE]D[LIVMS][LIVM][STAVD][STAR][GAC].[STAR]']\n",
      "PS00104 ['[LIVF][ACDEFGHIKMNPQRSTWY].[GANQK][NLG][SA][GA][TAI][STAGV][ACDEFGHIKLMPQRSTVWY]R.[LIVMFYAT].[GSTAP]']\n",
      "PS00105 ['[GS][LIVMFYTAC][GSTA]K.{2}[GSALVN][LIVMFA].[GNAR][ACDEFGHIKLMNPQRSTWY]R[LIVMA][GA]']\n",
      "PS00106 ['GR.N[LIV]IG[DE]H.DY']\n",
      "PS00107 ['[LIV]G[ACDEFGHIKLMNQRSTVWY]G[ACDEFGHIKLMNQRSTVWY][FYWMGSTNH][SGA][ACDEFGHIKLMNQRSTVY][LIVCAT][ACEFGHIKLMNQRSTVWY].[GSTACLIVMFY].{5,18}[LIVMFYWCSTAR][AIVP][LIVMFAGCKR]K']\n",
      "PS00108 ['[LIVMFYC].[HY].D[LIVMFY]K.{2}N[LIVMFYCT]{3}']\n",
      "PS00109 ['[LIVMFYC][CDEFGHIKLMNPQRSTVWY][HY].D[LIVMFY][RSTAC][ACEFGHIKLMNPQRSTVWY][ACDEGHIKLMNQRSTVWY]N[LIVMFYC]{3}']\n",
      "PS00110 ['[LIVAC].[LIVM]{2}[SAPCV]K[LIV]E[NKRST].[DEQHS][GSTA][LIVM]']\n",
      "PS00111 ['[KRHGTCVN][VT][LIVMF][LIVMC]R.D.N[SACV]P']\n",
      "PS00112 ['CP.{0,1}[ST]N[ILV]GT']\n",
      "PS00113 ['[LIVMFYWCA][LIVMFYW]{2}DG[FYI]PR.{3}[NQ]']\n",
      "PS00114 ['D[LIM]H[SANDT].[QS][IMSTAVF][QMLPH][GA][FY]F.{2}P[LIVMFCT]D']\n",
      "PS00115 ['Y[ST]P[ST]SP[STANK]']\n",
      "PS00116 ['[YA][GLIVMSTAC]DTD[SG][LIVMFTC][CDEFGHIKMNPQRSTVWY][LIVMSTAC]']\n",
      "PS00117 ['FEN[RK]G.{3}G.{4}HPH.Q']\n",
      "PS00118 ['CC[ACDEFGHIKLMNQRSTVWY].H[ACDEFHIKMNPQRSTVW].C']\n",
      "PS00119 ['[LIVMA]C[ADEGHKNQR]CD[ACDEFHIKLMNPQRTVWY][ACDEFHIKLMNPQRSTVWY][ACDEFGHIKLMPQRSTVWY].[ACDEFGHIKLMNPRTVWY]C']\n",
      "PS00120 ['[LIV][ACDEFHILMNPQRSTVWY][LIVFY][LIVMST]G[HYWV]S[CDEFHIKLMNPQRSTVW]G[GSTAC]']\n",
      "PS00121 ['Y.{2}YY.C.C']\n",
      "PS00122 ['F[GR]G.{4}[LIVM].[LIV].G.S[STAG]G']\n",
      "PS00123 ['[IV].DS[GAS][GASC][GAST][GA]T']\n",
      "PS00124 ['[AG][RK][LI].{1,2}[LIV][FY]E.{2}P[LIVM][GSA]']\n",
      "PS00125 ['[LIVMN][KR]GNHE']\n",
      "PS00126 ['HD[LIVMFY].H.[AG].{2}[NQ].[LIVMFY]']\n",
      "PS00127 ['CK.{2}NTF']\n",
      "PS00128 ['C.{3}C.{2}[LMF].{3}[DEN][LI].{5}C']\n",
      "PS00129 ['[GFY][LIVMF]W.DM[NSA]E']\n",
      "PS00130 ['[KR][LIVA][LIVC][LIVM].G[QI]DPY']\n",
      "PS00131 ['[LIVM].[GSTA]ESY[AG][GS]']\n",
      "PS00132 ['[PK].[LIVMFY].[LIVMFY].{2}[ACDFGHIKLMNPQRSTVWY].H[STAG].E.[LIVM][STAG][ACDEFGHIKMNPQRSTVWY].{5}[LIVMFYTA]']\n",
      "PS00133 ['H[STAG][CEFGHIKLMPQRSTWY][ACDEHKLMNPQRSTWY][CDEFGHIKLMNPQSTVW][LIVME][ACFGHIKLMNQRTVWY].[LIVMFYW]P[FYW]']\n",
      "PS00134 ['[LIVM][ST]A[STAG]HC']\n",
      "PS00135 ['[DNSTAGC][GSTAPIMVQH].{2}G[DE]SG[GS][SAPHV][LIVMFYWH][LIVMFYSTANQH]']\n",
      "PS00136 ['[STAIV][ACFGHIKMNPQSTVWY][LIVMF][LIVM]D[DSTA]G[LIVMFC].{2,3}[DNH]']\n",
      "PS00137 ['HG[STM].[VIC][STAGC][GS].[LIVMA][STAGCLV][SAGM]']\n",
      "PS00138 ['GTS.[SA].P.[ACDEFGHIKMNPQRSTVWY][STAVC][AG]']\n",
      "PS00139 ['Q[ACDEFGHIKLMNPQRSTWY].[ACFGHIKLMNPQRSTVWY][GE][ACDEGHIKLMNPQRSTVWY]C[YW][ACEFGHIKLMPQRSTVWY].[STAGC][STAGCV]']\n",
      "PS00140 ['Q.{3}N[SA]CG.{3}[LIVM]{2}H[SA][LIVM][SA]']\n",
      "PS00141 ['[LIVMFGAC][LIVMTADN][LIVFSA]D[ST]G[STAV][STAPDENQ][ACDEFHIKLMNPRSTVWY][LIVMFSTNC][ACDFHILMNPQRSTVWY][LIVMFGTA]']\n",
      "PS00142 ['[GSTALIVN][ADEFGIKLMNQSTVWY][ACEFGHILMPQRSTVWY]HE[LIVMFYW][ACFGILMNQSTVWY]H[ADFGHILMNQRSTVWY][LIVMFYWGSPQ]']\n",
      "PS00143 ['G.{8,9}G.[STA]H[LIVMFY][LIVMC][DERN][HRKL][LMFAT].[LFSTH].[GSTAN][GST]']\n",
      "PS00144 ['[LIVM].[ACDEFGHIKMNPQRSTVWY]TGGT[IV][AGS]']\n",
      "PS00145 ['[LIVM]{2}[CT]H[HNG]L.{3}[LIVM].{2}D[LIVM].F[AS]']\n",
      "PS00146 ['[FY].[LIVMFY][ACDFGHIKLMNPQRSTVWY]S[TV].K.{3}[ACDEFGHIKLMNPQRSVWY][AGLM][ACEFGHIKLMNPQRSTVWY][CDEFGHILMNPQRSTVWY][LC]']\n",
      "PS00149 ['G[YV].[ST].{2}[IVAS]GK.{0,1}[FYWMK][HL]']\n",
      "PS00150 ['[LIV].G.VQ[GH]V.[FM]R']\n",
      "PS00151 ['G[FYW][AVC][KRQAM]N.{3}G.V.{5}G']\n",
      "PS00152 ['P[SAP][LIV][DNH][ACDEFHIMPQRSTVWY][ACDEGHIKLMNPQRSTVWY][ACDEFGHIKLMNPQRTVWY]S[AEFGIKLMNQRSTVWY]S']\n",
      "PS00153 ['[IV]T.E.{2}[DE].{3}GA.[SAKR]']\n",
      "PS00154 ['DKTGT[LIVM][TI]']\n",
      "PS00155 ['P.[STA].[LIV][IVT].[GS]GYS[QL]G']\n",
      "PS00156 ['[LIVMFTAR][LIVMF].D.K.{2}D[IV][ADGP].T[CLIVMNTA]']\n",
      "PS00157 ['G.[DN]F.K.DE']\n",
      "PS00158 ['[LIVM].[LIVMFYW]EG.[LSI]LK[PA][SN]']\n",
      "PS00159 ['G[LIVM].{3}E[LIV]T[LF]R']\n",
      "PS00160 ['G.{3}[LIVMF]K[LF]FP[SA].{3}G']\n",
      "PS00161 ['K[KR]CGH[LMQR]']\n",
      "PS00162 ['SE[HN].[LIVM].{4}[FYH].{2}E[LIVMGA]H[LIVMFA]{2}']\n",
      "PS00163 ['GS.{2}M.[ACDEFGHIKLMNPQTVWY]K.N']\n",
      "PS00164 ['[LIVTMS][LIVP][LIV][KQ].[ND]Q[INV][GA][ST][LIVM][STL][DERKAQG][STA]']\n",
      "PS00165 ['[DESH].{4,5}[STVG][ACFGHILMNPQRSTWY][AS][FYI]K[DLIFSA][RLVMF][GA][LIVMGA]']\n",
      "PS00166 ['[LIVM][STAG].[LIVM][DENQRHSTA]G.{3}[AG]{3}.{4}[LIVMST].[CSTA][DQHP][LIVMFYA]']\n",
      "PS00167 ['[LIVM]E[LIVM][GQ].[PALT][FYCHTWP][STPK][DEKY][PA][LIVMYGK][SGALIMY][DE][GN]']\n",
      "PS00168 ['[LIVMYAHQ].[HPYNVF].G[STA]HK.N.{2}[LIVM].[QEH]']\n",
      "PS00169 ['G.D.[LIVM]{2}[IV]KP[GSA].{2}Y']\n",
      "PS00170 ['[FY].{2}[STCNLVA].[FV]H[RH][LIVMNS][LIVM].{2}F[LIVM].Q[AGFT]G']\n",
      "PS00171 ['[AVG][YLV]EP[LIVMEPKST][WYEAS][SAL][IV][GN][TEKDVS][GKNAD]']\n",
      "PS00174 ['[GSA].[LIVMCAYQS][LIVMFYWN].{4}[FY][DNTH]Q.[GA][IV][EQST].{2}K']\n",
      "PS00175 ['[LIVM].RHG[EQ].[ACDEFGHIKLMNPQRSTVW].N']\n",
      "PS00176 ['[DEN].{6}[GS][IT]SK.{2}Y[LIVM].{3}[LIVM]']\n",
      "PS00177 ['[LIVMA][ACDEFGHIKLMNPQSTVWY]EG[DN]SA[ACDEGHIKLMNPQRSTVWY][STAG]']\n",
      "PS00178 ['P.{0,2}[GSTAN][DENQGAPK].[LIVMFP][HT][LIVMYAC]G[HNTG][LIVMFYSTAGPC]']\n",
      "PS00180 ['[FYWL]DGSS.{6,8}[DENQSTAK][SA][DE].{2}[LIVMFY]']\n",
      "PS00181 ['KP[LIVMFYA].{3,5}[NPAT][GA][GSTAN][GA].H.{3}S']\n",
      "PS00182 ['K[LIVM].{5}[LIVMA]D[RK][DN][LI]Y']\n",
      "PS00183 ['[FYWLSP]H[PC][NHL][LIV].{3,4}G.[LIVP]C[LIV].{1,2}[LIVR]']\n",
      "PS00184 ['R[LF]GDPE.[EQIM]']\n",
      "PS00185 ['[RK].[STA].{2}S.CY[SL]']\n",
      "PS00186 ['[LIVM]{2}.CG[STA].{2}[STAG].{2}T.[DNG]']\n",
      "PS00187 ['[LIVMF][GSA].{5}P.{4}[LIVMFYW].[LIVMF].GD[GSA][GSAC]']\n",
      "PS00188 ['[GDN][DEQTR].[LIVMFY].{2}[LIVM].[AIV]MK[LVMAT].{3}[LIVM].[SAV]']\n",
      "PS00189 ['[GDN].{2}[LIVF].{3}[ACDEFGIKLMNPQRSTWY][ACDEFGHIKLNPQRSTVWY][LIVMFCA].{2}[LIVMFA][ACEGHIKMNPQRSTVW][ACDFGHILMNQRSTVWY].K[GSTAIVW][STAIVQDN].{2}[LIVMFS].{5}[GCN].[LIVMFY]']\n",
      "PS00191 ['[FY][LIVMK][ACDEFGHKLMNPQRSTVWY][ACDEFGHIKLMNPRSTVWY]HP[GA]G']\n",
      "PS00194 ['[LIVMF][LIVMSTA].[LIVMFYC][FYWSTHE].{2}[FYWGTN]C[GATPLVE][PHYWSTA]C[ACDEFGHKLMNPQRSTVWY].[CDEFGHIKLMNPQRSTVWY].{3}[LIVMFYWT]']\n",
      "PS00195 ['[LIVMD][FYSA].{4}C[PV][FYWH]C.{2}[TAV].{2,3}[LIV]']\n",
      "PS00196 ['[GA].{0,2}[YSA].{0,1}[VFY][ACFGHIKLMNPQRVWY]C.{1,2}[PG].{0,1}H.{2,4}[MQ]']\n",
      "PS00197 ['C[ADEFGHIKLMNPQRSTVWY][ADEFGHIKLMNPQRSTVWY][GA][ADEFGHIKLMNPQRSTVWY]C[GAST][AGILMNQSTV]C']\n",
      "PS00198 ['C.[ACDEFGHIKLMNQRSTVWY]C[ADEFGHIKLMNPQRSTVWY].C[ADEFGHIKLMNQRSTVWY].[ADEFGHIKLMNPQRSTVWY]C[PEG]']\n",
      "PS00201 ['[LIV][LIVFY][FY].[ST][ACDEFGHIKLMNPQRSTWY].[AGC].T[ACDEFGHIKLMNQRSTVWY].{2}A[ACDEFGHIKMNPQRSTVWY].[LIV]']\n",
      "PS00202 ['[LIVM].[ACDEFHIKLMNPQRSTVWY][ACDEFGHIKLMNPQSTVWY]W.CP.C[AGD]']\n",
      "PS00203 ['C.C[GSTAP].{2}C.C.{2}C.C.{2}C.K']\n",
      "PS00204 ['D.{2}[LIVMF][STACQV][DH][FYMI][LIV][EN].{2}[FYC]L.{6}[LIVMQ][KNER]']\n",
      "PS00205 ['Y.{0,1}[VAS]V[IVAC][IVA][IVA][RKH][RKS][GDENSA]']\n",
      "PS00206 ['[YI].GA[FLI][KRHNQS]CL.{3,4}G[DENQ]V[GAT][FYW]']\n",
      "PS00207 ['[DENQK][YF].[LY]LC.[DN].{5,8}[LIV].{4,5}C.{2}A.{4}[HQR].[LIVMFYW][LIVM]']\n",
      "PS00208 ['[SN]P.[LV].{2}HA.{3}F']\n",
      "PS00209 ['Y[FYW].ED[LIVM].{2}N.{6}H.{3}P']\n",
      "PS00210 ['T.{2}RDP.[FY][FYW]']\n",
      "PS00211 ['[LIVMFYC][SA][SAPGLVFYKQH]G[DENQMW][KRQASPCLIMFW][KRNQSTAVM][KRACLVM][LIVMFYPAN][ACDEFGIKLMNQRSTVW][LIVMFW][SAGCLIVP][ACDEGIKLMNQRSTV][ACDEFGILMNQSTVWY][LIVMFYWSTA]']\n",
      "PS00212 ['[FY].{6}CC.{2}[ADEFGHIKLMNPQRSTVWY].{4}C[LFY].{6}[LIVMFYW]']\n",
      "PS00213 ['[DENG][CDEFGHIKLMNPQRSTVWY][DENQGSTARK].{0,2}[DENQARK][LIVFY][ADEFGHIKLMNQRSTVWY]G[ADEFGHIKLMNPQRSTVWY]W[FYWLRH][ACEFGHIKLMNPQRSTVWY][LIVMTA]']\n",
      "PS00214 ['[GSAIVK][ACDGHIKLMNPQRSTVWY][FYW].[LIVMF].{2}[ACDEFGHILMNPQRSTVWY].[NHG][FY][DE].[LIVMFY][LIVM][ACDEFGHIKLMPQRSTVWY][ACDEFHIKLMNPQRSTVWY][LIVMAKR]']\n",
      "PS00216 ['[LIVMSTAG][LIVMFSAG][ACDEFGIKLMNPQRTVWY][ACFGHIKLMNPQSTVWY][LIVMSA][DE][ACEFGHIKLMNPQRSVWY][LIVMFYWA]GR[RK].{4,6}[GSTA]']\n",
      "PS00217 ['[LIVMF].G[LIVMFA][ACDEFGHIKLMNPQRSTWY].G[ACDEFGHILMNQRSTVWY].{7}[LIFY].{2}[EQ].{6}[RK]']\n",
      "PS00218 ['[STAGC]G[PAG].{2,3}[LIVMFYWA]{2}.[LIVMFYW].[LIVMFWSTAGC]{2}[STAGC].{3}[LIVMFYWT].[LIVMST].{3}[LIVMCTA][GA]E.{5}[PSAL]']\n",
      "PS00219 ['FGG[LIVM]{2}[KR]D[LIVM][RK]RRY']\n",
      "PS00220 ['[FI]LISLIFIYETF.KL']\n",
      "PS00221 ['[HNQA][ACEFGHIKLMNPQRSTVWY]NP[STA][LIVMF][ST][LIVMF][GSTAFY]']\n",
      "PS00222 ['[GP]C[GSET][CE][CA].{2}C[ALP].{6}C']\n",
      "PS00223 ['[TG][STV].{8}[LIVMF].{2}R.{3}[DEQNH].{2}[ACDEFGHIKLMNPQRTVWY].{4}[IFY].{7}[LIVMF].{3}[LIVMF].{5}[ACDEFGHKLMNPQRSTVWY].{5}[LIVMFA].{2}[LIVMF]']\n",
      "PS00224 ['FLA[QH][QE]ES']\n",
      "PS00226 ['[IV][ACDEFGHILMNPQRSTVWY][TACI]Y[RKH][ACDFGHIKLMNPQRSTVWY][LM]L[DE]']\n",
      "PS00227 ['[SAG]GGTG[SA]G']\n",
      "PS00228 ['^MR[DE][IL]']\n",
      "PS00229 ['GS.{2}N.{2}H.[PA][AG]G{2}']\n",
      "PS00230 ['[STAGDN]Y.YE[CDEFGHIKLMNPQRSTWY][ACDEFGHIKMNPQRSTVWY][DE][KR][STAGCI]']\n",
      "PS00231 ['C[DE][YF]NRD']\n",
      "PS00232 ['[LIV].[LIV].D.ND[NH].P']\n",
      "PS00233 ['G.{7}[DEN]G.{6}[FY].A[DNG].{2,3}G[FY].[APV]']\n",
      "PS00234 ['[LIVM].[DE][LIVMFYT][LIVM][DE].[LIVM]{2}[DKR]{2}G.[LIVMA][LIVM]']\n",
      "PS00235 ['FL.{2}T.{3}R.{3}A.{2}Q.{3}L.{2}F']\n",
      "PS00236 ['C.[LIVMFQ].[LIVMF].{2}[FY]P.D.{3}C']\n",
      "PS00237 ['[GSTALIVMFYWC][GSTANCPDE][ACFGILMNQSTVWY].[ACDEFGHIKLMNRSTVWY][LIVMNQGA][ACDEFGHILMNPQSTVWY][ACDEFGHILMNPQSTVWY][LIVMFT][GSTANC][LIVMFYWSTAC][DENH]R[FYWCSH][ACDFGHIKLMNQRSTVWY].[LIVM]']\n",
      "PS00238 ['[LIVMFWAC][PSGAC].[ACDEFHIKLMNPQRSTVWY].[SAC]K[STALIMR][GSACPNV][STACP].{2}[DENF][AP].{2}[IY]']\n",
      "PS00239 ['[DN][LIV]Y.{3}YYR']\n",
      "PS00240 ['G.H.N[LIVM]VNLLGACT']\n",
      "PS00242 ['[FYWS][RK].GFF.R']\n",
      "PS00243 ['C.[GNQ].{1,3}G.C.C.{2}C.C']\n",
      "PS00244 ['[NQH].{4}P.H.{2}[SAG].{11}[SAGC].H[SAG]{2}']\n",
      "PS00245 ['[RGS][GSA][PV]H.CH.{2}Y']\n",
      "PS00246 ['C[KR]CHG[LIVMT]SG.C']\n",
      "PS00247 ['G.[LIM].[STAGP].{6,7}[DENA]C.[FLM].[EQ].{6}Y']\n",
      "PS00248 ['[GSRE]C[KRL]G[LIVT][DE].{3}[YW].S.C']\n",
      "PS00249 ['P[PSR]CV.{3}RC[GSTA]GCC']\n",
      "PS00250 ['[LIVM].{2}P.{2}[FY].{4}C.G.C']\n",
      "PS00251 ['[LV].[LIVM][ACDEFGHIKLMNPQRSTWY].[ACDEFGHIKMNPQRSTVWY]G[LIVMF]Y[LIVMFY]{2}.{2}[QEKHL][LIVMGT].[LIVMFY]']\n",
      "PS00252 ['[FYH][FY].[GNRCDS][LIVM].{2}[FYL]L.{7}[CY][AT]W']\n",
      "PS00253 ['[FC].S[ASLV].{2}P.{2}[FYLIV][LI][SCA]T.{7}[LIVM]']\n",
      "PS00254 ['C.{9}C.{6}GL.{2}[FY].{3}L']\n",
      "PS00255 ['N.[LAP][SCT]FLK.LL']\n",
      "PS00256 ['Q[LV][NT][FY][ST].{2}W']\n",
      "PS00257 ['WA.G[SH][LF]M']\n",
      "PS00258 ['C[SAGDN][STN].{0,1}[SA]TC[VMA].{3}[LYF].{3}[LYF]']\n",
      "PS00259 ['Y.{0,1}[GD][WH]M[DR]F']\n",
      "PS00260 ['[YH][STAIVGD][DEQ][AGF][LIVMSTE][FYLR].[DENSTAK][DENSTA][LIVMFYG].{8}[ACDEFGHILMNPQRSTVWY][KREQL][KRDENQL][LVFYWG][LIVQ]']\n",
      "PS00261 ['C[STAGM]G[HFYL]C.[ST]']\n",
      "PS00262 ['CC[ACDEFGHIKLMNQRSTVWY][ACDEFGHIKLMNQRSTVWY].C[STDNEKPI].{3}[LIVMFS].{3}C']\n",
      "PS00263 ['CFG.{3}[DEA][RH]I.{3}S.{2}GC']\n",
      "PS00264 ['C[LIFY][LIFYV].N[CS]P.G']\n",
      "PS00265 ['[FY].{3}[LIVM].{2}Y.{3}[LIVMFY].R.R[YF]']\n",
      "PS00266 ['C.[STN].{2}[LIVMFYS].[LIVMSTA]P.{5}[TALIV].{7}[LIVMFY].{6}[LIVMFY].{2}[STACV]W']\n",
      "PS00267 ['F[IVFY]G[LM]M[G]', 'F[IVFY]G[LM]M$']\n",
      "PS00268 ['W.{0,2}[KDN][ACDEFGHIKLMNPRSTVWY][ACDEFGHIKMNPQRSTVWY]K[KRE][LI]E[RKN]']\n",
      "PS00269 ['C.C.{3,5}C.{7}G.C.{9}CC']\n",
      "PS00270 ['C.C.{4}D.{2}C.{2}[FY]C']\n",
      "PS00271 ['CC.{5}R.{2}[FY].{2}C']\n",
      "PS00272 ['GC.{1,3}CP.{8,10}CC.{2}[PDEN]']\n",
      "PS00273 ['CC.{2}CC.PAC.GC']\n",
      "PS00274 ['[KT].{2}NW.{2}T[DN]T']\n",
      "PS00275 ['[LIVMA].[LIVMSTA]{2}.E[SAGV][STAL]R[FY][RKNQST].[LIVM][EQS].{2}[LIVMF]']\n",
      "PS00276 ['T.{2}W.P[LIVMFY]{3}.{2}E']\n",
      "PS00277 ['YGG[LIV]T[ACDEFGHKLMNPQRSTVWY][ACDEFGHIKLMPQRSTVWY].{2}N']\n",
      "PS00278 ['K.{2}[LIVF].{4}[LIVF]D.{3}R.{2}L.{5}[LIV]Y']\n",
      "PS00279 ['Y.{6}[FY]GTH[FY]']\n",
      "PS00280 ['F.{2}[ACDEFGHKLMNPQRSTVWY]GC.{6}[FY].{5}C']\n",
      "PS00281 ['C.{5,6}[DENQKRHSTA]C[PASTDH][PASTDK][ASTDV]C[NDEKS][DEKRHSTA]C']\n",
      "PS00282 ['C.{4}[ADEFGHIKLMNPQRSTVWY].{2}C.[CDEFGHIKLMNPQRSTVWY].{4}Y.{3}C.{2,3}C']\n",
      "PS00283 ['[LIVM].D[ACDFGHILMNPQRSTVWY][EDNTY][DG][RKHDENQ].[LIVM].[ACDFGHIKLMNPQRSTVWY][ACDEFGHIKLMNPRSTVWY].{2}Y.[LIVM]']\n",
      "PS00284 ['[LIVMFY][ACDEFHIKLMNPQRSTVWY][LIVMFYAC][DNQ][RKHQS][PST]F[LIVMFY][LIVMFYC].[LIVMFAH]']\n",
      "PS00285 ['[FYW]P[EQH][LIV]{2}G.{2}[STAGV].{2}A']\n",
      "PS00286 ['CP.{5}C.{2}[DN].DC.{3}C.C']\n",
      "PS00287 ['[GSTEQKRV]Q[LIVT][VAF][SAGQ]G[ACEFHIKLMNPQRSTVWY][LIVMNK][ACDEFGHILMNPQRSVWY].[LIVMFY][ACDEFGHIKLMNPQRTVWY][LIVMFYA][DENQKRHSIV]']\n",
      "PS00288 ['C.C.P.HPQ.{2}[FIV]C']\n",
      "PS00289 ['H.C.[ST]W.[ST]']\n",
      "PS00290 ['[FY][ACDEFGHIKMNPQRSTVWY]C[CEFHIKLMNQRSTVWY][VA][ADEFGHIKMNPQRSTVWY]H']\n",
      "PS00291 ['AGAAAAGAVVGGLGGY']\n",
      "PS00292 ['R.{2}[LIVMSA].{2}[FYWS][LIVM].{8}[LIVMFC].{4}[LIVMFYA].{2}[STAGC][LIVMFYQ].[LIVMFYC][LIVMFY]D[RKH][LIVMFYW]']\n",
      "PS00293 ['[RKA]C[DE][RH].{3}[LIVMF].{3}[LIVM].[SGAN][LIVMF].K[LIVMF]{2}']\n",
      "PS00294 ['C[ACFGHIKLMPRSTVWY][LIVM].$']\n",
      "PS00295 ['[FY]RYG.[DE]{2}.[DE][LIVM]{2}G[LIVM].F.[RK][DEQ][LIVM]']\n",
      "PS00296 ['A[AS][ACDEFGHIKMNPQRSTVWY][DEQ]E[CDEFGHIKLMNPQRSTVWY][ACDEFGHIKLMNPRSTVWY][ACDEFGHIKLMNPQSTVWY].GG[GA]']\n",
      "PS00297 ['[IV]DLGT[ST].[SC]']\n",
      "PS00298 ['Y.[NQHD][KHR][DE][IVA]F[LM]R[ED]']\n",
      "PS00299 ['K.{2}[LIVM].[DESAK].{3}[LIVM][PAQ].{3}Q.[LIVM][LIVMC][LIVMFY].G.{4}[DE]']\n",
      "PS00300 ['P[LIVM].[FYL][LIVMAT][GS][ACDEFGHIKLMNPRSTVWY][GS][EQ].[ACDEFGHILMNPQRSTVWY].{2}[LIVMF]']\n",
      "PS00301 ['D[KRSTGANQFYW].{3}E[KRAQ].[RKQD][GC][IVMK][ST][IV].{2}[GSTACKRNQ]']\n",
      "PS00302 ['[PT]GKHG.AK']\n",
      "PS00303 ['[LIVMFYW]{2}.{2}[LKQ]D.{3}[DN].{3}[DNSG][FY].[ES][FYVC].{2}[LIVMFS][LIVMF]']\n",
      "PS00304 ['K.E[LIV]A.[DE][LIVMF]G[LIVMF]']\n",
      "PS00305 ['NG.[DE]{2}.[LIVMF]C[ST].{11,12}[PAG]D']\n",
      "PS00306 ['CL[LV]A.A[LVF]A']\n",
      "PS00307 ['[LIV][STAG]V[DEQV][FLI]D[ST]']\n",
      "PS00308 ['[LIV][CDEFGHIKMNPQRSTVWY][EDQ][FYWKR]V[ACDEGHIKLMNPQRSTWY][LIVF]G[LF][ST]']\n",
      "PS00310 ['[STA]C[LIVM][LIVMFYW]A.[LIVMFYW].{3}[LIVMFYW].{3}Y']\n",
      "PS00311 ['C.{2}D.{3,4}[LIVM]{2}P[LIVM].[LIVM]G.{2}[LIVM].G[LIVM]{2}.[LIVM]{4}A[FY].[LIVM].{2}[KR][RH].{1,2}[STAG]{2}Y[EQ]']\n",
      "PS00312 ['II.[GAC]VMAG[LIVM]{2}']\n",
      "PS00313 ['[IVM].GQD.VK.{5}[KN]G.{3}[STLV]']\n",
      "PS00314 ['AGYGST.T']\n",
      "PS00315 ['S{4}[SD][DE].[DE][GVE].{1,7}[GE].{0,2}[KR]{4}']\n",
      "PS00316 ['G.[GF].C.T[GA]DC.{1,2}[GQ].{2,3}C']\n",
      "PS00318 ['[LIVM]G.[LIVM]GG[AG]T']\n",
      "PS00319 ['G[VT][EK][FY]VCCP']\n",
      "PS00320 ['GYENPTY[KRS]']\n",
      "PS00321 ['AL[KR][IF][FY][STA][STAD][LIVMQ]R']\n",
      "PS00322 ['KAPRK[QH][LI]']\n",
      "PS00323 ['[STDNQ]G[KRNQMHSI].{6}[LIVM].{4}[LIVMC][GSD].{2}[LFI][GAS][DE][FYM].{2}[ST]']\n",
      "PS00324 ['[LIVM].K[FY]GG[ST][SC][LIVM]']\n",
      "PS00326 ['LK[EAD]AE.RA[ET]']\n",
      "PS00327 ['[FYIV][ACEFGHIKLMPQRSTVWY][FYVG][LIVM]D[LIVMF].[STA]K.[ACDEFGHILMNPQRSTVWY][FY]']\n",
      "PS00328 ['HRHRGH.{2}[DE]{7}']\n",
      "PS00329 ['[LIVMF][LIVMFY][DN][LIVMFS]G[GSH][GS][AST].{3}[ST][LIVM][LIVMFC]']\n",
      "PS00330 ['D.[LI].{4}G.D.[LI].GG.{3}D']\n",
      "PS00331 ['[FM].[DV]D.{2}[GS]T[GSA].[IV].[LIVMAT][GAST][GASTC][LIVMFA][LIVMFY]']\n",
      "PS00332 ['G[GNHD][SGA][GR].R.[SGAWRV]C.{2}[IV]']\n",
      "PS00333 ['EG[LIVMA][LIVM][LIVMA][KR].{5,8}[YW][QNEKTI].{2,6}[KRH].{3,5}K[LIVMFY]K']\n",
      "PS00335 ['VSE.Q.{2}H.{2}G']\n",
      "PS00336 ['[FY]E[LIVM]GS[LIVMG][SA]K']\n",
      "PS00337 ['[PA].S[ST]FK[LIV][PALV].[STA][LI]']\n",
      "PS00338 ['C[LIVMFY][ACDEFGHIKLMNQRSVWY].D[LIVMFYSTA].[ACDEFGHIKLMNPQRTVWY][ACDEFGHILMNPQSTVWY][CDEFGHIKLMNPQRSTVWY].[LIVMFY].{2}[LIVMFYT].{2}C']\n",
      "PS00341 ['IPCCPV']\n",
      "PS00342 ['[STAGCN][RKH][LIVMAFY]$']\n",
      "PS00344 ['C.[DNEHQSTI]C.{4,6}[ST].{2}[WM][HR][RKENAMSLPGQT].{3,4}[GNEP].{3,6}C[NES][ASNR]C']\n",
      "PS00345 ['L[FYW][QEDH]F[LI][LVQK][ACDEFGHIKLMPQRSTVWY][LI]L']\n",
      "PS00346 ['[RKHN].{2}M.Y[DENQ].[LIVM][STAG]R[STAG][LI]R.Y']\n",
      "PS00347 ['C[KR].C.{3}I.[KAL].{3}[RG].{16,18}W[FYH]H.{2}C']\n",
      "PS00348 ['MCNSSC[MV]GGMNRR']\n",
      "PS00349 ['RKRKYFKKHEKR']\n",
      "PS00350 ['[RGS].[RKS].{5}[IL].[DNGSK].{3}[KR].{2}T[FY].[RK]{3}.{2}[LIVM].[KE]K[AT].[EQ][LIVM][STA].L.{4}[LIVM].[LIVM][LIVMT][LIVM].{6}[LIVMFC].{2}[FYLS]']\n",
      "PS00351 ['Y.[PK].{2}[IF].{2}[LIVM]{2}.[KRH].{3}P[RKQ].{3}L[LIVM]F.[STN]G[KR][LIVMA].{3}G[TAGL][KR].{7}[AGCS].{7}[LIVMF]']\n",
      "PS00352 ['[FYKH]G[FL][IL].{6,7}[DER][LIVM][FQ].H.[STKR].[LIVMFYC]']\n",
      "PS00353 ['[FI]S[KR]KC.[EK]RWKT[MV]']\n",
      "PS00354 ['[AT].{1,2}[RK]{2}[GP]RGRP[RK].']\n",
      "PS00355 ['[RQ]RSA[RS]LSA[RKM][PL]']\n",
      "PS00356 ['[LIVM].[DE][LIVM]A.{2}[STAGV].V[GSTP].{2}[STAG][LIVMA].{2}[LIVMFYAN][LIVMC]']\n",
      "PS00357 ['[KR]E[LIVM][EQ]T.{2}[KR].[LIVM]{2}.[PAG][DE]L.[KR]HA[LIVM][STA]EG']\n",
      "PS00358 ['[LIVM].{2}[LIVM][STAVC][GE][QV].{2}[LIVMA].[STC].[STAG][KRH].[STA]']\n",
      "PS00359 ['[RKN].[LIVM].G[ST].{2}[SNQ][LIVM]G.[ACDEFGHIKLNPQRSTVWY][LIVM].{0,1}[DENG]']\n",
      "PS00360 ['[GS]GG.{2}[GSA][QK].{2}[SA].{3}[GSA].[GSTAV][KR][GSALVD][LIFV]']\n",
      "PS00361 ['[AV].{3}[GDNSR][LIVMSTAG].{3}GP[LIVM].[LIVM]PT']\n",
      "PS00362 ['[LIVM].{2}H[LIVMFY].{3}[ACDEFGHIKLMNPQRTVWY].D.{2}[STAGN].{3}[LF].{2}[CDEFGHIKLMNPQRSTVWY].{6}[LIVM].{2}[FY]']\n",
      "PS00363 ['[DEN][WV].{3}G[RKNM].{6}[FYW][SV].{4}[LIVM]N.{2}NV.{2}L[RKT]']\n",
      "PS00364 ['W.{4}[YF]D.{3}[DN][LIVMFYT][LIVMFY]{3}.{2}G.{2}[STAG][PVT]']\n",
      "PS00365 ['[STVN]GC.{3}C.{6}[DE][LIVMF][GAT][LIVMF]']\n",
      "PS00366 ['[LV].[LV][LIV]K[STV][ST].[SN].F.{2}[FY].{4}[FY].{2}L.{5}R']\n",
      "PS00367 ['PD.{2}H[DE][LIVF][LIVMFY]GH[LIVMC][PA]']\n",
      "PS00368 ['[IVMSEQ]E.{1,2}[LIVTA][HY][GSA].[STAVM]Y.{2}[LIVMQ].{3}[LIVFY][IVFYCSA]']\n",
      "PS00369 ['G[LIVM]H[STAV]R[PAS][GSTA][STAMVN]']\n",
      "PS00370 ['G[GA].[STN].H[STA][STAV][LIVM]{2}[STAV][RG]']\n",
      "PS00371 ['G.{2}[LIVMFA][LIVMF]{2}H[LIVMF]G[LIVMF].T[LIVA]']\n",
      "PS00372 ['[DENQ].{6}[LIVMF][GA].{2}[LIVM]A[LIVM]PH[GAC]']\n",
      "PS00373 ['G.[STMC][IVTL].[FYWVQELKR][VMAT].[DEVMKAI].[LIVMY]D.G.{2}[LIVTYA].{6}[LIVM]']\n",
      "PS00374 ['[LIVMF]PCHR[LIVMF]{2}']\n",
      "PS00375 ['[FW].{2}[QL].{2}[LIVMYA][LIMV].{4,6}[LVGAC][LVFYAHM][LIVMF][STAGCM][HNQ][STAGC]G.{2}[STAG].{3}[STAGL][LIVMFA].{4,5}[PQR][LIVMTA].{3}[PA].{2,3}[DES][QEHNR]']\n",
      "PS00376 ['[GN][AS]GDQG.{3}G[FYHG]']\n",
      "PS00377 ['G[GA]G[ASC][FY]S.K[DE]']\n",
      "PS00378 ['[LIVM]GF[TN]FS[FY]P.{5}[LIVM][DNST].{3}[LIVM].{2}WTK.[LF]']\n",
      "PS00379 ['DG.{2}AR.{7,8}G.{3}D.{3}D']\n",
      "PS00380 ['[FY].{3}H[LIV]PGA.{2}[LIVF]']\n",
      "PS00381 ['T.{2}[LIVMF]G.A[SAC]S[MSA][PAG][STA]']\n",
      "PS00382 ['R.{3}[EAP].{3}[LIVMFYT][LM][LIVM]HQP']\n",
      "PS00383 ['[LIVMF]HC.{2}G.{2}R[STC][STAGP]']\n",
      "PS00384 ['HY.[GT]D[LIVMAF][DNSH].P.H[PA].N']\n",
      "PS00385 ['P.{2}L.{3}KWE.C']\n",
      "PS00387 ['D[SGDN]D[PE][LIVMF]D[LIVMGAC]']\n",
      "PS00388 ['[FYNAGS].{4}[STNLV].[FYW]S[PDS].{0,1}G[RKHDS].{2}Q[LIVA][DENR][YH][GSAD].{2}[GSAG]']\n",
      "PS00389 ['[LIVM].[LIVMFYT].{3}[LIVMT][DENQK].[ACDEFHIKLMNPQRSTVWY][LIVM].[GSA]G[LIVMFYGA][ACDEFGHIKLMNPQRTVWY][LIVM][KRHENQ].[GSEN]']\n",
      "PS00390 ['[FYWMV].{2}[FYWM].[FYW][DN].{6}[LIVMF][GA]RT.{3}[WRL]']\n",
      "PS00391 ['[RK].{2}C[RKQWI].{5}L.{2}C[SA]G']\n",
      "PS00392 ['S[LIVMFYW].[ACDEFHILMNPQRSTVWY].{3}K[LIVMFYWGH][LIVMFYWG].[ACDEFGHIKLMNPQSTVWY].[LIVMFYW][ACDEFGHIKLMNPQRSTWY][CA].{2}[LIVMFYWQ][ACDEFGHILMNPQRSTVWY].[RK]']\n",
      "PS00393 ['[IVLC]M[LIVM]GYSDS.K[DF][STAG]G']\n",
      "PS00394 ['TG.P[LIVM]{2}DA.M[RA].[LIVM]']\n",
      "PS00395 ['[SACVLG][AIPTV].{0,1}K[ADGS][DEN][GA]YG[HACILN][GD]']\n",
      "PS00396 ['[EQ].LY[DEQSTLM].{3,12}[LIVST][ST]Y.R[ST][DEQSN]']\n",
      "PS00397 ['Y[LIVAC]R[VA]S[ST].{2}Q']\n",
      "PS00398 ['G[DE].{2}[LIVM][ACDFGHIKLMNPQRSTVWY].[ACDEFGHIKLMNPQRSTWY][LIVM][DT]R[LIVM][GSA]']\n",
      "PS00399 ['G.{2}A.{4,7}[RQT][LIVMF]GH[AS][GH]']\n",
      "PS00400 ['[PA][GA][LIVMC].{2}R[IV][ST].{3}L.{5}[EQAV].{4}[LIVM][EQK].{8}P']\n",
      "PS00401 ['K.[NQEK][GT]G[DQ].[LIVM].{3}QS']\n",
      "PS00405 ['GQDQTKQQI']\n",
      "PS00406 ['[FY][LIV][GV][DE]E[ARV][QLAH].{1,2}[RKQ]{2}[GD]']\n",
      "PS00407 ['C[DNH][TL].[QT]PGC.{2}[VAIL]C[FY]D']\n",
      "PS00408 ['C.{3,4}PC.{2,3}[LIVMTA][DENT]C[FYN][LIVMQ][SA][KRH]P']\n",
      "PS00409 ['[KRHEQSTAG]G[FYLIVM][ST][LT][LIVP]E[LIVMFWSTAG]{14}']\n",
      "PS00410 ['LP[RKT][GD][STNKEA][GND][LIVMG][VICA]TR']\n",
      "PS00411 ['[GSAT][KRHPSTQVME][LIVMFY].[LIVMF][IVC][DN][LS][AH]G[SAN]E']\n",
      "PS00412 ['^MLCC[LIVM]RR']\n",
      "PS00413 ['SFRGHI.RKK[LIVM]']\n",
      "PS00414 ['^.{0,1}[STA].{0,1}W[DENQH].[YI].[DEQ]']\n",
      "PS00415 ['LRRRLSDS']\n",
      "PS00416 ['GHAH[SA]GMGK[IV]K']\n",
      "PS00417 ['[NT][LIVMF][DENSTG][KLNRGS][VAI].[DEQA]R.{2}[KRN][LIVM][STDEA].[LIVM].[DEQGN][KR][TAS][DEAS]']\n",
      "PS00418 ['[RK][FYW]A[GAP]FD.F.{2}[LV].{3}[GASTY][GASTV]']\n",
      "PS00419 ['CDGP[GE]RGGTC']\n",
      "PS00420 ['[GNRVM].{5}[GLKA].{2}[EQ].{6}[WPS][GLKH].{2}C.{3}[FYW].{8}[CM].{3}G']\n",
      "PS00421 ['[GC].{3}[LIVMFS].{2}[GSA][LIVMFTC][LIVMFA]G[CLYI].[GA][STAPL].{2}[EGAR].{2}[CWNLF][LIVMGA][LIVM]']\n",
      "PS00422 ['[DE][SN]L[SAN][ACDFHKLMNPQRSTVWY][ACDFGHIKMNPQRSVWY][DE].EL']\n",
      "PS00423 ['C[LIVM]{2}E[LIVM]{2}S[DN][STA]L.K.[SN].{3}[LIVM][STA].EC']\n",
      "PS00424 ['TE[LF].{2}L.CL.{2}EL']\n",
      "PS00426 ['C.{4}[SAGDV].{4}[SPAL][LF].{2}C[RH].[LIVMFYA]{2}.{3,4}C']\n",
      "PS00427 ['C.{2}[GS].CC.{1,2}[NQRSEKD]C.[FMYLVI].{6}C[RKNQ]']\n",
      "PS00428 ['[NV].{5}[GTR][LIVMA].P[PTLIVME].G[LIVM].{3}[LIVMFW]{2}S[YSAQ]GG[STN][SA]']\n",
      "PS00429 ['ARP.{3}K.S.TNAYNVTT.{2}[DN]G.{3}YG']\n",
      "PS00430 ['^.{10,115}[DENF][ST][LIVMF][LIVSTEQ]V[CDEFHIKLMQRSTVWY][AGP][STANEQPK]']\n",
      "PS00431 ['G[EQ]TVVPGGT']\n",
      "PS00432 ['W[IVC][STAK][RK].[DE]Y[DNE][DE]']\n",
      "PS00433 ['[RK].{4}[GAS]H.[QL][QR][GS][GF].{5}[DE][RL]']\n",
      "PS00434 ['L.{3}[FY]KH.N.[STAN]SF[LIVM]RQL[NH].Y.[FYW][RKH]K[LIVM]']\n",
      "PS00435 ['[DET][LIVMTA][ACDEFGHIKMPQRTVW][ADEGHIKLMNQSTVWY][LIVM][LIVMSTAG][SAG][LIVMSTAG]H[STA][LIVMFY]']\n",
      "PS00436 ['[SGATV][ACEFGHIKLMNPQRSTVWY].{2}[LIVMA]R[LIVMA].[FW]H[ACDEFGHIKLMNPQRSTWY][SAC]']\n",
      "PS00437 ['R[LIVMFSTAN]F[GASTNP]Y.D[AST][QEH]']\n",
      "PS00438 ['[IF].[RH].{4}[EQ]R.{2}H.{2}[GAS][GASTFY][GAST]']\n",
      "PS00439 ['[LI][PK].[LVPQ]P[IVTAL]P.[LIVMA].[DENQAS][ST][LIVMA].{2}[LY]']\n",
      "PS00440 ['R[FYW].[DA][KA].{0,1}[LIVMFY].[LIVMFY]{2}.{3}[DNS][GSA].{6}[DE][HS].{3}[DE][GAC]']\n",
      "PS00441 ['R[LIVMFYS].[LIVM].[QHG].GC[FYNA][GAPV]G[GAC][STAVK].[LIVMF][RAL]']\n",
      "PS00444 ['[LIVMFY]G.{2}[FYL]Q[LIVM].DD[LIVMFY].[DNG]']\n",
      "PS00445 ['[GSA].[LIVMFYW][ACEFGHIKLMNPQRSTVWY]G[LIVM].{7,8}[HDENQ][LIVMF][ACDFGHIKLMNRSTVWY][CEFGHKLMNPQRSVWY][AS][STALIVM][LIVMFY][DEQ]']\n",
      "PS00446 ['N[SGAT][LIVMF]RR.{9}[SAR].{3}[VA].{4}N.[STA].{3}[DN]E.[LIV][GSA].R[LI][GAS][LIVM]{2}[PV]']\n",
      "PS00447 ['R.{2}[GSAV]K.{3}[LIVMFY][AGQ].{2}Y.{2}[GS].{3}[LIVMA]']\n",
      "PS00448 ['D[LIVMFY][DNV].[DNS].{2}[LIVM][DN][SALM].D.{3}[LIVMF].[RKS].[LIVMF]']\n",
      "PS00449 ['[STAGN][ACDFGHIKLMNPQRSTVWY][STAG][LIVMF]RL[ACDEFGHIKMNQRSTVWY][SAGV]N[LIVMT]']\n",
      "PS00450 ['[LIVM].{2}[GSACIVM].[LIV][GTIV][STP]C.{0,1}TN[GSTANI].{4}[LIVMA]']\n",
      "PS00451 ['G.{2}[LIVMF].{4}E.{2,3}[CSTAENV].{8,9}[GNDS][GS]{2}[CS].{2}[KT].{4}[FY]']\n",
      "PS00452 ['[GD][VI][LIVM].{0,1}[GS].{5}[FY].[LIVM][FYWL][GS][DNTHKWE][DNTAS][IV][DNTAY].{5}[DEC]']\n",
      "PS00455 ['[LIVMFY][ACDFGHIKLMNPQRSTVWY][ACDFGHIKLMNPQRTWY][STG][STAG]G[ST][STEI][SG].[PASLIVM][KR]']\n",
      "PS00456 ['[GS].{2}[LIY].{3}[LIVMFYWSTAG]{7}.{3}[LIY][STAV].{2}GG[LMF].[SAP]']\n",
      "PS00457 ['[GAST][LIVM].{3}[KR].{4}GA.{2}[GAS][LIVMGS][LIVMW][LIVMGAT]G.[LIVMGA]']\n",
      "PS00458 ['GP.C.Y.AA.V.R.{3}HW']\n",
      "PS00459 ['K.CH.K.{2}HC.{2}K.{3}C.{8}K.{2}C.{2}[RK].KCCKK']\n",
      "PS00460 ['[GNDRC][RKHNQFYCS].[LIVMFCS][LIVMF]{2}.N[VT].[STCA].[CU][GA].[TA]']\n",
      "PS00461 ['[LIVM].[DG].{2}[GAEHS][NQSD][KS]G[TE]G.W']\n",
      "PS00462 ['T[STA]H.[ST][LIVMA].{4}G[SN].V[STA].T.T[LIVM][NE].{1,2}[FY]G']\n",
      "PS00463 ['[GASTPV]C.{2}C[RKHSTACW].{2}[RKHQ].{2}C.{5,12}C.{2}C.{6,8}C']\n",
      "PS00464 ['[RKQN].{2}[ACDEFHIKLMNPQRSTVWY].[RH][GAS].G[KRQS].{8}[ACDEFGHIKMNPQRSTVWY][HDN][LIVM][CDEFGHIKLMNPQRSTVWY][LIVMS].[LIVM]']\n",
      "PS00465 ['SQ[STK][TA]I[SC]R[FH][ET].[LSQ].{0,1}[LIR][ST]']\n",
      "PS00466 ['C.{2}C.{9}[LIVMQSART][QH][STQLMI][RA][SACR].[DE][DET][PGSEAM].{6}C.{2,5}C.{3}[FWE]']\n",
      "PS00467 ['P.{2}RG[STAIV]{2}.N[APK].[DE]']\n",
      "PS00468 ['[SN][VT]DT[GAME]A[LIVM][AV].[LM]A[LIVMF][ST]C']\n",
      "PS00469 ['N.{2}H[GA]SD[GSA][LIVMPKNE]']\n",
      "PS00470 ['[NSK][LIMYTV][FYDNH][GEA][DNGSTY][IMVYL].[STGDN][DN].{1,2}[SGAP].{3,4}[GE][STG][LIVMPA][GA][LIVMF]']\n",
      "PS00471 ['C.C[LIVMS].{4,6}[LIVMFY].{2}[RKSEQNA].[LIVM].{2}[LIVMA].{5}[STAG].{2}C.{3}[EQ][LIVM]{2}.{9,10}C[LV][DN]']\n",
      "PS00472 ['CC[LIFYTRQ].{5,8}[LIR].{4}[LIVMFA].{2}[FYWECI].{5,8}C.{3,4}[SAG][LIVM]{2}[FL].{7,9}C[STAV]']\n",
      "PS00473 ['Q[HY][FYW]S.{4}PG']\n",
      "PS00474 ['[FL].{6}[DN].{2}[AGS].[ST].G[KRH]G.{2}G.{3}R']\n",
      "PS00475 ['[KR][LIVM]{2}[GASL].[GT].[LIVMA].{2,5}[LIVMF].[LIVMF].{3,4}[LIVMFCA][ST].{2}A.{3}[LIVM].{3}G']\n",
      "PS00476 ['GE.[FYN]HN[FY]HH.FP.DY']\n",
      "PS00477 ['[PG].[GS]C[GA]E[EQ].[LIVM]']\n",
      "PS00478 ['C.{2}C.{15,21}[FYWHPCR]H.{2}[CH].{2}C.{2}C.{3}[LIVMF]']\n",
      "PS00479 ['H.[LIVMFYW].{8,11}C.{2}C.{3}[LIVMFC].{5,10}C.{2}C.{4}[HD].{2}C.{5,9}C']\n",
      "PS00480 ['G[FYAV][GA]H.[IV].{1,2}[RKTQ].{2}[DV][PS]R']\n",
      "PS00481 ['[RK]ECTGL.WEWW[RK]']\n",
      "PS00482 ['D[LIVMFYWSAP]H[LIVA]H[LIVF][RN].[PGANF]']\n",
      "PS00483 ['[GAVS][ST]D.APH.{4}K']\n",
      "PS00484 ['[FYWHPVAS].{3}C.{3,4}[SG].[FYW].{3}Q.{5,12}[FYW]C[VA].{3,4}[SG]']\n",
      "PS00485 ['[SA][LIVM][NGS][STA]DDP']\n",
      "PS00486 ['[STA][LIVMF].[LIVM].DE[LIVMFY][GCA][RKHAS][GS][GST].{4}G']\n",
      "PS00487 ['[LIVMT][RK][LIVM]G[LIVM]G.G[SRK][LIVMAT]C.T']\n",
      "PS00488 ['[GS][STG][LIVM][STG][SAC]SG[DH]L.[PN]L[SA].{2,3}[SAGVTL]']\n",
      "PS00489 ['[LIVMF].R.{3}K.{2}[LIVMF]M[PT].{2}Y']\n",
      "PS00490 ['[STA].[STAC]{2}.{2}[STA]D[LIVMY]{2}LP.[STAC]{2}.{2}E']\n",
      "PS00491 ['[HA][GSYR][LIVMT][SG]H.[LIV]G[LIVMNKS].[IVEL][HNC][DEV]']\n",
      "PS00492 ['CKPCLK.TC']\n",
      "PS00493 ['CL[RK]M[RK].[EQ]C[ED]KC']\n",
      "PS00494 ['[GA][LIVM]P[LIVM].[LIVMFY].W.{6}[RK].{6}Y.{3}[AR]']\n",
      "PS00495 ['C.{3}[LIVMFY].{5}[LIVMFY].{3}[DENQ][LIVMFY].{10}C.{3}CT.{4}C.[LIVMFY]F.[FY].{13,14}C.[LIVMFY][RK].[ST].{14,15}SG.[ST][LIVMFY].{2}C']\n",
      "PS00496 ['Y[KR]G[AS][AE]Y']\n",
      "PS00497 ['H.{4,5}F[LIVMFTP].[FW]HR.{2}[LVMT].{3}E']\n",
      "PS00498 ['DP.F[LIVMFYW].{2}H.{3}D']\n",
      "PS00500 ['L[KR]KT[DENT]T.{2}KN[PT]L']\n",
      "PS00501 ['[GS][ACDEFGHIKLMNQSTVWY]SM[ACDEFGHIKLMNPQTVWY][PS][AT][LF]']\n",
      "PS00502 ['[GSDENKRH].{2}[VMFC].{2}[GS]HG[LIVMAG].{1,2}[LIVM]GS']\n",
      "PS00503 ['[IV].G[STAD][LIVT]D[FYI][IV][FSN]G']\n",
      "PS00504 ['R[ST]H[ST].{2}A.GG']\n",
      "PS00505 ['[FY]PS[AGMS]CGKT[NS]']\n",
      "PS00506 ['H.CGGNVGD']\n",
      "PS00507 ['RG[LIVMF]E.{15}[QESMP][RK].C[GR][LIVM]C']\n",
      "PS00508 ['[FY]D[PI][CU][LIMAV][ASG]C.{2,3}H']\n",
      "PS00509 ['[GSNA].[LIVMF][FYCI][LIVMFY]R[LIVMFY]{2}[GACNS][PAV][AV][LIV][LIVM][SGANT]P']\n",
      "PS00510 ['[KR][DENQ][HN].{2}GLN.G.WDY[LIVM]F']\n",
      "PS00511 ['[PQA].[LIVM]S[LIVM].{2}[PST][LIVMF].[LIVM]LR.{2}[LIVMW]']\n",
      "PS00512 ['G[LIVMFY].{2}[LIVMFY].[LIVM]D[DF].{1,2}W.{3,7}[RV][DNSF]']\n",
      "PS00513 ['GI[GR]P.Y.{2}K.{2}R']\n",
      "PS00514 ['WW[LIVMFYW].{2}C.{2}[GSA].{2}NG']\n",
      "PS00515 ['[GSA]Q.KS[FY].Q.K[SA]']\n",
      "PS00516 ['G[IV][GK].W[ST][AVI].[LIVMFY]{2}.[LIVM].{8}[MF].{2}[ED]D']\n",
      "PS00517 ['[DEQ][KRQT][LMF]E[FYW][LV]GD[SARHG]']\n",
      "PS00518 ['C.H.[LIVMFY]C.{2}C[LIVMYA]']\n",
      "PS00519 ['[GSTAP].{2}[DNEQA][LIVM][GSA].{2}[LIVMFYT][GAN][LIVMST][ST].{6}R[LIVT].{2}[LIVM].{3}G']\n",
      "PS00520 ['[KQS].{4}C[QYCS].{4}[LIVM]{2}.[FLR][FYT][LMVR].[DERTI][IV][LMF]']\n",
      "PS00521 ['[PALF].{2,3}[LIV].{3}[LIVM][STAC][STV].[GANK]G.T.{2}[AG][LIV].{2}[LMF][DENQK]']\n",
      "PS00522 ['G[SG][LFY].R[GE].{3}[SGCL].D[LIVM]D[LIVMFY]{3}.{2}[SAP]']\n",
      "PS00523 ['[SAPG][LIVMST][CS][STACG]P[STA]R.{2}[LIVMFW]{2}[TAR]G']\n",
      "PS00524 ['C.C.{3}C.{5,6}CC.[DN][FY].{3}C']\n",
      "PS00525 ['[PS][DENS].YK[GA]KG[LIVM]']\n",
      "PS00526 ['Q[KR]R[LIVM].[SA].{4}[CV]G.{3}[IV][WK][LIVF][DNSV][PE]']\n",
      "PS00527 ['[RP].{0,1}C.{11,12}[LIVMF][ACDEFGHIKMNPQRSTVWY][LIVMF][SC][RG].[ACEFGHIKLMNPQRSTVWY][ACDEFGHILMNQRSTVWY][RN]']\n",
      "PS00528 ['H.KR[LIVMF][SANK].P.{2}[WY].[LIVM].[KRP]']\n",
      "PS00529 ['[FYAT]G.{2}[KREILYV][STAI].[GCAVI][FYKRE][GTALV].[LIVYA]Y[DENQAKY][STDN].{7}E']\n",
      "PS00530 ['[FYWL].[LIVM]HGL[WY]P']\n",
      "PS00531 ['[LIVMF].{2}[HDGTY][EQ][FYW].[KRT]H[GA].C']\n",
      "PS00532 ['LIGDDEH.W.[DEPKVNA].[GVS][IV].N']\n",
      "PS00533 ['E[KR].[LIVMFAT].{3}[LIVMFAC].[GSALV][GSANHD]C.[IVTACS][PLA][LIVMF][GSA]']\n",
      "PS00534 ['[LIVMF][LIVMFC].[ST].H[GS][LIVM]P.{4,5}[DENQKRLHAFSTI].[GN][DPC].{1,4}[YA]']\n",
      "PS00535 ['[LIVMH]H[RT][GA].EK[LIVMTN].E.[KRQ]']\n",
      "PS00536 ['K[AI][CL]SGK[FI].[PQ]']\n",
      "PS00537 ['[LIV].[ST][LIVF]R[FYW].{2}[IVL]H[STGAV][LIV][STGA][IV]P']\n",
      "PS00538 ['RTE[EQ]Q.{2}[SA][LIVM].[EQ]TAASMEQLTATV']\n",
      "PS00539 ['F[GSTV]PRL[G]', 'F[GSTV]PRL$']\n",
      "PS00540 ['E.[KR]E.{2}E[KR][LF][LIVMA].{2}QN.R.GR']\n",
      "PS00541 ['SKRKYRK']\n",
      "PS00542 ['ERE.{2}[DE][LIVMFY]{2}.{6}[HK].{3}[KRP].[LIVM][LIVMYS]']\n",
      "PS00543 ['[LIVM].{2}G[LM].{3}[STGAV].[LIVMT].[LIVMTK][GE].[KR].[LIVMFYW]{2}.[LIVMFYW]{2}[LIVMFYWK]']\n",
      "PS00544 ['RIARN[TQ].{2}[LIVMFY]{2}.[EQH]E.{4}[KRN].{2}DP.[GSA]GS']\n",
      "PS00545 ['[NS].TNH.Y[FW]N[LI]']\n",
      "PS00546 ['PRC[GN].P[DR][LIVSAPKQ]']\n",
      "PS00547 ['[GT]Q[CA]WV.[SA][GAS][IVT].{2}T.[LMSC]R[CSAG][LV]G']\n",
      "PS00548 ['[GSTA][KR].{6}G.[LIVMT].{2}[NQSCH].{1,3}[LIVFCA].{3}[LIV][DENQ].{7}[LMT].{2}G.{2}[GS]']\n",
      "PS00549 ['^M.G.{3}[IV][LIV].{2}[LM].{3}L.{3}L']\n",
      "PS00550 ['HF.{2}[EQ][ENQ].{2}[LMF].{4,7}[FY].{5,6}H.{3}[HR]']\n",
      "PS00551 ['[STAN].[CH].{2,3}C[STAG][GSTVMF].C.[LIVMFYW].[LIVMA].{3,4}[DENQKHT]']\n",
      "PS00552 ['[GSA].[LIVMFA][ASM].{2}[STACLIV][GSDENQR][LIVC][STANHK].{3}[LIVM][RHF].[YW][DEQ].{2,3}[GHDNQ][LIVMF]{2}']\n",
      "PS00553 ['ES.L.R.{2}[KR].L.{4}[KR]{2}.{2}[DE].L']\n",
      "PS00554 ['GRNELI.{2}[YH]I.{3}[TC].{3}RT[RK]{2}Q[LIVM]SSH[LIVM]QV']\n",
      "PS00555 ['[FYW].[PSTA].{7}G.[LIVM].[LIVM].[FYWI].{2}D.{5}P']\n",
      "PS00556 ['[LIVMA]{4}C[LIVMFA]T[LIVMA]{2}.{4}[LIVM].[RG].{2}L[CY]']\n",
      "PS00557 ['SNHG[AG]RQ']\n",
      "PS00558 ['[YH].{2}D[SPCAD].[STA].{3}[TAG][KR][LIVMF][DNSTA][DNS].{4}[GSTAN][LIVMA].[LIVMY]']\n",
      "PS00559 ['[GA][CDEFGHIKLMNPQRSTVWY].{2}[KRNQHT].{11,14}[LIVMFYWS].{3}[ACDEFGHIKLMNPQRSTWY][ACDEFHILMNPQRSTVWY].{3}[LIVMF].C.{2}[DEN]R.{2}[DE]']\n",
      "PS00560 ['[LIVF].{2}[LIVSTA].[IVPST].[GSDNQL][SAGV][SG]H.[IVAQ]P.{3}[PSA]']\n",
      "PS00561 ['WN[STAGR][STDN][LIVM].{2}[GST].[GST].{2}[LIVMFT][GA]']\n",
      "PS00562 ['CGG.{4,7}G.{3}C.{4,5}C.{3,5}[NHGS].[FYWMI].{2}QC']\n",
      "PS00563 ['P[KRQ][KR]{2}[DE].SL[EG]E']\n",
      "PS00564 ['[ASL][FY]SGG[LV]DT[ST]']\n",
      "PS00565 ['G.T.[KRM]GND.{2}RF']\n",
      "PS00566 ['[GST][LIVMAPKR][IVEAT][FY][GSAC][IVL]E[FYV][SA].{0,1}[REA].{2}[RQSFT][DEK]']\n",
      "PS00567 ['K[LIVM].RD.{3}RG.[ST].E']\n",
      "PS00568 ['S[KR]S.K[AG].[SA]EKK[STA]K']\n",
      "PS00569 ['VVHFFKN']\n",
      "PS00570 ['C.HR[GAR].{7,8}[GEKVI][NERAQ].{4,5}C.[FY]H']\n",
      "PS00571 ['G[GAV]S[GS]{2}G.[GSAE][GSAVYCT].[LIVMT][GSA].{6}[GSAT].[GA].[DE].[GA].S[LIVM]R.P[GSACTL]']\n",
      "PS00572 ['[LIVMFSTC][LIVFYS][LIV][LIVMST]ENG[LIVMFAR][CSAGN]']\n",
      "PS00573 ['C.{2}CD[GAS].{2,4}[FYA].{4}[LIVMAT].{0,1}[LIVM]{2}[GI][GDS][GRD][DN]']\n",
      "PS00574 ['[ST][SA].{3}[QR][LI].{5,6}DY.{2}[LIVMFYW][LIVM][DE]']\n",
      "PS00575 ['G[MV]ALFCGCGH']\n",
      "PS00576 ['[LIVMFY].{2}G.{2}Y.F.K.{2}[SN][STAV][LIVMFYW]V']\n",
      "PS00577 ['[DENY].{2}[KRI][STA].{2}VG.[DN].[FW]T[KR]']\n",
      "PS00578 ['[LIVM][STAMR]GG.[DG].{2}G.[PV]M']\n",
      "PS00579 ['[KNQS][PSTLNH][ACEFGHIKLMNPQRSTVWY][ACDEGHIKLMNPQRSTVWY][LIVMFA][KRGSADN].[LIVYSTA][KR][KRHQS][DESTANQRL][LIV]A[KRCQVT][LIVMA]']\n",
      "PS00580 ['F.R.{4}[KRL].{2}[KRT][LIVMFT].{3,5}WR[KR].{2}G']\n",
      "PS00581 ['[KR][DS].[SE][KR][LIVMF][KR].[LIVM][LIVMY][LIVM].L[KA]']\n",
      "PS00582 ['[YW].[STKV].[KR][NSKQ].{3,4}[PATQS].{1,2}[LIVMF][EAQVSIT].{2}K[FYH][CSD]']\n",
      "PS00583 ['[AG]G.{0,1}[GAP].N[CDEFHIKMNPQRTVWY][STA].{2}[CDEFGHIKLMNPQRSTVWY].[ACDEFHIKLMNPQRSTVWY][CDEFHILMPQRSTVWY][GS].{9}G']\n",
      "PS00584 ['[DNSK][PSTV].[SAG]{2}[GD]D.{3}[SAGV][AG][LIVMFYA][LIVMSTAP]']\n",
      "PS00585 ['G[KRQEA].{3}[FYVIM].[ACVTI].{2}[LIVMA][LIVMAT][AG][DN].{2,3}G.[LIVMA][GS].[SAG].{5,6}[DEQGHS][LIVMARFY].{2,3}[AS][LIVMFRY]']\n",
      "PS00586 ['[KRG][KR].[GSAC][KRQVA][LIVMK][WY][LIVM][KRN][LIVM][LFY][APK]']\n",
      "PS00587 ['[LIVMKS].[LIVMFYWA]{3}[STAG]E[STACVI]G[WY]P[STN].[SAGQ]']\n",
      "PS00588 ['[GTARYQ].[ACDEFGHIKLMNPQSTVWY].{7}[LIVMYSTA]{2}[GSTA][STADEN]N[LIVM][SAN]N.[SADENFR][STV]']\n",
      "PS00589 ['[GSTADE][KREQSTIV].[ACDFGHILMNQSTVWY][ACDEFHIKMNQRSTWY].[KRDN]S[LIVMF]{2}[ACDFGHIKMNQRSTWY][LIVM][CDFGHIKLMPQRSVWY].[LIVM][GADE]']\n",
      "PS00590 ['[PST].{4}F[NQ].K.{3}C.[LF]L.{2}Y[HK]']\n",
      "PS00591 ['[GTA][CDEFHIKLMPRSTVWY][ACDEFHIKLMNPQRTWY][LIVN].[IVMF][ST]E[LIY][DN][LIVMF]']\n",
      "PS00592 ['[HLY][AILMV][FIL]G.[NSTW].{2,4}[SCTV][FY][LIVMFY][SITV]G.{1,5}[GSY].{2}[AFPSTY][FLPSV].{2}[AILPQVM][HV][DHLS][KRS]']\n",
      "PS00593 ['L[IV]AH[STACH]Y[STV][RT]Y[LIVM]G']\n",
      "PS00594 ['IG[GA]GM[LF][SA].P.{3}[SA]G.{2}F']\n",
      "PS00595 ['[LIVFYCHT][DGH][LIVMFYAC][LIVMFYA].{2}[GSTAC][GSTA][HQR]K.{4,6}G.[GSAT].[LIVMFYSAC]']\n",
      "PS00597 ['[LIVM][PA].{2}C.{1,2}[LIVM].{1,2}[LIVMST].[LIVMFY].{1,2}[LIVMF][STRD].{3}[DN]C.{2}[LIVM]']\n",
      "PS00598 ['[FYL].[LIVMC][KR]W.[GDNR][FYWLME].{5,6}[ST]W[ESV][PSTDEN].{2,3}[LIVMC]']\n",
      "PS00599 ['T[LIVMFYW][STAG]K[SAG][LIVMFYWR][SAG][ACDFGHILMPQSTVWY][ACEFGHIKLMPQSVWY][SAG]']\n",
      "PS00600 ['[LIVMFYWCS][LIVMFYWCAH].D[ED][IVA].{2,3}[GAT][LIVMFAGCYN].{0,1}[RSACLIH].[GSADEHRM].{10,16}[DH][LIVMFCAG][LIVMFYSTAR].{2}[GSA]K.{2,3}[GSTADNV][GSAC]']\n",
      "PS00601 ['W.[DNH].{5}[LIVF].[IV]PW.H.{9,10}[DE].{2}[LIVF]F[KRQ].[WR]A']\n",
      "PS00602 ['[FYVMT].{1,3}[LIVMH][APNT][LIVM].{1,2}[LIVM]H.DH[GACH]']\n",
      "PS00603 ['[GA].{1,2}[DE].Y.[STAPV].C[NKR].[CH][LIVMFYWH]']\n",
      "PS00604 ['LSV[DE]C.NKT']\n",
      "PS00605 ['[GSTA]R[NQ]P.{5}[CDEFGHIKLMNPQRSTVWY].[ACDEGHIKLMNPQRSTVWY].{2}[LIVMFYW]{2}.{3}[LIVMFYW].[DE]']\n",
      "PS00606 ['G[CDEFGHIKLMNPQRSTVWY][ACDEFHILMNPQSTVWY].{2}[LIVMFTAP][ACDEFGHIKLMNPQSTVWY].[AGC]C[STA]{2}[STAG].{2}[ACDEFGHKMNPQRSTVWY][LIVMF]']\n",
      "PS00607 ['H.HLDH[LIVM].[GS][LIVMA][LIVM]{2}.S[AP]']\n",
      "PS00608 ['[DENQLF][KRVW]N[HRY][STAPV][SAC][LIVMFS][LIVMFSA][LIVMFS]W[GSV].{2,3}NE']\n",
      "PS00609 ['H.{2}[PV].{4}[LIVMA]NDPN[GA]']\n",
      "PS00610 ['W[RK]F[GPA][YF].{4}[NYHS]GG[GCA].[FY]']\n",
      "PS00611 ['[IVTPM][DEG].{2,3}[AYEPQ]G[PT][ST][ED][LIVSTA][LIVMAECGFT][LIVMA][LIVMAYF][ACNDSTI].{2,3}[ACNGVST].{4,6}[LIVMAC][AVLKIT][SACLYWNRMTV][DEG][LIVMFCA][LIVMKFR][SAGVI].{2}EH']\n",
      "PS00612 ['C.[DNS].{2}C.{2}G[KRH].C.{6,7}P.C.C.{3,5}CP']\n",
      "PS00613 ['FP.R[IM].DWL.[NQ]']\n",
      "PS00614 ['[LIVMFY][LIVMC].E[LIVMFYC]K[KRSPQV][STAHKRYC]SP[STRK].{3,7}[LIVMFYST]']\n",
      "PS00615 ['C[LIVMFYATG].{5,12}[WL][ACDEFGHIKLMNPQRSVWY][DNSR][ADEFGHIKLMNPQRSTVWY][ACDEFGHKMNPQRSTVWY]C.{5,6}[FYWLIVSTA][LIVMSTA]C']\n",
      "PS00616 ['[LIVM].{2}[LIVMA].{2}[LIVM].RH[GN].R.[PAS]']\n",
      "PS00617 ['[LIFV].{6}[LIF][LIVF].[GSDE][GSTADNPE][PASG].{2}RR.[FYW][LIVMF][DN]']\n",
      "PS00618 ['[LIV][LIFYMV].[LIVM]D[DEA][LIVF].{2}[EHCGK]LD.{2}[KRH].{3}[LIVF]']\n",
      "PS00619 ['S[DE]C.[DE]W.W.{2}C.P.[SN].DCG[LIVMA]G.REG']\n",
      "PS00620 ['C[KR][LIVM]PCNWKK.FGA[DE]CKY.F[EQ].WG.C']\n",
      "PS00621 ['WK.KC.{2}T.[DEN]TECD[LIVM]TDE']\n",
      "PS00622 ['[GDC].{2}[NSTAVY].{2}[IV][GSTA].{2}[LIVMFYWCT].[LIVMFYWCR].{3}[NST][LIVM].{2}[ACDEFGHIKLMNPQRSVWY].{2}[NRHSA][LIVMSTA].{2}[KR]']\n",
      "PS00623 ['[GA][RKNC].[LIVW]G{2}[GST]{2}.[LIVM][NH].{3}[FYWA].{2}[PAG].{5}[DNESHQA]']\n",
      "PS00624 ['[GS][PSTA].{2}[ST][PS].[LIVM]{2}.{2}SG[LIVM]G']\n",
      "PS00625 ['G.ND.{2}[AV]LGR.T']\n",
      "PS00626 ['[LIVMFA][STAGC]{2}G.[CDEFGHIKLMNPQRSWY]H[STAGLI][LIVMFA][ACDEFGHLMNPQRSTVWY][LIVM]']\n",
      "PS00627 ['[LIVM][PK].[GSTA].{0,1}G[LM][GS]SS[GSA][GSTAC]']\n",
      "PS00628 ['P.{6}[SANG].{2}[LIVMAC].R.[ALIV][LV][QH].L[EQ]']\n",
      "PS00629 ['[FWV].{0,1}[LIVM]DP[LIVM]D[SG][ST].{2}[FYA].{0,1}[HKRNSTY]']\n",
      "PS00630 ['[WYV]D.[AC][GSA][GSAPV].[LIVFACP][LIVM][LIVAC].{3}[GH][GA]']\n",
      "PS00631 ['[NS][TS]DAEGR[LVMI]']\n",
      "PS00632 ['[LIVM][DERA].R[LI].{3}[LIVMC][VMFYHQL][KRTS].{3}[STAGCVF].[ST].{3}[SAI][KRQ].[LIVMF]{2}']\n",
      "PS00633 ['[STANVFHG].{2}[FAS].{4}[DNSPAKT].{0,7}[DENQTFG]Y[HFYLRKT].{2}[LIVMFY].{3}[LIVM].{4}[LIVM].{6,10}Y.{12,13}[LIVM].{2}N[SACF].{2}[FY]']\n",
      "PS00634 ['[IVTAS][LIVM].{2}[LF].[LI].[KRHQEG].{2}[STNQH].[IVTR].{10}[LMSN][LIV].{2}[LIVA].{2}[LMFY][IVT]']\n",
      "PS00635 ['[LIVMFY][APN].[DNS][KREQ]E[STR][LIVMAR].[FYWTE].[NCS][LIVM].{2}[LIVM]P[PAS]']\n",
      "PS00636 ['[FY][ACDEFHIKMNPQRSTVWY].[LIVMA][ACDEFGHKLMNQRSTVWY].{2}[FYWHNT][DENQSA].L.[DN].{3}[KR][ACDEGHIKLMNPQRSTVWY][ACDEFGHIKLMNQRSTVWY][FYI]']\n",
      "PS00638 ['[ST].{3}G[DY]G[KR][IV][FW][LIVM].{2}[LIVM]']\n",
      "PS00639 ['[LIVMGSTAN][ACDFGHLMNPQRSTWY]H[GSACE][LIVM][ACDEFHKLMNQRTVWY][LIVMAT]{2}G[CDEFHIKMNPQRTVWY][GSADNH]']\n",
      "PS00640 ['[FYCH][WI][LIVT].[KRQAG]N[ST]W.{3}[FYW]G.{2}G[LFYW][LIVMFYG].[LIVMF]']\n",
      "PS00641 ['P.{2}C[YWSD].{7}[GA].CR.C']\n",
      "PS00642 ['CP.C[DE].[GS]{2}.C.LQ']\n",
      "PS00643 ['RC[LIVM].C.RC[LIVMT].[LMFY]']\n",
      "PS00644 ['G[AM]G[AR]Y[LIVM]CG[DE]{2}[STA]{2}[LIM]{2}[END]S']\n",
      "PS00645 ['E[ST]CG.C.PCR.G']\n",
      "PS00646 ['[KRQSEAT][GS].RH.{2}[GSNHKLCD].{2}[LIVMCT][RNH]GQ']\n",
      "PS00647 ['[SA][GS]R[GA][LIV].{2}[TAP][GAS]GT.D.[LIVMF][EDS]']\n",
      "PS00648 ['[LIVMFYSNAD].{2}A.{2}R[NH][KRQLYAT][LIVMFSA][KRA]R.[LIVMTA][KR]']\n",
      "PS00649 ['C.{3}[FYWLIV]D.{3,4}C[FW].{2}[STAGV].{8,9}C[PF]']\n",
      "PS00650 ['[QL]G[LMFCAV][LIVMFTA][LIV].[LIVFSTM][LIFHV][VFYHLG]C[LFYAVI].[NKRQDS].{2}[VAI]']\n",
      "PS00651 ['G.{2}[GNF].{4}[VAI].{2}G[FY].{2}[NH][FYWL]L.{5}[GA].{3}[STNG]']\n",
      "PS00652 ['C.{4,6}[FYH].{5,10}C.{0,2}C.{2,3}C.{7,11}C.{4,6}[DNEQSKP][ACEFGHIKLMNQRSTVWY][ADEFGHIKLMNQRSTVWY]C']\n",
      "PS00653 ['F.[FYWM][GSTA].[GSTA].[GSTA]{2}[FYNH][NQ].E.[GSTA]']\n",
      "PS00654 ['[ST][DM]H[LIC].{2}[FA][LIY][EQK]R.{2}[QNKA]']\n",
      "PS00655 ['V.Y.{2}P.RDC[GSAF].{2}[GSA]{2}.G']\n",
      "PS00656 ['[LIVMYA][LIVA][LIVT][LIV]EPD[SAL][LI][PSAG]']\n",
      "PS00657 ['[KR]P[PTQ][FYLVQH]S[FY].{2}[LIVM].{3,4}[AC][LIM]']\n",
      "PS00658 ['W[QKR][NSD][SA][LIV]RH']\n",
      "PS00659 ['[LIV][LIVMFYWGA]{2}[DNEQG][LIVMGST][ACDFGHIKLMPQTVWY]NE[PV][RHDNSTLIVFY]']\n",
      "PS00660 ['W[LIV].{3}[KRQ].[LIVM].{2}[QH].{0,2}[LIVMF].{6,8}[LIVMF].{3,5}F[FY].{2}[DENS]']\n",
      "PS00661 ['[HYW].{9}[DENQSTV][SA].{3}[FYC][LIVM].{2,3}[ACVWD].{2}[LM].{2}[FY][GM].[DENQSTH][LIVMFYS]']\n",
      "PS00662 ['[LIVM]R.{2}PD.[LIVM]{3}GE[LIVM]RD']\n",
      "PS00663 ['[KR].[LIVMF].{3}[LIVMA].{2}[LIVM].{6}RQQEL']\n",
      "PS00664 ['[LIVM].[QA]A.{2}W[IL].[DN]P']\n",
      "PS00665 ['[GSA][LIVM][LIVMFY].{2}G[ST][TG]GE[GASNF].{6}[EQ]']\n",
      "PS00666 ['Y[DNSAH][LIVMFAN]P.{2}[STAV].{2,3}[LIVMFT].{13,14}[LIVMCF].[SGA][LIVMFNS]K[DEQAFYH][STACI]']\n",
      "PS00667 ['G[LIVMFYKRSAQT][LIVMAGPF][QAM].[LIVMFYCA].D[AGIM][LIVMFTA][KS][LVMYSTI][LIVMFYGA].[KRE][EQG]']\n",
      "PS00668 ['PFD[LIVMFYQN][STAGPVMI]E[GACS]E.{0,2}[EQLN][LIVMS].{1,2}G']\n",
      "PS00669 ['R[LIVA]{3}A[GS][LIVMFY].[TK].{3}[YFI][AG]']\n",
      "PS00670 ['[LIVMFYWA][LIVFYWC].{2}[SAC][DNQHR][IVFA][LIVF].[LIVF][HNI].P.{4}[STN].{2}[LIVMF].[GSDN]']\n",
      "PS00671 ['[LMFATCYV][KPQNHAR].[GSTDNK].[LIVMFYWRC][LIVMFYW]{2}N.[STAGC]R[GP].[LIVH][LIVMCT][DNVE]']\n",
      "PS00672 ['[ST]G[LIVMFYW]{3}[GN].{2}T[LIVM].T.{2}H']\n",
      "PS00673 ['T.{2}[GC][NQ]SGS.[LIVM][FY]']\n",
      "PS00674 ['[LIVMTR].[LIVMT][LIVMF].[GATMC][ST][NS].{4}[LIVM]D.[AS][LIFAV].{1,2}R']\n",
      "PS00675 ['[LIVMFY]{3}.G[DEQ][STE]G[STAV]GK.{2}[LIVMFY]']\n",
      "PS00676 ['[GS].[LIVMFA].{2}[AS][DNEQASH][GNEKT]G[STIM][LIVMFY]{3}[DE][EK][LIVM]']\n",
      "PS00677 ['[LIVM]{2}H[NHA]YG.[GSA]{2}.G.{5}G.A']\n",
      "PS00678 ['[LIVMSTAC][LIVMFYWSTAGC][LIMSTAG][LIVMSTAGC].{2}[DN].[ACDEFGHIKLMNQRSTVWY][LIVMWSTAC][ACEFGHIKLMNQRSTVWY][LIVMFSTAG]W[DEN][LIVMFSTAGCN]']\n",
      "PS00679 ['G.[SA]GE[LIVM]RYPSY']\n",
      "PS00680 ['[MFY].GHG[LIVMC][GSHN].{3}H.{4}[LIVM].{1,2}[HN][YWVHF]']\n",
      "PS00681 ['[LIVMFY].P[ILT].[DEN][KR][LIVMFA]{3}[KREQS].{8,9}[SG].[LIVMFY]{3}']\n",
      "PS00682 ['[LIVMFYW].{7}[STAPDNLR].{3}[LIVMFYW].[LIVMFYW].[LIVMFYW].{2}C[LIVMFYW].[STA][PSLT].{2,4}[DENSG].[STADNQLFM].{6}[LIVM]{2}.{3,4}C']\n",
      "PS00683 ['[AV].{2}[FY][DEAP]G[GSA][WF].E[FYW]']\n",
      "PS00684 ['[KRC][SAQ].G.[VF]G[GA].[LIVM].[KR][KRC][LIVM]{2}']\n",
      "PS00685 ['C[VA][ST]E.ISF[LIVM]T[SGC]EA[SCN][DE][KRQ]C']\n",
      "PS00686 ['[FY]VN[AS]KQ[FY].{2}I[ILM][KR]RR.{2}RAK[LA]E']\n",
      "PS00687 ['[LIVMFGA]E[LIMSTAC][GS]G[KNLM][SADN][TAPFV]']\n",
      "PS00688 ['[FYW]P[GS]N[LIVM]R[EQ]L.[NHAT]']\n",
      "PS00689 ['[PA]VA.{2}C.C.{2}C.{4}[STDAI][DEY]C.{6,8}[PGSTAVMI].{2}C']\n",
      "PS00690 ['[GSAH].[LIVMF]{3}DE[ALIV]H[NECR]']\n",
      "PS00691 ['[DN]R.R[LIVM][LIVMN].[STA][STAQ]F[LIVMFA].K.L.{2,3}W[KRQ]']\n",
      "PS00692 ['D.LGDVVCGGF[AGSP].P']\n",
      "PS00694 ['G[LIVMFY]N[LIVM]KYRYE']\n",
      "PS00695 ['[FYW].{2}G.GY[KR]F$']\n",
      "PS00696 ['[LI]Y[LIVM][AT].[GA][IV][SD]G.[IV]Q[HP].{2}G.{6}[IV].A[IV]N']\n",
      "PS00697 ['[EDQH][ACDEFGHILMNPQRSTVWY]K[ACFGHKLMNPQRSTWY][DN]G[ACDEFHIKMPQRSTVW]R[GACIVM]']\n",
      "PS00698 ['[FYW].D.{4}[FYW].{3}E.[STA].{3}N[STA]']\n",
      "PS00699 ['[LIVMFYH][LIVMFST]H[AG][AGSP][LIVMNQA][AG]C']\n",
      "PS00700 ['[QLS].{3}[LIVM].{2}[KRWYF].{2}R.F.DG[LIVM][YF][LIVM].{2}[KR]']\n",
      "PS00701 ['RMG.[GR]KG.{4}[FWKR]']\n",
      "PS00702 ['CP[LP]T.E[ST].C']\n",
      "PS00703 ['[STAV].S.HK.{2}[GSTAN]{2}.[STA]Q[STA]{2}']\n",
      "PS00704 ['C[SA]DSR[LIVM].[AP]']\n",
      "PS00705 ['[EQ][YF]A[LIVM].{2}[LIVM].{4}[LIVMF]{3}.GH.{2}CG']\n",
      "PS00706 ['E.[ED].K[LIVM]{2}.[KR][LIVM]{2}.[QE]MC.{2}QY']\n",
      "PS00707 ['G[AVP][DT][LIVMTAS][CG]G[FY].{3}[STP].{3}L[CL].RW.{2}[LVMI][GSA][SA][FY].P[FY].R[DNA]']\n",
      "PS00708 ['D.{3}A.{3}[LIVMFYW].{14}G.S.GG[LIVMFYW]{2}']\n",
      "PS00709 ['[STA].{5}G.[QKRN].{2}[LIVMQ][KRQT].{2}[KR].[GS].{2}[KQ].[LIVM]{3}']\n",
      "PS00710 ['[GSA][LIVMF].[LIVM][ST][PGA]SH[NIC]P']\n",
      "PS00711 ['[HQ][EQ].{3}H.[LMA][NEQHRCS][GSTA]H[LIVMSTAC]{2}.E']\n",
      "PS00712 ['[IVDYPKS].[ST]K.[LIVMTF][RIKM]N.[IV][SA]G[FY].[TV][HKRA]']\n",
      "PS00713 ['P.{0,1}[GP][DES].[LIVMF]{2}.[LIVMA][LIVM][KREQS][LIVMG][LIVM][LIVMF].[PS]']\n",
      "PS00714 ['P.G.[STA].[NT][LIVMCP]D[GA][STANQF].[LIVM][FY].{2}[LIVM].{2}[LIVM][FY][LIV][SA][QH]']\n",
      "PS00715 ['[DE][LIVMF]{2}[HEQS].G.[LIVMFA]GL[LIVMFYE].[GSAM][LIVMAP]']\n",
      "PS00716 ['[STN].{2}[DENQ][LIVMT][GAS].{4}[LIVMF][PSTG].{3}[LIVMA].[NQR][LIVMA][EQH].{3}[LIVMFWK].{2}[LIVM]']\n",
      "PS00717 ['P[LIVM].[LIVM].{2}[LIVM]A.{2}[LIVMFT].{2}[HS].ST[LIVM]SR']\n",
      "PS00718 ['RRT[IV][ATN]KYR']\n",
      "PS00719 ['N.[LIVMFYWD]R[STACN]{2}HYP.{4}[LIVMFYWS]{2}.{3}[DN].{2}G[LIVMFYW]{4}']\n",
      "PS00720 ['[VI]P[FYWVI].[GPSV].{2}[LIVMFYK].[DNE][LIVM].{13,35}[IVL]N[FYME].K']\n",
      "PS00721 ['[GN][LIVMS]KG[GST][AG][AST]G[GAS]G[YLHRKF]']\n",
      "PS00722 ['V[ASV][TS][IVLA][RQ][AGS][LIM][KER].[HN][GAS][GLKD]']\n",
      "PS00723 ['[LIVM]{2}.DD.{2,4}D.{4}RR[GH]']\n",
      "PS00724 ['PWY[ST]{2}RL']\n",
      "PS00725 ['G.{4}H.HP.[AGS].E[LIVM]']\n",
      "PS00726 ['[APF]D[LIVMF]{2}[ACDEFGHIKLMNPQRSVWY][LIVM]QE[ACDEFHIKLMNPQRSTVWY]K']\n",
      "PS00727 ['D[ST][FY][RP][KHQ].{7,8}[FYWD][ST][FYW]{2}']\n",
      "PS00728 ['N.G.R[LIVM]D[LIVMFYH].[LV].S']\n",
      "PS00729 ['H[GSAD].Y[LIF][LIMN]N[LIVMFCAP][AGC]']\n",
      "PS00730 ['[GSARY][LIVMF][CT][LIVMFY]DTCH']\n",
      "PS00731 ['[LIVMFW]H.N[DEG][SA].{4}[GNAQ].{3}D.H']\n",
      "PS00732 ['[LIVMT].[LIVM][KR]L[STAK]R[ACDFGHIKLMNPQRSTVWY]G[AKR]']\n",
      "PS00733 ['[YH]C[VI][SA]CAIH']\n",
      "PS00737 ['N.{2}GG.[LIVM][SA].GHP.[GAS].[ST]G']\n",
      "PS00738 ['[GSAP][CS]N.[FYLM]S[ST][QALKHD][DENG].[AV][AVTS][ADERQ][ACSG][LIVMCG]']\n",
      "PS00739 ['[GA][KSR].{3}[LIV].G[FY]G.[VC]G[KRL][GA].{1,2}[ASC]']\n",
      "PS00740 ['[GE].[LIVMFY]{2}.{3}[STA].{10,11}[LV].{4}[LIVMF].{6,7}C[LIVM].F.[LIVMFY].{3}[GSC]']\n",
      "PS00741 ['[LM].{2}[LIVMFYWGS][LI].{2}[PEQ][LIVMRF].{2}[LIVM].[KRS].{2}[LT].[LIVM].[DEQN][LIVM].{3}[STM]']\n",
      "PS00742 ['[DEQSKN].[LIVMF][SA][LIVMF]G[ST]ND[LIVM].Q[LIVMFYGT][STALIV][LIVMFY][GAS].{2}R']\n",
      "PS00743 ['[LI].[STN][HN].H[GSTAD]D.{2}G[GP].{7,8}[GS]']\n",
      "PS00744 ['P.{3}[LIVM]{2}.G.C[LIVMF]{2}K']\n",
      "PS00745 ['[ARH][STA].G.GGQ[HNGCSY][VI]N.{3}[ST][AKG][IV]']\n",
      "PS00746 ['E.GGP.{2}[GA].GC[AG]G']\n",
      "PS00747 ['[HY][LIVMAF].{2}[LIVM][GSTACIV][GSTAC][GSA][LIVMFA][DEQHY]S.[LIVMAS][LIVMFKS][GFA][DE].[EQRD][IV][LIVTQAM][TAGRKS]Q[LIVMF][KRE]']\n",
      "PS00748 ['[VA]H[FY]{2}[ER][DEC][GV]N[VL]']\n",
      "PS00749 ['K.[LM]RR.LP[IV][NT]R']\n",
      "PS00750 ['[RKEL][ST].[LMFY]GP.[GSA]..K[LIVMF]{2}']\n",
      "PS00751 ['[LIVM][TS][NK][DN][GA][AVNHK][TAVC][LIVM]{2}.{2}[LIVMA].[LIVM].[SNH][PQHA]']\n",
      "PS00752 ['C.[DE]C.{3}[LIVMF].{1,2}D.{2}L.{3}F.{4}C.{2}C']\n",
      "PS00753 ['[LIVM]{2}T[KR]TE.K.[DE]Y[LIVMF]{2}.D.[DE]']\n",
      "PS00754 ['[YF][LIVMFY].{2}[SC][LIVMFY][STQV].{2}[LVI]PW.{2}C.{3,4}[NWDS][GSTERHAK]']\n",
      "PS00755 ['[GSTL][LIVMFK][LIVMFCA].[LIVMF][GSAN][LIVM].P[LIVMFYN][LIVMFY].[AS][GSTQD][LIVMFAT]{3}[EQ][LIVMFA]{2}']\n",
      "PS00756 ['[LIVMFYW]{2}.[DE].[LIVM][STDNQ].{2,3}[GK][LIVMF][GST][NST]G.[GST][LIV][LIVFP]']\n",
      "PS00757 ['NPK[ST]SG.AR']\n",
      "PS00758 ['[LIV][GALMY][LIVMF][ACDEFGHIKLMNPRSTVWY][GSA]H.D[TV][STAV]']\n",
      "PS00759 ['[GSTAI][SANQCVIT]D.K[GSACN].{1,2}[LIVMA].{2}[LIVMFY].{12,17}[LIVM].[LIVMF][LIVMSTAGC][LIVMFA].{2}[DNGM]EE.{0,1}[GSTNE]']\n",
      "PS00760 ['KR[LIVMSTA]{2}[GA].[PG]G[DEQ].[LIVM].[LIVMFY]']\n",
      "PS00761 ['[LIVMFYW]{2}[CDEFGHIKMQRSTVWY][ACDEFGHIKLMNPQRSVWY]GD[NH][ACDFGHKLMNQRSTVY].{2}[SND].{2}[SG]']\n",
      "PS00762 ['[QYR][GH][DNEAR].[LIV][KR].{2}K.{2}[KRNG][AS].{4}[LIV][DENKA].{2}[IV].{2}L.{3}K']\n",
      "PS00763 ['[LIV][AGD]FP[CS][NG]QF']\n",
      "PS00764 ['C.{3}[KRSN]P[KRAGL]C.{2}C.{5}C']\n",
      "PS00765 ['[DENSA].[LIVM][GP]GR[FY][ST][LIVMFSTAP].[GSTA][PSTACM][LIVMSA][GSAN]']\n",
      "PS00766 ['[EQLT].[EQKDS][LIVM]{2}.{2}[LIVM].{2}[LIVMY]N.[DNS].{5}[LIVMF]{3}Q[LM]P[LVI]']\n",
      "PS00767 ['P[GK]G[VI]GP[MFI]T[IVA]']\n",
      "PS00768 ['[KH][IV]L[DN].{3}G.P[AG].{2}[LIVM].[IV]']\n",
      "PS00769 ['[YWF][TH][IVT][AP].{2}[LIVM][STA][PQ][FYWG][GS][FY][QST]']\n",
      "PS00770 ['E.[STAGCI].{2}N[LIVMFAC][FY].{6,12}[LIVMFA].T.{6,8}[LIVM].[GS][LIVM].[KR]']\n",
      "PS00771 ['CG[KR]CL.V.N']\n",
      "PS00772 ['V[DN]Y[EQD]FV[DN]C']\n",
      "PS00773 ['C.{4,5}FY[ST].{3}[FY][LIVMF].A.{3}[YF].{2}F[GSA]']\n",
      "PS00774 ['[LIVM][GSA]F.[STAG]{2}[LIVMFY]W[FY]W[LIVM]']\n",
      "PS00775 ['[LIVM]{2}[KR].[EQKRD].{4}G[LIVMFTC][LIVT][LIVMF][ST]D.{2}[SGADNIT]']\n",
      "PS00776 ['[PSA][LQ].E[YF]Y[LIVM]{2}[DE].[FYWHN]']\n",
      "PS00777 ['[LIVMF].{2}E[AG][YWG][QRFGS][SG][STAN]G.[SAF]']\n",
      "PS00778 ['[LIVMF].[LIVMFAG][ACDEFGHIKLMNPQRSVWY].[STAGI]HD[STANQ][ACDEFGHIKLMNPQRSTWY][LIVM].{2}[LIVMFY].{2}[STA]']\n",
      "PS00779 ['C.GCC[FY]S[RQS]A[FY]PTP']\n",
      "PS00780 ['NHT.C.C.TC.{2}HK']\n",
      "PS00781 ['[VTI].TAHPT[EQ].{2}R[KRHAQ]']\n",
      "PS00782 ['G[KR].{3}[STAGN][ACDEFGHIKLMNPQRTVWY][LIVMYA][GSTA]{2}[CSAV][LIVM][LIVMFY][LIVMA][GSA][STAC]']\n",
      "PS00783 ['[LIVM][KRVLYFS][GKR]M[LIV][PST].{4,5}[GSKR][NQEKRAH].{5}[LIVM].[AIVL][LFYV].[GDNS]']\n",
      "PS00784 ['[KFQ][RGMP][TN][FYWL][EQSG].{5}[KRHS].{4,5}GF.{2}R']\n",
      "PS00785 ['[LIVM].[LIVM]{2}[HEA][TI].D.H[GSA].[LIVMF]']\n",
      "PS00786 ['[FYPH].{4}[LIVM]GNHEF[DN]']\n",
      "PS00787 ['G[DES]SH[GC].{2}[LIVM][GTIVLAMS].[LIVTM][LIVM][DEST][GH].[PV]']\n",
      "PS00788 ['[GE].{2}S[AG]R.[ST].{3}[VT].{2}[GA][STAVY][LIVMF]']\n",
      "PS00789 ['R[SHF]D[PSV][CSAVT].{4}[SGAIVM].[IVGSTAPM][LIVM].E[STAHNCG][LIVMA]']\n",
      "PS00790 ['F.[DN].[GAW][GAS]C[LIVM][SA][LIVM]{2}[SA][LV][KRHQ][LIVA].{3}[KREQT]C[PSAWR]']\n",
      "PS00791 ['C.{2}[DE]G[DEQKRG]W.{2,3}[PAQ][LIVMT][GT].C.C.{2}G[HFY][EQ]']\n",
      "PS00792 ['[LIVM].[AG][LIVMF]{2}N.T.[DN]S[FLMI].D.[SG]']\n",
      "PS00793 ['[GE][SAV].[LIVM]{2}D[LIVMF]G[GPA].{2}[STA].P']\n",
      "PS00794 ['[KRHD].[GA][PSAE]R.{2}D[LIV]D[LIVM]{2}']\n",
      "PS00795 ['^MS[QH]Q.T[LV]PVT[LV]']\n",
      "PS00796 ['[RA]NL[LIV]S[VG][GA]Y[KN]N[IVA]']\n",
      "PS00797 ['YK[DE][SG]TLI[IML]QL[LF][RHC]DN[LF]T[LS]W[TANS][SAD]']\n",
      "PS00798 ['G[FY]R[HSAL][LIVMF]D[STAGCL][AS].{5}[EQ].{2}[LIVMCA][GS]']\n",
      "PS00799 ['C.D.{2}HCCP.{4}C']\n",
      "PS00800 ['[GSTNP].{6}[FYVHR][IVN][KEP].G[STIVKRQ]Y[DNQKRMV][EP].{3}[LIMVA]']\n",
      "PS00801 ['R.{3}[LIVMTA][DENQSTHKF].{5,6}[GSN]GH[PLIVMF][GSTA].{2}[LIMC][GS]']\n",
      "PS00802 ['[GP][DEQGSANPHVT][DN]G[PAEQ][ST][HQ].[PAGM][LIVMYACNQS][DEFYWLA].{2}[STAPG].{2}[RGANQS]']\n",
      "PS00803 ['[KRHN].[DEQN][DEQNK].{3}CGG[AG][FY][LIVM][KN][LIVMFY]{2}']\n",
      "PS00804 ['[LIVM]{2}FGPD.C[AG]']\n",
      "PS00805 ['[IVM].[DV].[DENST].{2}KP[DEH]DW[DEN]']\n",
      "PS00806 ['[LIVM]E.E[LIVM]G.{2}[GM][GSTA].E']\n",
      "PS00808 ['[AG]GG.G[STKA].L.{2}L[TA].{3}[AST].P[AS][LV]']\n",
      "PS00809 ['W[FY].G[ST][AS][DNSH][AS][LIVMFYW]']\n",
      "PS00810 ['[APV][GS]MG[LIVMN]Y[IVC][LIVMFY].{2}[DENPHKRQS]']\n",
      "PS00811 ['[AG][ST].{2}[AG].{2}[LIVM][SAD][TIF]P[LIVMF]{4}FSP[LIVM]{3}PA']\n",
      "PS00812 ['A[ST]D[AG]D.{2}[IM]A.[SA][LIVM][LIVMG].A.{3}[FW]']\n",
      "PS00813 ['[DE][IFYL].{2}F[KRL].{2}[LIVM].P.WE[DVA].{5}GG[KR]W']\n",
      "PS00814 ['C.{2}[STAQ].[STAMV]C[STA]TC[HR]']\n",
      "PS00815 ['LR[DE]G.Q.{4}[ACDEFGHIKMNPQRSTVWY].{5}K']\n",
      "PS00816 ['[LIVMFW].{2}H.H[DN]D.G.[GAS].[GASLI]']\n",
      "PS00817 ['P.{4}CD.R[LIVM]{2}.[KR].{14}C']\n",
      "PS00818 ['H[FW].[LIVM].G.{5}[LV]H.{3}[DE]']\n",
      "PS00819 ['[LIVMFY][DH].[LIVM][GA]ER.{3}[LIF][GDN].{2}[PA]']\n",
      "PS00820 ['[STN][GP].{1,2}[DE].WEE.{2}[GS]']\n",
      "PS00821 ['HN.{2}NE.{2}W[NQKRS].{4}WE']\n",
      "PS00822 ['PFDRHDW']\n",
      "PS00823 ['[KR][LIM]K[DE]K[LIM]PG']\n",
      "PS00824 ['[DEG]{2}[DEK][DE][LIVMF]DLFG']\n",
      "PS00825 ['[IV]QS.D[LIVM].A[FWM][DNQS]K[LIVM]']\n",
      "PS00826 ['GQENGHV[KR]']\n",
      "PS00827 ['ETPK{5}.{0,1}FSFKK.FKLSG.SFK[KR][NS][KR]KE']\n",
      "PS00828 ['[CHDS].{2}[CND].{2}[LIVM].R.{3}[LIVMNR].[LIVM].[CN].{3,4}[KRSN][HLFR].[QCAV].Q']\n",
      "PS00829 ['[EKH][LHVI].{9,10}[IVNLR].{3}[LIV].{6}GD.{2}EN[GSA].Y']\n",
      "PS00830 ['S.{2}S[PK][LIVMF][AG].[SAGNE][LIVM][LIVY].{4}[DNG][DE]']\n",
      "PS00831 ['G.[LIVM]{2}.RQRG.{5}G']\n",
      "PS00832 ['GS.[AG][KRN].T.L[KRN].{3}[DE].[DET][LM][VI].F']\n",
      "PS00833 ['RP[VI]ILDP.[DE]PT']\n",
      "PS00834 ['WTD.S.HP.T']\n",
      "PS00835 ['AGYQE[ST]R[FYW]S[FYW][TN]A.GG[ST]Y']\n",
      "PS00836 ['[GA][LIVM][PKV].{0,1}E.{3}[NG]E.{1,3}R[VT][AG].[ST]P.[GSTVN][VA].{2}[LI].[KRHNGSED].G']\n",
      "PS00837 ['[LIVM][LIVMF]G[GAV]G.[AV][GA].{2}[SA].{3}[GA].[SGR][LIVM][GN]A.V.{3}[DE]']\n",
      "PS00838 ['[LI].E[LIVM]{2}.{4,5}[LIVM][TL].{5,7}C.{4}[IVA].[DNS][LIVMA]']\n",
      "PS00839 ['[LIVM][GS][STAL]GPG.{3}[LIVMFY][LIVM]T[LIVM][KRHQG][AG]']\n",
      "PS00840 ['[VW].{2}[LI].{2}[GA][DT].{3}[FYW][GS].{8}[LIVFA].{5,6}[LIVMFYWPAC].[LIVMY].[PN]G']\n",
      "PS00841 ['[VILT][KREIT][PV].[FYIL][VI][FW]DG.{2}[PILHSTF].[LVCQMFAKS]K']\n",
      "PS00842 ['[GSN][LIVM][PERD][FYSCV][LIVM].AP.EA[DE][PAS][QSE][CLM]']\n",
      "PS00843 ['H[GN].{2}[GC]E[DNT]G.[LIVMAFT][QSAPH][GSA]']\n",
      "PS00844 ['[LIVAMSFT].{3}[GAHDVSI].[GSAIVCT]R[LIVMCAFST][DE][LIVMFAYGT][LIVMFAR].{7,12}[LIVWCAF].[EK][LIVAPMT]N[STPA].P[GA]']\n",
      "PS00845 ['[GD].{0,10}[FYWA].{0,1}G[LIVM].{0,2}[LIVMFYD].{0,7}G[KN][NHW].{0,1}G[STARCV].{0,2}[GD].{0,2}[LY][FC]']\n",
      "PS00846 ['C.{2}D[LIVM].{6}[ST].{4}S[HYR][HQ]']\n",
      "PS00847 ['G[IVT][LVAC]{2}[IVT]D[DE][FL][DNST]']\n",
      "PS00848 ['[LIVM]{2}[FYW].{10}C.{2}CG.{2}[FY]KL']\n",
      "PS00850 ['[STIV].R[IVT][CSA]GY[ACDEFHKLMNPQRSTVWY][GACV]']\n",
      "PS00853 ['[YV].D.{3}MS[GA]KKD.[LIVMF][LIVMAG].[LIVM]GG']\n",
      "PS00854 ['[LIVMACFT][GSA][LIVMF].[FYLVGAC].{2}[GSACFYI][LIVMSTACF][LIVMSTAC][LIVMFSTAC][GACI][GSTACV][DES].{15,16}[RK].{12,13}G.{2}[GSTA]D']\n",
      "PS00855 ['[LIVM].[GASF][GA][GAST][LIVMTF][GAS]N[LVMFGIA][LIVFYGT]D[RIK][LIVMFAC]']\n",
      "PS00856 ['[TS][ST]R.{2}[KR].{2}[DE].{2}[GA].{2}Y.[FY][LIVMKHRT]']\n",
      "PS00857 ['[FY].[LIVM].{2}[LIVM].{5}[DN].{5}TRF[LIVMW].[LIVM]']\n",
      "PS00858 ['[LIVM][ST][KR][LIVMF]E[ST]RP']\n",
      "PS00859 ['[DENGQST][LIVMPF][LIVM].{1,2}[KRNQELD][DENKGS][LIVM].{3}[STG].C[EP]HH']\n",
      "PS00860 ['[SA].[RK].Q[LIVMT]QE[RNAK][LIM][TSNV]']\n",
      "PS00861 ['GWTLNSAGYLLGP']\n",
      "PS00862 ['P.{7}[ACDEFGHIKMNPQRSTVWY].{2}[DE][LIVM].{3}[LIVM].{9,12}[LIVM].{3}[GSA][GSTCHRQ]GH']\n",
      "PS00863 ['[EQ][DE]GL[DN]FP.YDG.DRV']\n",
      "PS00864 ['[DE]LEDW[LIVM]EDVL.G.[LIVM]NTEDDD']\n",
      "PS00865 ['P[LIVMG]CT[LIVM][KRHA].[FTNM]P']\n",
      "PS00866 ['[FYV][PS][LIVMC][LIVMA][LIVM][KR][PSA][STA].{3}[SG]G.[AG]']\n",
      "PS00867 ['[LIVMF][LIMN]E[LIVMCA]N[PATLIVM][KR][LIVMSTAC]']\n",
      "PS00868 ['[DQ][LIVMFY].{3}[STAGCN][STAGCIL]TK[FYWQI][LIVMF].G[HQD][SGNH]']\n",
      "PS00869 ['[LIVM]EG[GA].{2}[LIVMF].{6}L.{3}Y.{2}G[LIVM]R']\n",
      "PS00870 ['[DA][AI][SGA][NQS][LIVMF]{2}K[PT].[LM].{2}G']\n",
      "PS00871 ['[RGT][LIVMFY][DN].[ST]E[LIVMFY].[ED][KRQEAS].[STA].[STAD][KRS][LIVM].G[STAP]']\n",
      "PS00872 ['[DG].{3}G.{3}[DN].{6,8}[GA][KRHQ][FSAR][KRL][PT][FYW][LIVMWQ][LIV].{0,1}[GAFV][GSTA]']\n",
      "PS00873 ['GG.[GA]{2}[LIVM]FWMW[LIVM].[STAV][LIVMFA]{2}G']\n",
      "PS00874 ['[KRQ][LIVMAW].{2}[SAIV][LIVM].[TY]P.{2}[LIVM].{3}[STAGV].{6}[LMY].{3}[LIVMF]{2}P']\n",
      "PS00875 ['[GRH][DEQKG][STVM][LIVMA]{3}[GA]G[LIVMFY].{11}[LIVM]P[LIVMFYWGS][LIVMF][GSAE].[LIVMS]P[LIVMFYW][LIVMFYWS].{2,3}[LV][FK]']\n",
      "PS00876 ['GGS[AN][GA]QSS.{2}Q']\n",
      "PS00877 ['[FY]L[DQ][DE][LIVM].{2}YM.{3}H[KR]']\n",
      "PS00878 ['[FY][PA].K[SACV][NHCLFW].{4}[LIVMF][LIVMTA].{2}[LIVMA].{3}[GTE]']\n",
      "PS00879 ['[GSA].{2,6}[LIVMSCP].[ACDEFGHIKLMPQRSTVWY][LIVMF][DNS][LIVMCA]GGG[LIVMFY][GSTPCEQ]']\n",
      "PS00880 ['[PTLV][GSTA].[DENQA].[LMFK].{2}[LIVMFY][YV][GSA].[FYH]KQ[GSAV][STL].G']\n",
      "PS00882 ['R[LIVMFYW].HW[LIVM].{2}[LIVMF][STAC][LIVM].{2}L.[LIVM]TG']\n",
      "PS00883 ['[RH][STA][LIVMFYW]H[RH][LIVM].{2}W.[LIVMF].{2}F.{3}H']\n",
      "PS00884 ['[KQ].[TA].{2}[GA]SSEEK']\n",
      "PS00885 ['[KR].[KH]E[CSTVI][DNE]R[LIVMY].[GSTAVLD][LIVMCTF].{3}[LIVMFA].{2}[LIVMFCGANY]G']\n",
      "PS00886 ['CDK.{2}P[GA].{3}[GA]']\n",
      "PS00887 ['[SGALC][LIMF][LIVMF]TD[GA]R[LIVMFY]S[GA][GAV][ST]']\n",
      "PS00888 ['[LIVM][VIC].[ACDEFGIKLMNPQRSTVWY]G[DENQTA].[GAC][ACDEFGHIKMNPQRSTVWY].[LIVMFY]{4}.{2}G']\n",
      "PS00889 ['[LIVMF]GE.[GAS][LIVM].{5,11}R[STAQ]A.[LIVMA].[STACV]']\n",
      "PS00892 ['[NQAR].{4}[GSAVY].[QFLPA].[LIVMY].[HWYRQ][LIVMFYST]H[LIVMFT]H[LIVMF][LIVMFPT][PSGAWN]']\n",
      "PS00893 ['G.{5}E.{4}[TAGCV][LIVMACF].R[EL][LIVMFGSTA].[EA]E.[GNDTHR]']\n",
      "PS00894 ['R[ACDEFHIKLMNPQRSTVWY].{2}[LIVM].{3}[LIVM].{16,17}[STA].{2}T[LIVMA][RH][KRNAQ]D[LIVMF]']\n",
      "PS00895 ['[LIVMFY]{2}GLG.[MQ]G.{2}[MA][SAV].[SNHR]']\n",
      "PS00896 ['G[LIVM]{2}.D[RK]LGL[RK]{2}.[LIVM]{2}W']\n",
      "PS00897 ['P.[LIVMF]{2}NR[LIVM]G.KN[STA][LIVM]{3}']\n",
      "PS00898 ['[DE][SQLM][KAH][NT][QEK][SQ]C[SRKH].[EQKM][STM]L']\n",
      "PS00899 ['F.[PL]P[STA][FYT]C[DEQ][GAMI][LVMA].[TLF][DE][DEK]']\n",
      "PS00900 ['P[LIVM].{2}D[GA][ST][AC][SN][GA][LIVMFY]Q']\n",
      "PS00901 ['K.E.{3,4}[PAF][STAGC].S[IVAPM]K.R.[STAG].{2}[LIVM]']\n",
      "PS00902 ['[GSTNAD].{2}[GAS].G[GC][IM].[STAG]K[LIVMCT].[SAI][TCAGFS].{2}[GALVCMI]']\n",
      "PS00903 ['[CH][AGV]E.{2}[LIVMFGAT][LIVM].{17,33}PC.{2,8}C.{3}[LIVM]']\n",
      "PS00905 ['D[LIVMA]PG[LIVM]{2}[DEYPKQV][GN]A.{2}G.G']\n",
      "PS00906 ['[SP][IVCLAM]W[LIVMFYC][LM]R[QR][AVS]GR']\n",
      "PS00907 ['[LIMF][GAVS]F[STAGCV][STAGC].[PA][FWYV]T[LIVM].{2}Y.{2,3}[ADE][GK]']\n",
      "PS00908 ['[AT].[SAGCN][SAGC][LIVM][DEQ].A[LA].[DE][LIA].[GA][KRQ].{4}[PSA][LIV].{2}L[LIVMF]G']\n",
      "PS00909 ['[LIVF].{2}D.[NH].{7}[ACL].{6}[LIVMF].{7}[LIVM]E[DENQ]P']\n",
      "PS00910 ['G.{2}[LIVM]{2}.{2}[LIVM].{4}[LIVM].{5}[LIVM]{2}.R[FYW]{2}GG.{2}[LIVM]G']\n",
      "PS00911 ['[GSA].{4}[GK][GSTA][LIVFSTA][GST].{3}[NQRK].G[NHY].{2}P[RTV]']\n",
      "PS00912 ['[LIVM]{2}[GSA].GG[IV].[STGDN].{3}[ACV].{2}[CDEFGHIKLMNPQRSTVWY][ACDEFGHIKLMNPQSTVWY].[ACDEFGHIKMNPQRSTVWY]GA']\n",
      "PS00913 ['[STALIV][LIVF].[DE].{6,7}P.{4}[ALIV].[GST].{2}D[TAIVM][LIVMF].{4}E']\n",
      "PS00914 ['[RQLKA].{3}[LIVMAW].{2}[LIVM][ESHLKAGQV].{2}[LIVMT].[DEVMNRAST][LIVMT].{2}[LIVMQ][FSAQGVM].{2}[LIVMF].{3}[LIVT].{2}Q[GADEQVST].{2}[LIVMA][DNQTEI].[LIVMF][DESVHAG].{2,3}[LIVM]']\n",
      "PS00915 ['[LIVMFAC]K.{1,3}[DEA][DE][LIVMCP]RQ[DE].{4}Q']\n",
      "PS00916 ['[GSET].[AVE].{3}[LIVM].{2}[FYHW][LIVM]{2}.[LIVMFN].DR[HNG].{2}N']\n",
      "PS00917 ['[GA].[LIVM].{2}HGTDT[LIVM]']\n",
      "PS00918 ['GDFNA.C[SAK]']\n",
      "PS00919 ['[LIVM]{2}[AP][LQ]H[STA][STAE]P.{5}E[LIVM][DN].L.[DE]V']\n",
      "PS00920 ['G.{2}[LIVMFY]{2}.[IF].E.{2}[LIVM].GYP']\n",
      "PS00921 ['G[GAQ].{2}C[WA]E[NH].{2}[PST][LIVMFYS].[KR]']\n",
      "PS00922 ['[LIVM].{3}ES.{3}[AP].{3}S.{5}G[LIVM][LIVMFYW].[LIVMFYW].{4}[SAG]']\n",
      "PS00923 ['[IVA][LIVM].C.{0,1}N[ST][MSA][STH][LIVFYSTANK]']\n",
      "PS00924 ['[LIVM]{2}.[AG]CT[DEH][LIVMFY][PNGRS].[LIVM]']\n",
      "PS00925 ['[EQT]G.VYCD[TNP]CR']\n",
      "PS00926 ['W.WH.CH.H[YN]HS[MI][DE]']\n",
      "PS00927 ['PGGRF.E.Y.WD.Y']\n",
      "PS00928 ['QWD.P.[GAV]W[PAS]P']\n",
      "PS00929 ['SQ[IV][STGNH]DGQ[LIV]Q[AIV][STA]']\n",
      "PS00930 ['D[GS]VPF[ST]CCNP.SPRPC']\n",
      "PS00931 ['C.{3}D.[IV]C.G[GST].{2}[LIVM].{2,3}H']\n",
      "PS00932 ['A.{3}[GDTN][IF].[DNQTKEH].[DEAQ].[LIVM].[LIVMC].[NS].{2}[GS].{4,5}[AV].[LIVMEF][STY]']\n",
      "PS00933 ['[MFYGS].[PST].{2}K[LIVMFYW][ACDEFHIKLMNPQRSTVWY]W[LIVMF][ACDFGHIKLMNPQRSTVWY][DENQTKR][ENQH]']\n",
      "PS00934 ['[HQ][IVT].[LIVFY].[IV].{4}[ACDFGHIKLMNPQRSTVWY][STA].{2}F[YM].{2,3}[LMF]G[LMF]']\n",
      "PS00935 ['G[NTKQ].{0,5}[GA][LVFY][GH]H[IVF][CGA].[STAGLE].{2}[DNC]']\n",
      "PS00936 ['K[STNV][ACDEGHIKLMNPQRSTVWY].[GSAM][SAILV].[KRA]R[IVFY].{14,16}[GSANQKR]H']\n",
      "PS00937 ['K.{3}[KRCV].[LIVM]W[IVN][STNALVQCMI][RH][LIVM][NS].{3}[RKHSG]']\n",
      "PS00938 ['[KR][LIVM]{2}[DN][FY][GSTN][KR][LIVMFYS].[FY][DEQTAHI].{2}[KRQ]']\n",
      "PS00939 ['[NP].{3}[KRM].{2}A[LIVTK].[SA][AC][LIV].[ALCM][STL][SGAKTI].{7}[RK][GS]H']\n",
      "PS00940 ['[KRGAQS].C.{3}[SVA].{2}[FYWH].{1,2}[GF].C.{5}C.{3}C']\n",
      "PS00941 ['[EDA][DG]CL[YTF][LIVT][DNS][LIV][LIVFYW].[PQR]']\n",
      "PS00942 ['[QEK][RF]G.{3}[GSA][LIVF][WL][NS].[SA][HM]N[LIV][GA]G']\n",
      "PS00943 ['N.{3}[DEH].{2}[LIMFYT]D.{2}[VM].R[ST].{2}R.{4}[GYNKR]']\n",
      "PS00944 ['YS.[KR]Y.[DE]{2}.[FY]EYRHV.[LV][PT][KRP]']\n",
      "PS00945 ['H.PE.H[IV]LLF[KR]']\n",
      "PS00946 ['Y.[ED].V.[RQ]A[LIVMA][DQG].[LIVMFY]N[EQ]']\n",
      "PS00947 ['F.[LIVM]KET.C.{10}C.F[KR][KE]']\n",
      "PS00948 ['[KRAI]L.RELEKKF[SAPQG].[KRN][HED]']\n",
      "PS00949 ['[LMFYA]R.{3}F.{2}[KRQ].{2}W.[LIVM].{6,9}E.D.[FY]D']\n",
      "PS00950 ['RY.[DT]W.[LIVMF][ST][TV]P[LIVM][LIVMNQ][LIVM]']\n",
      "PS00951 ['G[LIV]S.[KR].[QH].L[FY].[LIV]{2}[FYW].{2}RY']\n",
      "PS00952 ['LE[SA]VAI[LM]PQ[LI]']\n",
      "PS00953 ['D[LIVM].{3}[NQ][PGE].{9,15}[GR].{4}[LIVMFY]{2}K.[ST]E[GS].{2}[FYL].[DN]']\n",
      "PS00954 ['[LIVMY][DE].HH.{2}E.{2}[GCA][LIVM][STAVCL][LIVMF]']\n",
      "PS00955 ['[GW].[DNIE].HH.{2}E[STAGC].[VMFYHS]K']\n",
      "PS00956 ['[GN][DNQPSA].C[GSTANK][GSTADNQ][STNQI][PTIV].CC[DENQKPST]']\n",
      "PS00957 ['[GSA][ATIVS][LIVMYCAFST]K[DN][LIVMA][LIVMFYT][GA].[GACKMSIFT].G[ALIVMF].{2}[SGAQ][LIVMYERAKQFS].{0,1}[TLIVMFYWAQ][ETGAS].{0,1}[NDVS]']\n",
      "PS00958 ['[LIVMA].[LIVM]K[LIVM][PAS].[STC].[DENQPAS][GC][LIVM].[AGV].{0,1}[QEKRSTH].[LIVMF]']\n",
      "PS00959 ['PF.[RA]L[VA][KRQ][DEG][IV]']\n",
      "PS00960 ['Y.{2}[HP]W[FYH][APS][DE].P.KG.[GA][FY]RC[IV][RH][IV]']\n",
      "PS00961 ['E[ST][EA]REA[RK].[LI]']\n",
      "PS00962 ['[LIVMFA].[ACDEFHIKLMNQSTWY][LIVMFYC]{2}[ADEFGHIKMNQRSTVWY][STAC][GSTANQEKR][STALV][HY][LIVMF]G']\n",
      "PS00963 ['P.{2}[LIVMF]{2}[LIVMS].[GDN].{3}[DENL].{3}[LIVM].E.{4}[GNQKRH][LIVM][AP]']\n",
      "PS00964 ['[FY]R[IM][KR]K{2}DEGSY']\n",
      "PS00965 ['Y.D.NHKPE']\n",
      "PS00966 ['HAY[LIVM].G.{2}[LIVM]E.MA.SDN.[LIVM]RAG.TPK']\n",
      "PS00967 ['F[LIVMF]FRPRN']\n",
      "PS00968 ['[LIVFAG].[GASV][LIVFA].[IV]H.{3}[LIVM][GSTAE][STANH].{1,3}[STN]W[LIVMFYW]']\n",
      "PS00969 ['[EQ].{4}[HGQ].{5}[GSTA].{3}[FYV].{3}[AG].{2}[AV]H.{7}P']\n",
      "PS00970 ['H.{3}HS[NS]S.PQ[SG]']\n",
      "PS00971 ['K.RK.{2}EGK.{2}K[KR]K']\n",
      "PS00972 ['G[LIVMFY].{1,3}[AGCY][NASMQG].C[FYWC][LIVMFCA][NSTAD][SACV].[LIVMSF][QF]']\n",
      "PS00973 ['[YMF].[LK].[SAGNC][LIVMFT].{2}H.G.{4,6}GH[YF]']\n",
      "PS00974 ['[LIVMY].[FS].{2}[STAGCV].VDR[IV].[PS]']\n",
      "PS00975 ['[DEK][IV]N[FS]LC.HK']\n",
      "PS00976 ['KFG.GDG']\n",
      "PS00977 ['[IV]GGG.{2}G[STACV]G.[AT].[DQ].{3}[RAS]G']\n",
      "PS00978 ['GGK.{2}[GSTE]YR.{2}A']\n",
      "PS00979 ['[LV].N[LIVM]{2}.LF.I[PA]Q[LIVM][STA].[STA]{3}[STAN]']\n",
      "PS00980 ['CC[FYW].C.{2}C.{4}[FYW].{2,5}[DNE].{2}[STAHENRI]C.{2}C']\n",
      "PS00981 ['[FLY]N[ED][STA]K.[IV][STAG][FM][ST][MVL]']\n",
      "PS00982 ['[NG].[FYWV][LIVMF].G[AGC][GS][TA][HQT]PG[STAV]G[LIVM].{5}[GS]']\n",
      "PS00983 ['[EQR]C[LIVMFYAH].C.{5,8}C.{3,8}[EDNQSTV]C[ADEFGHIKLMNPQRSTVWY].{5}C.{12,24}C']\n",
      "PS00984 ['CFWKYC']\n",
      "PS00985 ['CG.{2}[LIY].{4}G.I.{9}C.WT']\n",
      "PS00986 ['C.KE.[LIVM]E[LIVM].[DE].{3}[GSE].{5}K.C']\n",
      "PS00987 ['CNN.{2}GHGHNY']\n",
      "PS00988 ['DHKNLD.D']\n",
      "PS00989 ['[LIVMC][LIVM]Y[KR].{4}LYF']\n",
      "PS00990 ['[IVT][GSP]WR.{2,3}[GAD].{2}[HY].{2}N.[LIVMAFY]{3}D[LIVM][LIVMT]E']\n",
      "PS00991 ['[LIV].[FL][IQ]PP.G.[LIVMFY].[LV].{2}Y']\n",
      "PS00992 ['ARGNY[ED]A.[QKR]RG.GG.WA']\n",
      "PS00993 ['[DE][LM]G[STALPD].{2}[GK][KR].{6}[LIVM].[LIVM].[DEN].[GI]']\n",
      "PS00994 ['R[LIVM][GSAT]EV[GSAR]ARF[STAIV]LD[GSA][LM]PGKQM[GSA]ID[GSA][DAE]']\n",
      "PS00995 ['Q[DEK]..[LIVMGTA][GA]DGT']\n",
      "PS00996 ['[LV]Y[IVC]PRKCS[SAT]']\n",
      "PS00997 ['LCC.[KR]C.{4}[DE].N.{4}C.CRVP']\n",
      "PS00998 ['C.HCGC[KRH]GC[SA]']\n",
      "PS00999 ['C.P.{2,3}G.{0,1}H[PA].{4}AC[ATDE].L']\n",
      "PS01000 ['RP[LIVMT].{3}[LIVM].{6}[LIVMWPK].{4}S.{2}HR.[ST]']\n",
      "PS01001 ['H.{3}[GA][LIVMT]R[HF][LIVMF].[FYWM]D.[GVA]']\n",
      "PS01002 ['[IFAED][GA][GASF]N[PAK]S[GTA]E[GDEVCF][PAGEQV][DEQGAV]']\n",
      "PS01003 ['[FLIV].{4}[FLVH][FY][MIVCT]GE.{4,7}[DENP][GAST].[LIVM][GAVI].{3}[FYWQ]']\n",
      "PS01004 ['C.[ST].[DE].{3}[ST][FY].L[FY]I.{4}GA']\n",
      "PS01005 ['[LIVMA][LIVMY].G[GSTA][DES]L[FI][TN][GS]']\n",
      "PS01006 ['[GA].{2}[CA]N[LIVMFYW]{2}VC[LV]A']\n",
      "PS01007 ['D.{3}G[LIVMF].{6}[STAV][LIVMFYW][PT].[STAV].{2}[QR].C.{2}H']\n",
      "PS01008 ['[IL][GA].{2}[LIVMF][SGADENK].{0,1}[KR].H[STPA][STAV][LIVM].{2}[SGAMN].{3}[LIVM]']\n",
      "PS01009 ['[GDER][HR][FYWH][TVS][QA][LIVM][LIVMA]W.{2}[STN]']\n",
      "PS01010 ['[LIVMFYH][LIVMFY].C[NQRHS]Y.[PARH].[GL]N[LIVMFYWDN]']\n",
      "PS01011 ['[LIVMFY].[LIVM][STAG]GT[NK]GK.[STG].{4}[CDEFGHIKLMNPQRSTVWY].[CFGHIKLMNPQRSTVWY][LIVM]{2}.{3,4}[GSKQT]']\n",
      "PS01012 ['[LIVMFY]{2}[EK].G[LIVM][GA]G.{2}D.[GST].[LIVM]{2}']\n",
      "PS01013 ['E[KQ].[SC]H[HR][PG][PL].{1,2}[STACFI][ACGY]']\n",
      "PS01014 ['[LIVM]FG[KRW].TP[IV].[LIVM]']\n",
      "PS01015 ['[LIVMF].[KRGTIEQSN].[GSAIYN][KRQDAVLSIH][VGAIT][RSNAK].{0,1}[KRAQ][SAKG][KYR][KLI][LYSFT][YF][LIM][RK]']\n",
      "PS01016 ['[KRC][GSAT].{4}[FYWLMH][DQNGKRH].P.[LIVMFY].{3}H.{2}[GSA]H[LIVMFA]']\n",
      "PS01017 ['G.{2}[LIVMF][IFYH][DN].[FYWM].G.{2}[LF][NY]P[RQ]']\n",
      "PS01018 ['[LIVM]{2}[LIVMFT][HWD]R.{2}RD.{3}C.{2}KY[GK].{2}[FW].{2}Y']\n",
      "PS01021 ['K.[WQA][CA].{2}[FYH]{2}.[LIVM].[HY]R.E.RG[LIVMT]GG[LIVM]F[FY]D']\n",
      "PS01022 ['[GA][GAS][LIVMFYWA][LIVM][GAS]D.[LIVMFYWT][LIVMFYW]G.{3}[TAV][IV].{3}[GSTAV].[LIVMF].{3}[GA]']\n",
      "PS01023 ['[FYT].{2}[LMFY][FYV][LIVMFYWA].[IVG]N[LIVMAG]G[GSA][LIMF]']\n",
      "PS01024 ['EFDYLKSLEIEEKIN']\n",
      "PS01025 ['[NH][AG]H[TAD]YHINSIS[LIVMN][NS]SD']\n",
      "PS01026 ['[GTND][FPMI].[LIVMH].[DEAT].{2}[GA].[GTAM][STA].GH.[LIVM][GAS]']\n",
      "PS01027 ['W.FE.WNEP[DN]']\n",
      "PS01028 ['D[LIVM][DE][LIVMN].{18,20}[LIVM]{2}.[SC][NHY]H[DN]']\n",
      "PS01029 ['[LIVM][NQHS]GPN[LVI].{2}[LT]G.R[QED].{3}[FY]G']\n",
      "PS01030 ['[FY]C[ACDEFGHIKLNPQRSTVWY][DEKSTG]C[GNK][DNSA][LIFVMHG][LIVM].{8,14}C.{1,2}C']\n",
      "PS01032 ['[LIVMFY][LIVMFYA][GSAC][LIVM][FYC]DGH[GAV]']\n",
      "PS01034 ['E[LIV]D[LIVF].{0,1}E.{2}[GQ][KRNF].[PSTA]']\n",
      "PS01035 ['N[LIVMFY].{5}C.TR[LIVMF].[LIVMF].[LIVM].[DQEN]']\n",
      "PS01036 ['[LIVMY].[LIVMF].GG.[ST][ACDEFGHIKMNPQRTVWY][LIVM]P.[LIVM].[DEQKRSTA]']\n",
      "PS01037 ['[GAP][LIVMFA][STAVDN].[ACDEFGIKLMNPQRSTVWY].{2}[GSAV][LIVMFY]{2}Y[ND].{3}[LIVMF].[KNDE]']\n",
      "PS01039 ['G[FYIL][DE][LIVMT][DE][LIVMF][ACDEFGHIKLMNQRTVWY][ACDEFHIKLMNPQRSTVW].[LIVMA][VAGC][ACDEFHIKLMNQSVWY][ACDEFHIKMNPQRSTVWY][LIVMAGN]']\n",
      "PS01040 ['[AG].{6,7}[DNEG].{2}[STAIVE][LIVMFYWA].[LIVMFY].[LIVM][KR][KRHDE][GDN][LIVMA][KNGSP][FW]']\n",
      "PS01041 ['AE[KR]REHE[KR]EV']\n",
      "PS01042 ['A.{3}G[LIVMFY][STAG].{2,3}[DNS]P.{2}D[LIVM].G.D.{3}K']\n",
      "PS01043 ['RG.{2}EN.NG[LIVM]{2}R[QE][LIVMFY]{2}PK']\n",
      "PS01044 ['Y[CSAM].{2}[VSG]A[GSA][LIVAT][IV]G.{2}[LMSC].{2}[LIV]']\n",
      "PS01045 ['[LIVM]G.{3}Q.{2,3}[ND][IFL].[RE]D[LIVMFY].{2}[DE].{4,7}R.[FY].P']\n",
      "PS01046 ['DG[PD]SA[GS][LIVMCA][TA][LIVM]']\n",
      "PS01047 ['[LIVNS].[ACDEFGHIKMNPQRSTVWY][LIVMFA].C.[STAGCDNH]C.{3}[LIVFG][ACDEFGHIKMNPQRSTWY].{2}[LIV].{9,11}[IVA].[LVFYS]']\n",
      "PS01048 ['G.[KRC][DENQRH]L[SA]Y.I[KRNSA]']\n",
      "PS01049 ['[SAV][IVW][LVA][LIV]G[PNS]GL[GP].[DENQT]']\n",
      "PS01050 ['[GA]G.GD[TV][LT][STA]G.[LIVM]']\n",
      "PS01052 ['[LIVM].[LS]Q[MASY]G[STY][NT][KRQ].{2}[STN]Q.G.{3,4}G']\n",
      "PS01053 ['[ST][LIVMFY]D[LIVM]D.{3}[PAQ].{3}P[GSA].{7}G']\n",
      "PS01054 ['[DGH][IVSAC]T[ST]NP[STA][LIVMF]{2}']\n",
      "PS01055 ['K[LIVMF]DG[LIVMAS][SAG].{4}Y.{2}[GRD].[LF].{4}[ST]RG[DN]G.{2}G[DE][DENL]']\n",
      "PS01056 ['[IV]G[KR][ST]G.[LIVM][STNK].[VTLYF].{2}[LVMF].[PS][IV]']\n",
      "PS01057 ['[LIVMRPA][LIVFY][PLNRKG][LIVMF]E.[IV][LVCATI]R.{3}[TAEYSI]G[ST]']\n",
      "PS01058 ['[LI][IVCAP]D.K[LIFY]E[FI]G']\n",
      "PS01060 ['[PA][AS][FY].[LIVT][STH][EQ][LI].{2}[GA]F[KREQ][IM][GV][LIF]']\n",
      "PS01061 ['P[LIVMF]K[LIVMF]{5}.[LIVMA][DNGS]GW']\n",
      "PS01062 ['SVAGLGGCPY']\n",
      "PS01063 ['[STAIV][PQDEL][DE][LIV][LIVTA]Q.[STAV][LIVMFYC][LIVMAK].[GSTAIV][LIMFYWQ].{12,14}[STAP][FYW][LIF].{2}[IV]']\n",
      "PS01064 ['[LIVFMYWA]EFW[QHGENRAMVYLCF].{4}[RW][LIVMK][HN][DNESIT]R']\n",
      "PS01065 ['[IVAG].[KR].{2}[DE][GDE]{2}.{1,2}[EQHF].[LIV].{4}P.[LIVM]{2}[TACS]']\n",
      "PS01066 ['[DEH][LIVMF][LIVMFC][LIVMF]R[STPV][SGAC][GEN].{1,2}R.S.[FY][LMFV][LIPMVT][YWL]']\n",
      "PS01067 ['[LIVMFY].[ACEFGHIKLMNPQRSTVWY][DENQGA].[ACDFGHIKLMNPQRSTVWY].{2}[LIVMFTA].[KRV].{2}[KW]P.{3}[SEQ].{5}[ACEFGHIKLMNPQRSTVWY][ADEFHIKLMNPQRSTVWY][LIVT][LIVGA][LIVFGAST]']\n",
      "PS01068 ['[LIVMA].[GT].[TA][DAN].{2,3}[DG][GSTPNKQ].{2}[LFYDEPAVI][NQS].{2}[LI][SG][QEA][KRQENAD]RA.{2}[LVAIT].{3}[LIVMF].{4,5}[LIVMF].{4}[LIVM].{3}[SGW].G']\n",
      "PS01069 ['E.[LIVM]N[ST][SA][LIV]E.{2}VD']\n",
      "PS01070 ['DRGH[QLIM].{3}[AG]']\n",
      "PS01071 ['[FYLV][DNST][PHEAYVS].{2}[HMACNQ].[ALV][LIVMTNSF].{16,21}[GYP][FY].{3,4}[DENGKS].{2,3}[LIV][KRIV].[STAG].V.{0,1}[IV]']\n",
      "PS01073 ['[FY].[GSHE].{2}[IVLF].P[GA].G.{2}[FYV].[KRHE].D']\n",
      "PS01074 ['[DE]GSW.[GE].W[GA][LIVM].[FY].Y[GA]']\n",
      "PS01075 ['[LIVMFANT][LIVM].[LIVMA]N.GS[ST]{2}.[KE]']\n",
      "PS01076 ['[LIVMFATQ][LIVMA].{2}H.G.[GT].[ST][LIVMA].[TAVC].{3}G']\n",
      "PS01077 ['GT.[SAVTP].G.[KRHIM].{4,5}H.{2}C.RCG']\n",
      "PS01078 ['[LIVMCA][LIVM]{2}[LITF][LITN]GGTG.{4}D']\n",
      "PS01079 ['S.[GS].{2}D.{5}[LIVW].{10,12}[LIV].{2}[KR]PG[KRL]P.{2}[LIVMF][GA]']\n",
      "PS01080 ['[LVMENQ][FTLS].[GSDECQ][GLPCKH].{1,2}[NST][YW]G[RK][LIV][LIVC][GAT][LIVMF]{2}.F[GSAEC][GSARY]']\n",
      "PS01081 ['[GS][LIVMFYSP].{2,3}[TS][LIVMTA].{2}[LIVM].{5}[LIVQSA][STAGENQH].[GPART].[LIVMFA][FYSTNRH].[HFYRA][FVW].[DNSTKAG][KQMT].{2,3}[LIVM]']\n",
      "PS01082 ['[CAS].{4}[IVA]P[FY].{2}[LIVM].[GSQNK][KRQ].{2}LG']\n",
      "PS01083 ['F.EE.[LIVM]{2}RREL.{2}NF']\n",
      "PS01084 ['G.HD.{2}W.ER.[LIVM]FGK[LIVM]R[FY]MN']\n",
      "PS01085 ['[LIVMF]H[LIVMFY]D[LIVM].D.{1,2}[FY][LIVM].N.[STAV]']\n",
      "PS01086 ['[LIVMA].[LIVM]M[ST][VS].P.{3}[GN]Q.{0,1}[FMK].{6}[NKR][LIVMC]']\n",
      "PS01087 ['[GVPS].[GKS].[KRS].{3}[FL].{2}G.{0,1}C.{3}C.{2}C.[NLF]']\n",
      "PS01088 ['[LIVM]{2}.RL[DE].{4}RLE']\n",
      "PS01089 ['D[LIVMFY].E.[PA].PEQ[LIVMFY]K']\n",
      "PS01090 ['P[LIVM].[LIVM]H.R.[TA].[DE]']\n",
      "PS01091 ['[LVSAT][LIVA].{2}[LIVMT][PSD].{3}[LI][LIVMT][LIVMST]ETD.P']\n",
      "PS01092 ['EYFG[SA]{2}LW.LYK']\n",
      "PS01093 ['YRN.W[NS]E[LIVM]RTLHF.G']\n",
      "PS01094 ['[PA][ASTPV]R[SACVF].[LIVMFY].{2}[GSAKR].[LMVA].{5,8}[LIVM]E[MI]']\n",
      "PS01095 ['[LIVMFY][DN]G[LIVMF][DN][LIVMF][DN].E']\n",
      "PS01096 ['F[GSADEI].[LVAQ]A.{3}[ST].{3,4}[STQ].{3,5}[GER]G.[LIVM][GS]']\n",
      "PS01097 ['^MC[LIV][GA][LIV]P.[QKR][LIV]']\n",
      "PS01098 ['[LIVMFYAG]{4}GDS[LIVM].{1,2}[TAG]G']\n",
      "PS01099 ['[DKG].{2}[FLV][STKD].{5}C[LMNQ][GA].C.{2}[GA]P']\n",
      "PS01100 ['LIDIGSGPT[IV]YQ[LV]L[SA]AC']\n",
      "PS01101 ['CP.[LIVMYAT].C.{5}[LI]P[LIVMCA]G.{9}V[KRM].{2}C[PA].C']\n",
      "PS01102 ['C[DESN].[CTS].{3}I.{3}[RK].{4}P.{4}[CSLAT].{2}[CAYF]']\n",
      "PS01103 ['[LIVM][SADN].{2}C.R[LIVM].{4}[GSC]H[STA]']\n",
      "PS01104 ['[KR].G[KR]GF[ST][LVF].E[LVI].{3}G']\n",
      "PS01105 ['G[KT][LIVM].[RD].HG.{2}G.V.[AVS].F.{3}[LI]P']\n",
      "PS01106 ['[KREWDI].L.{2}[PSRG][KRS].{2}[RHY][PSA].[LIVM][NSA][LIVM].[RK][LIVM]']\n",
      "PS01107 ['GK[NS].WFF.{2}L[RH]F$']\n",
      "PS01108 ['[GDEN]D.[IV].[IV][LIVMA].G.{2}[KRA][GNQK].{2,3}[GA].[IV]']\n",
      "PS01109 ['[KNQ].{2}[ACDEFGHILMNPQRSTVWY].{3}[CDEFGHIKLMNPQRSTVWY][ACDEFGHIKMNPQRSTVWY].{9}[LIVMFY].{2}[DENHR].{2}[GS][LIVMF][STDNQC][VTA].[DENQKHPSA][LIVMSAD].{2}[LIMF][KR]']\n",
      "PS01110 ['H[NQEIVMYD][LIVMYAS]VP[EKT]H.{2}[LIVM].{2}[DESAG][ET]']\n",
      "PS01111 ['[ST].[FY]E.[AT]R.[LIVM][GSA].R[SA].Q']\n",
      "PS01112 ['[LIVMFD][LIVMFP]P[LIVM].C[FL][ST]CG']\n",
      "PS01114 ['NP[AV]P[LF]GL.[GSA]F']\n",
      "PS01116 ['[LIVM]P.[PASIF]V[LIVMG][GF][GA].{4}[LIVM][FY][GSA].[LIVM].{3}[GA]']\n",
      "PS01117 ['[STNAQ][LIAMV].{0,1}[RNGSYKE].{4,5}[LM][EIVLA].{2}[GESD][LFYWHA][LIVC].{7}[DNS][RKQG][RK].{6}[TS].{2}[GAS]']\n",
      "PS01119 ['M[LIVMFP][LIVMF]{2}.{3}[KN][MY]AC.{2}C[IL][KR].H[KR].{3}C.H.{8,9}[KR].[KRP]GRP']\n",
      "PS01120 ['T[AY][GA][GATR][LIVMF]D.H[LIVM]H.{3}[PA]']\n",
      "PS01121 ['H.{2,4}[SC].{2}[CDEFGHIKLMNPQRSTVWY].[LIVMF]{2}[ST]HG']\n",
      "PS01122 ['KPK[LIVMF][LIVMFY][LIVMF]{2}[QP][AF]C[RQG][GE]']\n",
      "PS01123 ['DGDT[LIVM].[LIVMC].{9,10}R[LIVM].{2}[LIVM]D.PE']\n",
      "PS01125 ['[LIVM].{2}G[LIVMFCT]G.[GA][LIVMFA].{3}[ACDEFGHIKLMNPQRSTWY].{4}G.{3,5}[GATP][ACDEFHIKLMNPQRSTVWY].G[RKH]']\n",
      "PS01126 ['LR.{2}[TS][GSDNQ].[GSA][LIVMF].{0,1}[DENKAC].K[KRNEQS][AV]L']\n",
      "PS01127 ['[ELAS][LIVMF][NVCKGST][SCVA][QE]TD[FS][VLA][SAT][KRNLAQS]']\n",
      "PS01128 ['[KR].{2}E.{3}[LIVMF].{8,12}[LIVMF]{2}[SA].G{3}.[LIVMFG]']\n",
      "PS01129 ['[LIVCA][NHYTQ]R[LI][DG].{2}[TV][STAC]G[LIVAGC][LIVMF]{2}[LIVMFGCA][SGTACV]']\n",
      "PS01130 ['[PAV].[FYLCVI][GS]LY[STAG][STAGL].{4}[LIVFYA][LIVMST][YI].{3}[GA][GST][SRV][KRNP]']\n",
      "PS01131 ['[LIVMAC][LIVMFYWT][DE].G[STAPVLCG]G.[GAS].[LIVMF][ST].{2,3}[LIVMA].{5,8}[LIVMYF].[STAGVLC][LIVMFYHCS]E.D']\n",
      "PS01132 ['[LM][LIVMA]TE[GAPQ].[LIVMFYWHQPK][NS][PSTAQ].{2}N[KR]']\n",
      "PS01133 ['D.{8}[GN][LFY].{4}[DET][LY]Y.{3}[ST].{7}[IV].{2}[PS].[LIVM].[LIVM].{3}[DN]D']\n",
      "PS01134 ['N[ST]D.[QS].L.{16,18}G.G[ATVS]G[GSAN].P.{2}G']\n",
      "PS01135 ['[DNHKR][LIVMF].[LIVMF]{2}[VSTAC][STAC]G.G[GKN]GTG[ST]G[GSARC][STA]P[LIVMFT][LIVMF][SGAV]']\n",
      "PS01136 ['[LIVM][DNG][LIVMF]N.GC[PS].{3,4}[LIVMASQ].{5,6}G[SACY]']\n",
      "PS01137 ['[LIVMFY]{2}D[STA]H.H[LIVMFP][DN]']\n",
      "PS01138 ['C.{3}C.{6,9}[GAS]KC[IMQT].{3}C.C']\n",
      "PS01139 ['D.{0,1}M.K[SAG]{2}.[IV].[LIVM][LIVMA][GCSY].{4}[GDE][SGPDR][GA]']\n",
      "PS01140 ['[STA]TRY[FYW]D.{5}[CA]']\n",
      "PS01141 ['P.{6}F.{4}[LF].{3}D[LIVM]A[LIVM].[LIVM]N.[LIVMQ].[LF]']\n",
      "PS01142 ['GTLW.G.{11}L.{4}W']\n",
      "PS01143 ['[DES][IVT].{4}H[PT][FAVY][FYW][TISN].{9,13}[GN][KRHNQ]']\n",
      "PS01144 ['[VI][KRWVI][LIV][DSAG].{2}[LIV][NS].[AKQEHFYLCT].W.[KRQE][GS]']\n",
      "PS01145 ['[ST].S.[KRQ].{4}[KR][TL]P[GS][GN]']\n",
      "PS01148 ['[LIV][DEN].{2}[TAG].{2}CP.[PT].[LIVMF].{11}[GN]']\n",
      "PS01149 ['GRLD.{2}[STA].G[LIVFA][LIVMF]{3}[ST][DNST]']\n",
      "PS01150 ['[GN].[DE][KRHST][LIVMFA][LIVMF]P[IV]D[LIVMFYWA][LIVMFYWK].P.CP[PT]']\n",
      "PS01151 ['[VL][PASQ][PAS]G[PAD][FY].[LI][DNQSTAP][DNH][LIVMFY]']\n",
      "PS01152 ['[FQ].[LIVMFY].[NH][PGT][NSKQR].{4}C.C[GSN].SF']\n",
      "PS01153 ['[FV][DQ][KRA][LIVMA]L.D[AV]PC[ST][GA]']\n",
      "PS01154 ['[LIVMFA].{2}[EYD].[HYN][ST][LIVMFY].[NSTR].[LIV].{3}[LIVA].{4,5}[VPG].{4}[FY].{3}[HPY][PEV]']\n",
      "PS01155 ['[GSTENA].[LIVMF]P.{5}[LIVMW].{2,3}[LI][PAS]G[IV][GA].{3}[GAC].{2,3}[LIVMA].{1,2}[GSALVI][LIVMFYW][GANKD]']\n",
      "PS01156 ['[LYGSTANEQ].{3}[GSTAENQ].[PGE]R.[LIVFYWA].[LIVMFTA][STAGNQ][LIVMFYGTA].[LIVMFYWGTADQ].F$']\n",
      "PS01157 ['GSYPSGHT']\n",
      "PS01158 ['[DE]P[CLV][APT].{3}[LIVM].S[IS][GT].[LIVM][GST]']\n",
      "PS01159 ['W.{9,11}[VFY][FYW].{6,7}[GSTNE][GSTQCR][FYW][ACDEFGHIKLMNPQSTVWY][CDEFGHIKLMNPQRTVWY]P']\n",
      "PS01160 ['[DEQR]AL.{3}[GEQ].{3}G.[DNS].P.VA.{3}N.L[AS].{5}[QR].[KR][FY].{2}[AV].{4}[HKNQ]']\n",
      "PS01161 ['[LIVM].{3}[GNH].{0,1}[LITCRV].[LIVWF].[LIVMF].[GS][LIVM]G.[DENV]G[HN]']\n",
      "PS01162 ['[GSDN][DEQHKM].{2}L.{3}[SAG]{2}GG.G.{4}Q.{2}[KRS]']\n",
      "PS01163 ['DLPI[VS]GG[ST][LIVM]{2}[STAV]H[DEN]H[FY]Q[GAT]G']\n",
      "PS01164 ['[LIVM][LIVMA][LIVMF].{4}[ST].{2}NY[DE][YN]']\n",
      "PS01165 ['T.[GS].{2}H[LIVMF].{3}E[DE].P']\n",
      "PS01166 ['G.[KN][LIVMFA][STAC][GSTNR].[HSTA][GSAI][QNH]K[GL][IVTEC]']\n",
      "PS01167 ['[IL].[STV][GT].{2}[KR].[KRAF].{6}[DE].[LIMV][LIVMT][TE].[STAG][KR]']\n",
      "PS01168 ['[QKRTE]C.{2}C.{6}F[GSDA].[PSA].{5}C.{2}C[GSAQ].{2}[LIV].{2}[PS].G']\n",
      "PS01169 ['[IVTL].{3}[KR].{3}[KRQ][KT].{6}G[HFY][RK][RQT].{2}[STL]']\n",
      "PS01170 ['N.{2}P[LI]RR.{4}[FY]VIATS.{1,2}K']\n",
      "PS01171 ['[GN][DEQ].V.{10,11}[GVT].{2}[FYHDN].{2}[FY].G.[TV]G']\n",
      "PS01172 ['K.[TV]KK.{2}L[KR].{2}C']\n",
      "PS01173 ['[LIVMF]{2}.[LIVMF]HGG[SAG][FYW].{3}[STDN].{1,2}[STYA][HAGFT]']\n",
      "PS01174 ['[LIVM].[LIVMF][SA]GDS[CAS]G[GA].[LI][CAVT]']\n",
      "PS01175 ['[HI][FYE][GSTAM][LIVM].{4,5}Y[STALV].[FWVAC][TV][SA]P[LIVMA][RQ][KR][FY].D.{3}[HQ]']\n",
      "PS01176 ['[GLES].[LIVM].{2}L[KR][KRHNS].K.{5}[LIVM].{2}[GNKADS].[DEN][CRG][GI]']\n",
      "PS01177 ['[CSH]C.{2}[GAP].{7,8}[GASTDEQR]C[GASTDEQL].{3,9}[GASTDEQN].{2}[CE].{6,7}CC']\n",
      "PS01181 ['[DE].A[LIY][KR][RA][FL]K[KR].{3}[KR]']\n",
      "PS01182 ['GGP[LIVM]{2}.{2}Q.ENE[FY]']\n",
      "PS01183 ['Y[DN].[MLAFTI]N.{2}[LIVMN]S.{3}[HQD].{2}W']\n",
      "PS01184 ['RV[LIVMCT][KRQ][PVRK][GMKD][GAS].[LIVMFAT].{2}[LIVMCA][ED].[SGT]']\n",
      "PS01185 ['CC.{13}C.{2}[GN].{12}C.C.{2,4}C']\n",
      "PS01186 ['C.C.{2}[GP][FYW].{4,8}C']\n",
      "PS01187 ['[DEQN].[DEQN]{2}C.{3,14}C.{3,7}C.[DN].{4}[FY].C']\n",
      "PS01188 ['[LIV].F[LIV]H[VYWT][FY]HH']\n",
      "PS01189 ['[SA][LI][KREQP].{2}[LIVM].{2}[SA].{3}[DNG]G[LIV].{2}G[LIV]']\n",
      "PS01190 ['PYE[KR]R.[LIVMT][DE][LIVM]{2}[KR]']\n",
      "PS01191 ['[LIV].[GHN]R[IVNT].E.[SCT][LV].[DE][LVI]']\n",
      "PS01192 ['A[LIVM].[STAN].{2}[LI].[KRNQ][GSA]H[LM].[FYLH]']\n",
      "PS01193 ['[KR].{2}[ST]G[GAR].{5,6}[KRHSA].[KRT].[KR].[EA][LIMPA]G']\n",
      "PS01194 ['[DENT][KR][AL]R.[LIQ]G[FY].[SAP].{2}G[LIVMFY][LIVMFYKS][LIVMFY][LIVMFYA]R.[RNAS][IVL].[KRC]G']\n",
      "PS01195 ['[FYH].{2}[TN][RK]HN.G.{2}[LIVMFAYCT][LIVMFA][DEN]']\n",
      "PS01196 ['[GS].{3}HNG[LIVM][KR][DNS][LIVMTC]']\n",
      "PS01199 ['[IMGV].{2}[LIVA].{2,3}[LIVMY][GAS].{2}[LMSF][GSNH][PTKR][KRAVG][GN].[LIMF]P[DENSTKQPRAGVI]']\n",
      "PS01200 ['F[KRHQ]GRV[ST].ASVKNFQ']\n",
      "PS01201 ['AF[AG]I[GSAC][LIVM][ST]SF.[GST]K.ACE']\n",
      "PS01202 ['[DA][LIVMY].K[LIVM]D.G.[HQ][LIVMS][DNS]G.{3}[DN]']\n",
      "PS01203 ['[LV]P.[DE][LM][ST][LIVM]W[IV]DP.EV[SC].[RQ].G[EK]']\n",
      "PS01204 ['FRY.CEG']\n",
      "PS01205 ['RPL[IV].[NS]FGS[CA][TS][CU]P.F']\n",
      "PS01206 ['Y.{2}[EQTFPMSI].C.{3}C.[QTAVKS].{2}[LIVMT][LIVMSAQ].{2}C.C']\n",
      "PS01207 ['C.{2}C.[GS][LIVM].{4}PC.{2}[FY]C.{2}[LIVM].{2}GC']\n",
      "PS01208 ['C.{2,3}C[ADEFHIKLMNPQRSTVWY]C.{6,14}C.{3,4}C.{2,10}C.{9,16}CC.{2,4}C']\n",
      "PS01209 ['C[VILMA].{5,6}C[DNH].{3}[DENQHT]C.{3,4}[STADEW][DEH][DE].{1,5}C']\n",
      "PS01210 ['FED[LV]IA[DE][PA]']\n",
      "PS01211 ['[FW]H[FM][IV]G.[LIV]Q.[NKRQ][KN].{3}[LIV]']\n",
      "PS01212 ['GG.[LIVM]G[LIVM].[IV].W.C[DN]LD.{5}C.P.Y.F']\n",
      "PS01213 ['F[LF].{4}[GE]G[PAT].{2}[YW].[GSE][KRQAE].{1,5}[LIVM].{3}H']\n",
      "PS01214 ['E[LIVM]GDKTF[LIVMF]{2}A']\n",
      "PS01215 ['W.{2}[LIVM]D[VFY][LIVM]{3}D.PPGT[GS]D']\n",
      "PS01216 ['S[KR]SG[GT][LIVM][GST].[EQ].{8,10}G.{4}[LIVM][GA][LIVM]GGD']\n",
      "PS01217 ['G.[IVT].{2}[LIVMF].[NAK][GS][GA]G[LMAI][STAV].{4}[DN].[LIVM].{3,4}[GD][GREAK]']\n",
      "PS01218 ['Y.{2}[FL][LIVMAFNT][LIVMAFT].[LVSI].{4}[GASF].{2}F[EQ][LIVMFC]P[LIVM]']\n",
      "PS01219 ['D[FYWS][AS]G[GSC].{2}[IV].{3}[SAG]{2}.{2}[SAG][LIVMF].{3}[LIVMFYWA]{2}.[GK].R']\n",
      "PS01220 ['[FYL].[LVM][LIVF].[TIVM][DC]PD.P[SNG].{10}H']\n",
      "PS01221 ['[LIVMF][LIVMFC][LIVMF]{2}[SA][TL].{2}[DNKS].W.{9,13}[LIV]W.{2}[CG]']\n",
      "PS01222 ['[RQ][AVS].[MC][IV]L[SA].[LI].{4}[GSA][LIVMF][LIVMFS][LIVMF]']\n",
      "PS01223 ['[VA].{5}A[LIVAMTCK].[HWFY][IM].{2}[HYWNRFT][GSNT][STAG].{0,1}H[ST][DE].{1,2}I']\n",
      "PS01224 ['[LIVMA][GSA].[PA]GC[FYN][AVPST]T[GSACVT].{3}[GTACLPS][LIVMCAF].[PL]']\n",
      "PS01226 ['N.[DN][IV]EG[IV]D.{2}NAC[FY].G']\n",
      "PS01227 ['[GTAL].{2}[IVT]CYD[LIVM].FP.{9}[GD]']\n",
      "PS01228 ['[LIVFYAN][LIVMFA].{2}D[LIVMF][ND]GT[LIV][LVY][STANLM]']\n",
      "PS01229 ['[LIVMFC]GD[GSANQ].ND.{3}[LIMFY].{2}[AV].{2}[GSCP].{2}[LMP].{2}[GAS]']\n",
      "PS01230 ['[DN]P[PAS]R.G.{14,19}[LIVMAF][LIVMCAFT][YAHG].[SAG]C[NAMDSYHKGQ].{1,2}[TNKSI]']\n",
      "PS01231 ['[LIVMF][DN].FP[QHYWM][ST].[HR][LIVMFYT]E']\n",
      "PS01232 ['[GST].G[LIVM]G.[PA]S.[GSTAL][IL].{3}EL']\n",
      "PS01233 ['RDH.D.[GS][GS].{2}SP.RET']\n",
      "PS01234 ['[LMFYCVI][DN]R.{3}[PGA]L[LIVMCA]E[LIVMT].[STL].[PA]']\n",
      "PS01235 ['[LV]P[VI][VTPI][NQLHT][FL][ATVS][AS]GG[LIV][AT]TP[AQS]D[AGVS][AS][LM]']\n",
      "PS01236 ['[GARVS][LVI][ILAV][LIVF]PGGES[TS][STAV]']\n",
      "PS01237 ['[GA][GS]G[GA]ARG.[SA]H.G.{9}[IVY].[IV][DV].{2}[GA]G.S.G']\n",
      "PS01238 ['[LIVM].G.{2}EG.[FYLS].[FW][LIVA][TAG].N[HYF]']\n",
      "PS01239 ['H.[IV].G[KR].F[GA]S.V[ST][HY]E']\n",
      "PS01240 ['[LIVF].{3}[GS].{2}H.[LIVMFY].{4}[LIVMF].{3}[ATV].{1,2}[LIVM].[ATV].{4}[GN].{3,4}[LIVMF]{2}.{2}[STN][SAGT].G[GS][LIVM]']\n",
      "PS01241 ['C.{15}A.{3,4}[GK].{3}C.{2}G.{8,9}P.{7}C']\n",
      "PS01242 ['C.{1,4}C[GSANHK].{1,2}[IVML].{7,11}R[GSANPVLMT].{2}[FYWIL]C.{2}CQ']\n",
      "PS01243 ['G.{2}[LIVM][GC]P.[LI].{4}[SAGDT].{4,6}[LIVM]{2}.{2}A.{2}[MG]T.[LIVM].F']\n",
      "PS01244 ['G.{2}[LIVWPQT].{3}[GACST]C[GSTAM][LIMPTA]C[LIMV][GA]']\n",
      "PS01245 ['[LIVMY][VI]H[GA]D[LF][SN]E[FY]N.[LIVM]']\n",
      "PS01246 ['G[STIF]V.{2}[LIVM].{6}[LIVMF].{3}[DQT].{3}[LIVH].[LIV]P[NW].{2}[LIVMF][LIVFSTA].{5}[NV]']\n",
      "PS01247 ['D.D[PT][GA].DD[TAV][VI]A']\n",
      "PS01248 ['C.{1,2}C.{5}G.{2}C.{2}C.{3,4}[FYW].{3,15}C']\n",
      "PS01249 ['[GS].{4}[LIVMT].{4}[LIVMF].{2}[CSAM][LMFY].{6}[STC].{4,5}[PAC].[LIVMF].[LIVMF].{8}C.{1,2}[CH]']\n",
      "PS01250 ['[LIVM].{3}C[KR].[DENGRH]C[FY].[STN].{2}F.{2}C']\n",
      "PS01251 ['[GSTA][LIVMF].[LIVMAS].[GSAVI][LIVM][DS].[NSAED][HKRNS][VIT].[LMYF][VIGAL].[LIVMF].[LIVM].{4}F']\n",
      "PS01252 ['C.{3}C.{2}C.{2}[KRH].{6,7}[LIF][DNS].{3}C.[LIVM][EQ]C[EQ].{8}W.{2}C']\n",
      "PS01253 ['C.{6,8}[LFY].{5}[FYW].[RK].{8,10}C.C.{6,9}C']\n",
      "PS01254 ['C[DN][DE].{54}CH.{9}C.{12,14}C.{17,19}C.{13}C.{2}C']\n",
      "PS01255 ['[ND].LET.CH.L']\n",
      "PS01256 ['[LIV][KL].{2}[LIVM].{2}L[IL][DEQGVT][KRHNQ].[YT][LIVM].R.{5,7}[FYLV].Y.[SAT]']\n",
      "PS01257 ['ADR.{3}[GR][MH]R.[SAP][FYW]G[KRVTS][PANI].[GS].{1,2}A[KRLV][LIVQ]']\n",
      "PS01258 ['W[LIM].{3}[GR]G[WQ][DENSAV].[FLGA][LIVFTC]']\n",
      "PS01259 ['[LIVAT].{3}L[KARQ].[IVAL]GD[DESG][LIMFV][DENSHQ][LVSHRQ][NSR]']\n",
      "PS01260 ['[DS][NT]R[AE][LI]V.[KD][FY][LIV][GHS]YKL[SR]Q[RK]G[HY].[CW]']\n",
      "PS01261 ['DP[LIVMF]CG[ST]G.{3}[LI]E']\n",
      "PS01262 ['[IMVALH].G.[GSDENK][KRHFW].{4}[CL].DG.{2}[RY].{2}[RH][IL].G']\n",
      "PS01263 ['CK.{2}F.{4}E.{22,23}SGGKD']\n",
      "PS01264 ['[LIVMFYWE]H[PADHL][DENQRS][GSE].{3}G.{2}[WL][ML].{3}[IVA].F']\n",
      "PS01265 ['S.DLPF[AS].{2}[KRQ][FWI]C']\n",
      "PS01266 ['[QGF][WLCF]GDE[GA]K[GA]']\n",
      "PS01267 ['[DEN].{2}[LIVF][DE]{2}[LIV]L.{4}[IV][FY].{4}KG']\n",
      "PS01268 ['G.KD[KRAT].[AG][LVRSIT][TKS].Q.[LIVF][SGCYAT]']\n",
      "PS01269 ['DV[LIV].{2}GH[ST]H.{12}[LIVMF]NPG']\n",
      "PS01270 ['R.{2}[LIV][SAN].{6}[LIV]D.{2}T.{2}WG[LIVT][KRH][LIV].[KRA][LIV]E[LIV][KRQ]']\n",
      "PS01271 ['[STACPI]S.{2}[FY].{2}P[LIVM][GSA].{3}N.[LIVM]V']\n",
      "PS01272 ['G[PA]E.[LIV][STAM]GS[ST]R[LIVM]K[STGA]{3}.{2}K']\n",
      "PS01273 ['[DN][GN].{2}[LIVMFA]{3}GGF.{3}G.P']\n",
      "PS01274 ['[LF][HQ]SENG[LIVF]{2}[GA]']\n",
      "PS01275 ['K.[AV].{4}G.{2}[LIVT].VP.{2}[LIVC].{2}[GD]']\n",
      "PS01276 ['E.F.{2}G[SA][LIVM]C.{4}G.C.[LIVM]S']\n",
      "PS01277 ['[CA][DE][LIVM]{2}[NQV][GTA]D[GA][SG].{2,3}[TAVLC][AT]']\n",
      "PS01278 ['[LIVM].[LIVMT].{2}GC.{3}C[STAN][FY]C.[LIVMT].{4}G']\n",
      "PS01279 ['[GSAED][DN]G.{2}G[FYWLV].{3}[GSA][PTL][FY][DNSHE].I']\n",
      "PS01280 ['[GSA][PTAV].[YH]CPS[LIVMF][ED].K[LIVMFA].[KRNT][FY]']\n",
      "PS01281 ['[AC]G[QNV].[NTAI]G.{2}GY.[EA][SAG]{2}[SAGC][QSAGMVIT]G[LIVMWF][LIVMAF][AST][GA][LIVMTAR][NYAMF][ALVST]']\n",
      "PS01282 ['[HKEPILVY].{2}R.{3,7}[FYW].{11,14}[STAN]G[LMF].[FYHDA].{4}[DESL].{2,3}C.{2}C.{6}[WA].{9}H.{4}[PRSD].C.{2}[LIVMA]']\n",
      "PS01283 ['[LQI]W.{2}[FCL].{3,4}[NT]E[MQ][LIVNM][LIV][TLF].{2}[GR][RG][KRQM]']\n",
      "PS01284 ['[DT][KRP][YQ][GQ]R.[LVY][GA].[IV][FYW]']\n",
      "PS01285 ['[GASP]W.{7,15}[FYW][LIV].[LIVFA][GSTDEN].{6}[LIVF].{2}[IV].[LIVT][QKMT]G']\n",
      "PS01286 ['P.{8,10}[LM]R.[GE][LIVP].GC']\n",
      "PS01287 ['[RH]G.{2}P.G{3}.[LIV]']\n",
      "PS01288 ['Q[LIVM].N.A.[LIVM]P.I.{6}[LIVM]PD.H.G.G.{2}[IV]G']\n",
      "PS01289 ['MDLVK.HL.{2}AVREEVE']\n",
      "PS01290 ['YDI[SA].L[FY].F[IV]D.{3}D[LIV]S']\n",
      "PS01291 ['[FY].[FY]K.{2}H[FY].L[STI].A']\n",
      "PS01292 ['H.SGH[GA].{3}[DE].{3}[LM].{5}P.{3}[LIVM]P.HG[DE]']\n",
      "PS01293 ['[LM][LF]T.R[SA].{3}[RK].{3}G.{3}FPGG']\n",
      "PS01295 ['[IVT][LIVMC][IVT][HS]D[SGAV][AV]R']\n",
      "PS01296 ['S[DN][AS]G.P.[LIV][SNC]DPG']\n",
      "PS01297 ['G.{3}FERV[FY].A[NQ].NC']\n",
      "PS01298 ['[ERND][IVL].[ED].H.{3}K.[DE].{2}S[GA][TAS][ALCGS]']\n",
      "PS01299 ['[KRQ][LF][CST].K[IF]Q.[FY][ST][PA].{3}G.EF.{5}[FY]{2}.{2}[SA]']\n",
      "PS01300 ['C.{2}C.{3,5}[STACD].{4}C.[LIVFQ]C.{4}[RD][NQDS]']\n",
      "PS01302 ['HNHP[SQ]G']\n",
      "PS01303 ['[GSDNA]WT[LIVM].[FY]W.WW']\n",
      "PS01304 ['HP[LIV][AG]GQG.N.G.{2}D']\n",
      "PS01305 ['[LIV].{3}C[NDP][LIVMF][DNQRS]C.[FYM]C']\n",
      "PS01306 ['H[GSA].[LVCYT]H[LAI][LIMSANQVF]G[FYWMH].[HD]']\n",
      "PS01307 ['A[LMF].[GAT]T[LIVMF].G.[LIVMF].{7}P']\n",
      "PS01309 ['[LIVF].[STAC][LIVF]{3}P[PF][LIVA][GAV][IV].{4}[GKN]']\n",
      "PS01310 ['[DNSE].F.Y[DN].{2}[STNR][LIVM][RQ].{2}G']\n",
      "PS01311 ['[GI]R.[GA]N[FWY][LIVMFA][NG].E.{2}G']\n",
      "PS01312 ['[IV].[IV][SA]T[NQ]MAGRG.DI.L']\n",
      "PS01313 ['RGG.{2}T[FYWCAH]H.{2}[GHS]Q.[LIVMT].Y']\n",
      "PS01314 ['S.{2}[LIV].[LIV].{2}G.{4}GTWQ.[LIV]']\n",
      "PS01315 ['S.[LIVMF]KR.{4}KD.[GSA].{2}[LIF][PGS].HGG[LIVMF].DR[LIVMFT]D']\n",
      "PS01316 ['E.{5}[GND].[SAG].{2}[IV].[DE][LIV].{2}[ST]G.T[LMI]']\n",
      "PS01317 ['[STAC]G[LIVM].L.G.E[LIVM][KQ][SA][LIVMA]']\n",
      "PS01318 ['G[AV]F[STA].R[SA].{2}RPN']\n",
      "PS01319 ['[LIVMF][QKRHSA][ACDFGHIKLMNPQRSTVWY].[LIVMAC].{5,6}[LIVMW][RKAYF].[STACIVMF][PV][ACDEFHIKMNPQRSTVWY][LIVMF].[FYI].{2}D']\n",
      "PS01320 ['DV.{5}HI[SA]CD.{4}SE']\n",
      "PS01321 ['G.[GA].[AG].K.[EQA][IVM].{16,19}D.[SAVT]D[AG].[AGS][LIVMCA][ACS]']\n",
      "PS01322 ['G.TL.HEH[LIV]']\n",
      "PS01324 ['[PS].[SAC].[LIVMFY]{2}[QN].{2}NP.{4}[TA].{9,11}[KRD].[LIV][GN].C']\n",
      "PS01325 ['PHHD[SA]STF']\n",
      "PS01326 ['N.[DN][GS][SENGFT].{4}C[GI]N[GA].R']\n",
      "PS01327 ['[KR]GN[LIVM]{2}D[LIVM]A[LIVM][GA][LIVM]{3}G']\n",
      "PS01328 ['[QR][IV].{4}[TC]D.{2}G[IV]V.[HF].{2}[FY]']\n",
      "PS01329 ['[LIV]R.K.[FYW].W[GS]DG.[KH][ST].F.N']\n",
      "PS01330 ['[VAI][LAV][LIV]{2}GGG.[GC].{2}[LIVA].E']\n",
      "PS01331 ['[LIV][LIVMGSTC][DET][RH][FYHCS].{2}S[GSTNP].[AVC][FY][STANQ]']\n",
      "PS01332 ['L.{3}[GRS][LIVY].{2}[STA].{2}G.{2}GG[FYIV].[LIF]']\n",
      "PS01333 ['G.{2}[GAP].{4}[LIV][ST].E[KR][LIVC][AG].[NG]']\n",
      "PS01334 ['[LIVF].[GSAVC].[LIVM]S.[STAD]AG.[FY][LIVN]C[DNS]']\n",
      "PS01335 ['[SP]GP[LIVMWY]GGD.{0,1}Q']\n",
      "PS01336 ['[SA][FY][LIV]L[STN]ESS[LIVMF]F[LIV]']\n",
      "PS01337 ['[LT]LE[FY][AVC][DE][DE][KNQHT][LMT]']\n",
      "PS01339 ['[KR]Y[DE]F[FY]Q.{2}S.[LIVM]GGLL']\n",
      "PS01340 ['[VI].{3}[DE]L.{2}D[FY].[NS][PS]I[DE]']\n",
      "PS01344 ['[IV][LIVF][NS][KRTAVSL][QH].[PAVS].{2}[EQ][LIVM]W.[STA][STAD]']\n",
      "PS01345 ['CSRCC[DE][KR]KSC']\n",
      "PS01346 ['[GN]LW.{2}C.{7,9}[STDENQH]C']\n",
      "PS01347 ['[KRA].{2}[TIVK]P[ST][MGA][GA]G[LIVSA].[LIVMF]{2}']\n",
      "PS01348 ['[NHS].{2}[NK].[TINAS][DN]G[ILVM]DG[LM]']\n",
      "PS01349 ['[RHQ][ST]W[GSA]GARPE']\n",
      "PS01350 ['S[DN][GA]D[LIVAP][LIVAG].H[STAC].{2}[DNT][SAG].{2}[SGA]']\n",
      "PS01351 ['F.{10}RE.{72,86}RD.K.{9}[CS]']\n",
      "PS01352 ['[LIV].[LIV].W.{12,16}Y[EQ][LIV].{25,27}L.{10}[RKH].[RKH].{5,9}[FYW].{2}[FYW].{5}[LIV]']\n",
      "PS01353 ['N.{4}S.{28,35}[LVIM].W.{0,3}P.{5,9}[YF].{1,2}[VILM].W']\n",
      "PS01354 ['[LIV].PDP{2}.{2}[LIV].{8,11}[LVAM].{3}W.{2}P.[ST]W.{4,6}[FY].L.[FY].[LVI]']\n",
      "PS01355 ['[LIVF].{9}[LIV][RK].{9,20}WS.WS.{4}[FYW]']\n",
      "PS01356 ['[LIVM].C.W.{2}G.{5}D.{2}Y.[LIVM].{10,14}C']\n",
      "PS01357 ['C.{2}C.{4,8}[RHDGSCV][YWFMVIL].[CS].{2,5}[CHEQ].[DNSAGE][YFVLI].[LIVFM]C.{2}C']\n",
      "PS01358 ['W.C.{2,4}C.{3}N.{6}C.{2}C']\n",
      "PS01359 ['C.{1,2}C.{5,45}[VMFLWIE].C.{1,4}C.{1,4}[WYFVQHLT]H.{2}C.{5,45}[WFLYI].C.{2}C']\n",
      "PS01360 ['[CH].{2,4}C.{7,17}C.{0,2}C.{4}[YFT]C.{3}[CH].{6,9}H.{3,4}C']\n",
      "PS01361 ['C.{2}C.{7}[CS].{13}C.{2}C.R.WT.GG']\n",
      "PS40000 ['C.{2}C.{2}H.{8}H.{3,4}C.{4}C.C.{2,3}C']\n",
      "PS60000 ['E[DNQ].{8,17}Y.{7}D.[RD][GP].[TS].{3}[AIVFLY]G.{5,11}D']\n",
      "PS60001 ['[GR]C[IV]GR[ILS].W']\n",
      "PS60002 ['EGGELGY']\n",
      "PS60003 ['G.{3}[DN].P.{2}[LIVFT].{3}[LIVM].GDGE']\n",
      "PS60004 ['C[SREYKLIMQVN].{2}[DGWET].[FYSPKV]C[GNDSRHTP].{1,5}[NPGSMTAHF][GWPNIYRSKLQ].CC[STRHGD].{0,2}[NFLWSRYIT]C.{0,3}[VFGAITSNRKL][FLIKRNGH][VWIARKF]C']\n",
      "PS60005 ['C.{2}[EPSAGT].{3}C[GSNDL].{0,3}[PILV].[FPNDSG][GQ].CC.{3,4}C[FLVIA].{1,2}[FVIWA]C']\n",
      "PS60007 ['[RK].{3}[DE].{2}Y']\n",
      "PS60008 ['C.{0,1}[ES]SC[AV][MFYW]I[PS].{0,1}C']\n",
      "PS60009 ['C[GA]E[ST]C[FTV][GLTI]G[TSK]C']\n",
      "PS60010 ['C[LI].{2}G[SA].C.G.{2}K.CC.{4,5}C.{2}YAN[RK]C']\n",
      "PS60011 ['C[IV].{2,4}[RG]C.{2,6}[GP].CCS.{2,4}C.[ADEFGHIKLMNPQRSTVWY].{4,8}C']\n",
      "PS60013 ['CC[TGN][PFG][PRG].{0,2}C[KRS][DS][RK][RQW]C[KR][PD][MLQH]X{0,1}[KR]CC']\n",
      "PS60014 ['CC[SHYN].{0,1}[PRG][RPATV]C[ARMFTNHG].{0,4}[QWHDGENLFYVP][RIVYLGSDW]C']\n",
      "PS60015 ['C[LAV].[DEK].{3}C.{6,7}CC.{4}C.C.{5}C.C']\n",
      "PS60016 ['C[IT]PSGQPC']\n",
      "PS60017 ['CC[GE][ML]TP.C']\n",
      "PS60018 ['C.{5}WC.{4}DCCC.{3}C.{2}AWY.{5}C.{10,11}C']\n",
      "PS60019 ['C[ADEFGHIKLMNPQRSTVWY]{6}C[ADEFGHIKLMNPQRSTVWY]{5}CC.{1,3}CC.{2,4}C.{3,10}C']\n",
      "PS60020 ['C[ADEFGHIKLMNPQRSTVWY]{6}C.{2}CC.CC.{4}C.{9,10}C']\n",
      "PS60021 ['C[KALRVG].[ACDEFGHIKLNPQRSTVWY].{1,3}C.{4,6}CC.{4,6}C.{4}[ERK]WC']\n",
      "PS60022 ['C.{1,4}[FLIV][SEP]C[DE][EIQ].{4,7}C.{0,7}C[KST].{4,18}C[YK].{1,3}C']\n",
      "PS60023 ['CQCC.{2}N[GA][FY]CS']\n",
      "PS60024 ['C.{6}C.{6}CC.{2}C.{2}C.C.{6}C.C.{6,9}C']\n",
      "PS60025 ['GEEE.{2}[KE][ACDFGHIKLMNPQRSTVWY]{2}.{0,1}E.[ILA]RE']\n",
      "PS60026 ['C.{5}C.K.{6}C.{2}CC.{9}C.{4}C.C']\n",
      "PS60027 ['C[PV][WL].P[YW]C.{0,1}$']\n",
      "PS60028 ['C.{6}C.{5}CC.{3}C.{9}RC']\n",
      "PS60029 ['C[ADEFGHIKLMNPQRSTVWY]{6}C[ADEFGHIKLMNPQRSTVWY]{6}CC[ADEFGHIKLMNPQRSTVWY]{8}C[ADEFGHIKLMNPQRSTVWY]C']\n",
      "PS60030 ['YGNG[VL].C.{4}C']\n",
      "PS60031 ['CDVQTR[VAE][MVT][GA]AG[GS]L']\n",
      "PS60032 ['[LVS].[GK]G[WFYLM][YHF]D[ACGS]G[DSN].{2}[KMR][FAILY].[FWYLQTV][APTNS][MLGAQS]']\n"
     ]
    }
   ],
   "source": [
    "# This script goes through the prosite .dat file and create a dictionary with \n",
    "# accession pattern pairs\n",
    "# also converts patterns to be compatible with python re module\n",
    "\n",
    "\n",
    "aaList =  [\"A\",\n",
    "           \"C\",\n",
    "           \"D\",\n",
    "           \"E\",\n",
    "           \"F\",\n",
    "           \"G\",\n",
    "           \"H\",\n",
    "           \"I\",\n",
    "           \"K\",\n",
    "           \"L\",\n",
    "           \"M\",\n",
    "           \"N\",\n",
    "           \"P\",\n",
    "           \"Q\",\n",
    "           \"R\",\n",
    "           \"S\",\n",
    "           \"T\",\n",
    "           \"V\",\n",
    "           \"W\",\n",
    "           \"Y\"]\n",
    "\n",
    "prositeFile = open('Motifs/prosite.dat.txt','r')\n",
    "outputFile = open('Motifs/prosite_preprocessed.txt','a')\n",
    "\n",
    "currentDict = {}\n",
    "\n",
    "#currentPA holds the prosite pattern from dat file; if it spans more than one line, read multiple files and concatenate \n",
    "#pattern from consecutive lines with PA tag in the beginning of the line\n",
    "\n",
    "#when a new line with AC tag is read, currentPA from previous accession is cleared\n",
    "# AC is the row accession for the new protein (i.e. in that row accession of protein such as PS10203)\n",
    "\n",
    "\n",
    "# PA is the row where the pattern is found \n",
    "\n",
    "\n",
    "\n",
    "for line in prositeFile:\n",
    "    line = line.strip()\n",
    "    if re.match(\"AC   \", line):\n",
    "        currentAC = line.split()[1][:-1]\n",
    "        if currentAC not in currentDict.keys():\n",
    "            currentDict[currentAC] = []\n",
    "            if 'currentPA' in globals():\n",
    "                del currentPA\n",
    "    elif re.match(\"PA   \", line): \n",
    "        currentPA = line.split()[1] # Gets the current Pattern\n",
    "        if currentPA[-1] == \".\":\n",
    "            currentPA = currentPA[:-1]\n",
    "        if lastLineTag == \"PA   \": # check if in the last line thre was also a PA\n",
    "            currentPA =  lastLinePA + currentPA # if yes, concatenate the patterns (both Rows PA)\n",
    "            # since we iterate over all lines in the main loop, we catch all PA rows and concatenate everything with this code\n",
    "    else:\n",
    "        if not 'currentPA' in globals():\n",
    "            lastLineTag = line[:5]\n",
    "            continue # skip the current loop (i.e. go until we find again AC/PA)\n",
    "\n",
    "        \n",
    "        currentPAList = currentPA.split(\"-\")\n",
    "        refinedCurrentPAList = []\n",
    "\n",
    "        for n in range(len(currentPAList)):\n",
    "        # Redefine the Pattern such that it works with Python Regex\n",
    "\n",
    "            #change to range\n",
    "\n",
    "            betweenCurlyList = []\n",
    "            betweenSquaresList = []\n",
    "\n",
    "            currentAAList = aaList.copy()\n",
    "            if re.search(\"x\", currentPAList[n]):\n",
    "                currentPAList[n] = currentPAList[n].replace(\"x\",\".\")\n",
    "            if re.search(\"{\" , currentPAList[n]):\n",
    "                currentPAList[n] = currentPAList[n].replace(\"{\",\"#\")\n",
    "            if re.search(\"}\" , currentPAList[n]):\n",
    "                currentPAList[n] = currentPAList[n].replace(\"}\",\"%\")\n",
    "            if \"(\" in currentPAList[n]:\n",
    "                currentPAList[n] = currentPAList[n].replace(\"(\",\"{\")\n",
    "            if \")\" in currentPAList[n]:\n",
    "                currentPAList[n] = currentPAList[n].replace(\")\",\"}\")\n",
    "            if currentPAList[n][0] == \"<\":\n",
    "                currentPAList[n] = currentPAList[n].replace(\"<\",\"^\")\n",
    "            if currentPAList[n][-1] == \">\":\n",
    "                currentPAList[n] = currentPAList[n].replace(\">\",\"$\")\n",
    "            if re.search(\"#\" , currentPAList[n]):\n",
    "                element = currentPAList[n]\n",
    "                betweenCurly =  element[element.find(\"#\")+1 : element.find(\"%\")]\n",
    "                if len(betweenCurly)>1:\n",
    "                    betweenCurlyList = list(betweenCurly)\n",
    "                    for aa in betweenCurlyList:\n",
    "                        if aa in currentAAList:\n",
    "                            currentAAList.remove(aa)\n",
    "                    betweenCurly = str(\"[\") + \"\".join(currentAAList) + str(\"]\")\n",
    "                    #print(betweenCurly)\n",
    "                    currentPAList[n] = re.sub(r'#.*%', betweenCurly,currentPAList[n])\n",
    "                else:\n",
    "                    if betweenCurly in currentAAList:\n",
    "                            currentAAList.remove(betweenCurly)\n",
    "                    betweenCurly = str(\"[\") + \"\".join(currentAAList) + str(\"]\")\n",
    "                    #print(betweenCurly)\n",
    "                    currentPAList[n] = re.sub(r'#.*%', betweenCurly,currentPAList[n])\n",
    "\n",
    "            \n",
    "        finalPA = \"\".join(currentPAList)\n",
    "        if len(finalPA) > 1:\n",
    "            if finalPA[-2] == \">\":\n",
    "                finalPA1 = finalPA[:-2] + \"]\"\n",
    "                refinedCurrentPAList.append(finalPA1)\n",
    "                finalPA2 = finalPA[:finalPA.rindex(\"[\")] + \"$\"\n",
    "                refinedCurrentPAList.append(finalPA2)\n",
    "            else:\n",
    "                refinedCurrentPAList.append(finalPA)\n",
    "\n",
    "        currentDict[currentAC] = refinedCurrentPAList\n",
    "        del currentPA\n",
    "#save information from the present line for retrival while reading the next line\n",
    "# useful while dealing with long Prosite patterns spanning multiple lines\n",
    "    lastLineTag = line[:5]\n",
    "    if re.match(\"PA   \", line):\n",
    "        if 'currentPA' in globals():\n",
    "            lastLinePA = currentPA\n",
    "        else:\n",
    "            lastLinePA = line.split()[1]\n",
    "\n",
    "for key in currentDict.keys():\n",
    "    if len(currentDict[key]) != 0:\n",
    "        print(key,currentDict[key] )\n",
    "        outputFile.write(key + \"\\t\" + \"\\t&\\t\".join(currentDict[key])+ \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     P06857\n",
      "1     P54316\n",
      "2     Q5BKQ4\n",
      "3     P16233\n",
      "4     P54315\n",
      "       ...  \n",
      "78    P02844\n",
      "79    P27587\n",
      "80    P0DSI2\n",
      "81    Q68KK0\n",
      "82    P83629\n",
      "Name: uniprot_id, Length: 83, dtype: object\n",
      "Results saved to Motifs/conserved_motifs_in_disorder.txt\n"
     ]
    }
   ],
   "source": [
    "def load_protein_ids(psiblast_file, hmm_file, e_threshold=0.001):\n",
    "    \"\"\"Load protein IDs from PSI-BLAST and HMM search results.\"\"\"\n",
    "    psiblast_df = pd.read_csv(psiblast_file)\n",
    "    hmm_df = pd.read_csv(hmm_file)\n",
    "    \n",
    "    filtered_hmm_proteins = hmm_df[hmm_df['E-value'] <= e_threshold]['uniprot_id']\n",
    "    print(filtered_hmm_proteins)\n",
    "    psiblast_proteins = set(psiblast_df['uniprot_id'])\n",
    "    hmm_proteins = set(filtered_hmm_proteins)\n",
    "    \n",
    "    return psiblast_proteins.union(hmm_proteins)\n",
    "\n",
    "def load_disordered_regions(mobidb_file, protein_ids):\n",
    "    \"\"\" Fidnthe disordered regions of the proteins in our family to do the analysis on\"\"\"\n",
    "    disordered_regions = {}\n",
    "    with open(mobidb_file, \"r\") as file:\n",
    "        reader = csv.reader(file)\n",
    "        for row in reader:\n",
    "            protein_id = row[0]\n",
    "            # If protein found in family, save its disordered regions\n",
    "            if protein_id in protein_ids: #\n",
    "                parsed_list = ast.literal_eval(row[1]) # use literal_eval bc the areas are saved as nested lists [[...]] , and we want to parse it as these lists \n",
    "                disordered_regions[protein_id] = [tuple(pair) for pair in parsed_list] # turn each list (i.e. disordered region area) to tuple \n",
    "    return disordered_regions\n",
    "\n",
    "def load_sequences(fasta_file, protein_ids):\n",
    "    sequences = {}\n",
    "    for record in SeqIO.parse(fasta_file, \"fasta\"):\n",
    "        protein_id = record.id.split(\"|\")[1]\n",
    "        if protein_id in protein_ids:\n",
    "            sequences[protein_id] = str(record.seq)\n",
    "    return sequences\n",
    "\n",
    "def load_motifs(elm_file, prosite_file):\n",
    "    motifs = {}\n",
    "    # Extract needed information from ELM \n",
    "    with open(elm_file, 'r') as file:\n",
    "        for _ in range(5):  # Skip first 5 header lines (see elm file, first 5 are just headers)\n",
    "            next(file)\n",
    "        reader = csv.DictReader(file, delimiter='\\t')\n",
    "        for row in reader:\n",
    "            motif_name = row['ELMIdentifier']\n",
    "            pattern = row['Regex'].strip()\n",
    "            motifs[motif_name] = pattern\n",
    "\n",
    "    # Extract needed information from Prosite\n",
    "    with open(prosite_file, 'r') as file:\n",
    "        for line in file:\n",
    "            # Split on first tab to separate AC from pattern\n",
    "            parts = line.strip().split('\\t') # split on tab character, as entries on one row are like : PS00001 \"TAB\" N[ACDEFGHIKLMNQRSTVWY][ST][ACDEFGHIKLMNQRSTVWY]\n",
    "            if len(parts) == 2:\n",
    "                ac, pattern = parts # split into the two parts (i.e. the name and the pattern)\n",
    "                motifs[ac] = pattern\n",
    "    return motifs\n",
    "\n",
    "def match_motifs(sequences, disordered_regions, motifs):\n",
    "    # in the sequences found in the family that have a disordered region (or we atleast found one), now look into the disordered regions and see if we find any motifs\n",
    "    results = {}\n",
    "    for protein_id, sequence in sequences.items():\n",
    "        if protein_id in disordered_regions:\n",
    "            results[protein_id] = []\n",
    "            for start, end in disordered_regions[protein_id]:\n",
    "                region_seq = sequence[start - 1:end] # start-1 due to python indexing starting from 0 \n",
    "                # Now that we are looking at a disordered region in one of the proteins of our family, lets\n",
    "                # see if we find any motifs\n",
    "                for motif_name, pattern in motifs.items():\n",
    "                    # Example values:\n",
    "                    #pattern = \"N[ACDEFGHIKLMNQRSTVWY][ST][ACDEFGHIKLMNQRSTVWY]\"\n",
    "                    #region_seq = \"NKSTMNLSTPNQSTV\"\n",
    "\n",
    "                    # The re.finditer() will find ALL occurrences where:\n",
    "                    # - N matches literally\n",
    "                    # - followed by ANY ONE of the amino acids in [ACDEFGHIKLMNQRSTVWY]\n",
    "                    # - followed by either S or T\n",
    "                    # - followed by ANY ONE of the amino acids in [ACDEFGHIKLMNQRSTVWY]\n",
    "                    # --> that is what matches does with the regex search\n",
    "                    matches = re.finditer(r\"{}\".format(pattern), region_seq)\n",
    "                    for match in matches:\n",
    "                        results[protein_id].append({\n",
    "                            \"motif\": motif_name,\n",
    "                            \"match\": match.group(),\n",
    "                            \"start\": start + match.start(),\n",
    "                            \"end\": start + match.end()\n",
    "                        })\n",
    "    return results\n",
    "\n",
    "def save_results(results, output_file):\n",
    "    with open(output_file, \"w\") as file:\n",
    "        for protein_id, matches in results.items():\n",
    "            file.write(f\"> {protein_id}\\n\")\n",
    "            for match in matches:\n",
    "                file.write(\n",
    "                    f\"{match['motif']}\\t{match['match']}\\t{match['start']}-{match['end']}\\n\"\n",
    "                )\n",
    "\n",
    "\n",
    "# Note that the uniprot_sprot.fasta we downloaded locally and not on the Git repository due to its size\n",
    "def main():\n",
    "    mobidb_file = \"Motifs/mobidb_lite_swissprot.csv\"\n",
    "    fasta_file = \"uniprot_sprot.fasta\"\n",
    "    elm_file = \"Motifs/elm_classes.tsv\"\n",
    "    prosite_file = \"Motifs/prosite_preprocessed.txt\"\n",
    "    psiblast_file = \"Model/Evaluation/Predictions/PSI-BLAST/psiblastsearch_output.csv\"\n",
    "    hmm_file = \"Model/Evaluation/Predictions/HMM-SEARCH/hmmsearch_output.csv\"\n",
    "    output_file = \"Motifs/conserved_motifs_in_disorder.txt\"\n",
    "\n",
    "    protein_ids = load_protein_ids(psiblast_file, hmm_file)\n",
    "    sequences = load_sequences(fasta_file, protein_ids)\n",
    "    disordered_regions = load_disordered_regions(mobidb_file, protein_ids)\n",
    "    motifs = load_motifs(elm_file, prosite_file)\n",
    "\n",
    "    results = match_motifs(sequences, disordered_regions, motifs)\n",
    "    save_results(results, output_file)\n",
    "    print(f\"Results saved to {output_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
