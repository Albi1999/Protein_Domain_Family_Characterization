{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biological Data Project\n",
    "\n",
    "Group members:\n",
    "\n",
    "- Alberto Calabrese\n",
    "\n",
    "- Marlon Helbing\n",
    "\n",
    "- Lorenzo Baietti"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"A protein domain is a conserved part of a given protein sequence and tertiary structure that can evolve, function, and exist independently of the rest of the protein chain. Each domain forms a compact three-dimensional structure and often can be independently stable and folded.\" (Wikipedia)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The project is about the characterization of a single domain. Each group is provided with a representative domain sequence and the corresponding Pfam identifier (see table below). The objective of the project is to build a sequence model starting from the assigned sequence and to provide a functional characterization of the entire domain family (homologous proteins)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input\n",
    "A representative sequence of the domain family. Columns are: group, UniProt accession, organism, Pfam identifier, Pfam name, domain position in the corresponding UniProt protein, domain sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "UniProt : P54315 \n",
    "PfamID : PF00151 \n",
    "Domain Position : 18-353 \n",
    "Organism : Homo sapiens (Human) \n",
    "Pfam Name : Lipase/vitellogenin \n",
    "Domain Sequence : KEVCYEDLGCFSDTEPWGGTAIRPLKILPWSPEKIGTRFLLYTNENPNNFQILLLSDPSTIEASNFQMDRKTRFIIHGFIDKGDESWVTDMCKKLFEVEEVNCICVDWKKGSQATYTQAANNVRVVGAQVAQMLDILLTEYSYPPSKVHLIGHSLGAHVAGEAGSKTPGLSRITGLDPVEASFESTPEEVRLDPSDADFVDVIHTDAAPLIPFLGFGTNQQMGHLDFFPNGGESMPGCKKNALSQIVDLDGIWAGTRDFVACNHLRSYKYYLESILNPDGFAAYPCTSYKSFESDKCFPCPDQGCPQMGHYADKFAGRTSEEQQKFFLNTGEASNF\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Domain model definition\n",
    "The objective of the first part of the project is to build a PSSM and HMM model representing the assigned domain. The two models will be generated starting from the assigned input sequence. The accuracy of the models will be evaluated against Pfam annotations as provided in the SwissProt database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import AlignIO\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from scipy.stats import entropy\n",
    "import math\n",
    "from Bio.Align import MultipleSeqAlignment\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "from Bio.Seq import Seq\n",
    "from Bio import SeqIO\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConservationAnalyzer:\n",
    "    def __init__(self, alignment_file):\n",
    "        \"\"\"\n",
    "        Initialize with an alignment file\n",
    "            alignment_file (str): Path to the alignment file\n",
    "        \"\"\"\n",
    "        self.alignment = AlignIO.read(alignment_file, 'fasta')\n",
    "        self.num_sequences = len(self.alignment)\n",
    "        self.alignment_length = self.alignment.get_alignment_length()\n",
    "        \n",
    "    def get_column(self, pos):\n",
    "        \"\"\"Extract a column from the alignment\"\"\"\n",
    "        return [record.seq[pos] for record in self.alignment]\n",
    "    \n",
    "    def calculate_gap_frequency(self, pos):\n",
    "        \"\"\"Calculate frequency of gaps in a column\"\"\"\n",
    "        column = self.get_column(pos)\n",
    "        return column.count('-') / len(column)\n",
    "    \n",
    "    def calculate_amino_acid_frequencies(self, pos):\n",
    "        \"\"\"Calculate frequencies of each amino acid in a column\"\"\"\n",
    "        column = self.get_column(pos)\n",
    "        total = len(column) - column.count('-')  # Don't count gaps, such that when we calculate conservation scores the gaps don't mess it up \n",
    "        if total == 0:\n",
    "            return {}\n",
    "        \n",
    "        counts = Counter(aa for aa in column if aa != '-')\n",
    "        return {aa: count/total for aa, count in counts.items()}\n",
    "    \n",
    "    def calculate_conservation_score(self, pos):\n",
    "        \"\"\"\n",
    "        Calculate conservation score based on frequency of most common amino acid\n",
    "        Ignores gaps in calculation\n",
    "        \"\"\"\n",
    "        freqs = self.calculate_amino_acid_frequencies(pos)\n",
    "        if not freqs:\n",
    "            return 0\n",
    "        return max(freqs.values())\n",
    "    \n",
    "    def calculate_entropy(self, pos):\n",
    "        \"\"\"\n",
    "        Calculate Shannon entropy for a column\n",
    "        Lower entropy means higher conservation\n",
    "        \"\"\"\n",
    "        freqs = self.calculate_amino_acid_frequencies(pos)\n",
    "        if not freqs:\n",
    "            return float('inf')  \n",
    "        \n",
    "        return -sum(p * math.log2(p) for p in freqs.values())\n",
    "    \n",
    "    def get_amino_acid_groups(self):\n",
    "        \"\"\"Define groups of similar amino acids \n",
    "           Based on : https://en.wikipedia.org/wiki/Conservative_replacement#:~:text=There%20are%2020%20naturally%20occurring,both%20small%2C%20negatively%20charged%20residues.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'aliphatic': set('GAVLI'),\n",
    "            'hydroxyl': set('SCUTM'),\n",
    "            'cyclic': set('P'),\n",
    "            'aromatic': set('FYW'),\n",
    "            'basic': set('HKR'),\n",
    "            'acidic': set('DENQ')\n",
    "        }\n",
    "    \n",
    "    def calculate_group_conservation(self, pos):\n",
    "        \"\"\"\n",
    "        Calculate conservation considering amino acid groups\n",
    "        Basically the same as calculate_conversation_score, just that it calculates based on the groups, not single amino acids !\n",
    "        \"\"\"\n",
    "        column = self.get_column(pos)\n",
    "        groups = self.get_amino_acid_groups()\n",
    "        \n",
    "        # Assign each amino acid to its group\n",
    "        aa_to_group = {}\n",
    "        for group_name, aas in groups.items():\n",
    "            for aa in aas:\n",
    "                aa_to_group[aa] = group_name\n",
    "        \n",
    "        # Count group occurrences\n",
    "        group_counts = Counter(aa_to_group.get(aa, 'other') \n",
    "                             for aa in column if aa != '-')\n",
    "        \n",
    "        if not group_counts:\n",
    "            return 0\n",
    "            \n",
    "        return max(group_counts.values()) / sum(group_counts.values())\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    def find_similar_sequences(self, similarity_threshold):\n",
    "        # TODO : I think using JalView for this is better : JalView --> Edit --> Remove Redundancy \n",
    "        similar_pairs = []\n",
    "        \n",
    "        for i in range(len(self.alignment)):\n",
    "            for j in range(i + 1, len(self.alignment)):\n",
    "                seq1 = str(self.alignment[i].seq)\n",
    "                seq2 = str(self.alignment[j].seq)\n",
    "                \n",
    "                # Calculate similarity (ignoring gaps)\n",
    "                matches = sum(a == b for a, b in zip(seq1, seq2) if a != '-' and b != '-')\n",
    "                total = sum(1 for a, b in zip(seq1, seq2) if a != '-' and b != '-')\n",
    "                \n",
    "                if total > 0:\n",
    "                    similarity = matches / total\n",
    "                    if similarity >= similarity_threshold:\n",
    "                        similar_pairs.append((\n",
    "                            self.alignment[i].id,\n",
    "                            self.alignment[j].id,\n",
    "                            similarity\n",
    "                        ))\n",
    "    \n",
    "        return similar_pairs\n",
    "\n",
    "\n",
    "    def analyze_rows(self, similarity_threshold = 0.95):\n",
    "        similar_pairs = self.find_similar_sequences(similarity_threshold)\n",
    "        print(f\"We have {len(similar_pairs)} many pairs with {similarity_threshold} or more identity (excluding gaps) of a total of {self.num_sequences} sequences\")\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO : I took very strict values now such that the number of residues per sequence is below 100 (right now we have length 77) ; the PSSM creation with \n",
    "    # much higher length did not work, but maybe we should write an email and ask ; nevertheless, we can first try some evaluation based on that PSSM and see our scores\n",
    "    def analyze_columns(self, gap_threshold=0.37, conservation_threshold=0.9):\n",
    "        \"\"\"\n",
    "        Analyze all columns and return comprehensive metrics\n",
    "        Returns DataFrame with various conservation metrics for each position\n",
    "        \"\"\"\n",
    "        data = []\n",
    "        \n",
    "        for i in range(self.alignment_length):\n",
    "            gap_freq = self.calculate_gap_frequency(i)\n",
    "            cons_score = self.calculate_conservation_score(i)\n",
    "            info_content = self.calculate_entropy(i)\n",
    "            group_cons = self.calculate_group_conservation(i)\n",
    "            \n",
    "            data.append({\n",
    "                'position': i + 1,\n",
    "                'gap_frequency': gap_freq,\n",
    "                'single_conservation': cons_score,\n",
    "                'entropy': info_content,\n",
    "                'group_conservation': group_cons,\n",
    "                # Here we should look possibly for better ideas\n",
    "                # Check gap frequency not too high (i.e. not nearly all elements in the columns gaps (-))\n",
    "                # Check that the group conservation is high enough (i.e. the amino acids are not too different\n",
    "                # ; right now we do with groups and not single amino acid sequence since I'd say the groups\n",
    "                # are more representative (if we do single amino acids, we'd delete more stuff))\n",
    "                'suggested_remove': (gap_freq > gap_threshold or       \n",
    "                                   group_cons < conservation_threshold)\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_columns_from_alignment(input_file, output_file, columns_to_remove, format=\"fasta\"):\n",
    "    \"\"\"\n",
    "    Remove specified columns from a multiple sequence alignment and save to new file\n",
    "    \n",
    "    Args:\n",
    "        input_file (str): Path to input alignment file\n",
    "        output_file (str): Path where to save trimmed alignment\n",
    "        columns_to_remove (list): List of column indices to remove (0-based)\n",
    "        format (str): File format (default: \"fasta\")\n",
    "    \"\"\"\n",
    "    # Read the alignment\n",
    "    alignment = AlignIO.read(input_file, format)\n",
    "    \n",
    "    # Sort columns to remove in descending order\n",
    "    # (so removing them doesn't affect the indices of remaining columns)\n",
    "    columns_to_remove = sorted(columns_to_remove, reverse=True)\n",
    "    \n",
    "    # Create new alignment records\n",
    "    new_records = []\n",
    "    \n",
    "    # Process each sequence\n",
    "    for record in alignment:\n",
    "        # Convert sequence to list for easier manipulation\n",
    "        seq_list = list(record.seq)\n",
    "        \n",
    "        # Remove specified columns\n",
    "        for col in columns_to_remove:\n",
    "            del seq_list[col]\n",
    "        \n",
    "        # Create new sequence record\n",
    "        new_seq = Seq(''.join(seq_list)) # Join the list element to a string again (i.e. after removal of amino acids out of sequence represented as list, turn into one string again) and turn into Seq object\n",
    "        new_record = SeqRecord(new_seq,\n",
    "                            id=record.id,\n",
    "                            name=record.name,\n",
    "                            description=record.description)\n",
    "        new_records.append(new_record)\n",
    "    \n",
    "    # Create new alignment\n",
    "    # TODO : Maybe we have to add some variables here (i.e. how to do the MSA)!\n",
    "    new_alignment = MultipleSeqAlignment(new_records)\n",
    "    \n",
    "    # Write to file\n",
    "    AlignIO.write(new_alignment, output_file, format)\n",
    "    \n",
    "    return new_alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize analyzer \n",
    "    analyzer = ConservationAnalyzer(\"clustal_rows_removed_100threshold.fa\")\n",
    "    \n",
    "    # Get comprehensive analysis\n",
    "    analysis = analyzer.analyze_columns()\n",
    "   # analysis_2 = analyzer.analyze_rows()\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\nAlignment Summary:\")\n",
    "    print(f\"Number of sequences: {analyzer.num_sequences}\")\n",
    "    print(f\"Alignment length: {analyzer.alignment_length}\")\n",
    "\n",
    "\n",
    "    # Print number of True/False\n",
    "    counts = analysis['suggested_remove'].value_counts()\n",
    "\n",
    "    counts_true = counts[True]  # To be removed\n",
    "    counts_false = counts[False] # To be kept\n",
    "\n",
    "    print(f\"With the current removal tactic, we would remove {(counts_true / (counts_true + counts_false)):.2f} percent of columns ; we keep {counts_false} of {counts_false + counts_true} columns\")\n",
    "    \n",
    "\n",
    "    # Save detailed analysis to CSV\n",
    "    analysis.to_csv(\"conservation_analysis.csv\", index=False)\n",
    "\n",
    "\n",
    "    # Get indices of columns marked for removal\n",
    "    columns_to_remove = analysis[analysis['suggested_remove']]['position'].values.tolist()\n",
    "    # Convert to 0-based indices (if positions were 1-based)\n",
    "    columns_to_remove = [x-1 for x in columns_to_remove]\n",
    "    \n",
    "    # Remove columns and save new alignment\n",
    "    new_alignment = remove_columns_from_alignment(\n",
    "        \"clustal_rows_removed_100threshold.fa\",\n",
    "        \"trimmed_alignment.fasta\",\n",
    "        columns_to_remove\n",
    "    )\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    print(f\"Original alignment length: {analyzer.alignment_length}\")\n",
    "    print(f\"Number of columns removed: {len(columns_to_remove)}\")\n",
    "    print(f\"New alignment length: {new_alignment.get_alignment_length()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models building\n",
    "\n",
    "1. Retrieve homologous proteins starting from your input sequence performing a BLAST search\n",
    "against UniProt or UniRef50 or UniRef90, or any other database\n",
    "\n",
    "2. Generate a multiple sequence alignment (MSA) starting from retrieved hits using T-coffee or\n",
    "ClustalOmega or MUSCLE\n",
    "\n",
    "3. If necessary, edit the MSA with JalView (or with your custom script or CD-HIT) to remove not\n",
    "conserved positions (columns) and/or redundant information (rows)\n",
    "\n",
    "4. Build a PSSM model starting from the MSA\n",
    "\n",
    "5. Build a HMM model starting from the MSA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models evaluation\n",
    "1. Generate predictions. Run HMM-SEARCH and PSI-BLAST with your models against\n",
    "SwissProt.\n",
    "\n",
    "    - Collect the list of retrieved hits\n",
    "\n",
    "    - Collect matching positions of your models in the retrieved hits\n",
    "\n",
    "2. Define your ground truth. Find all proteins in SwissProt annotated (and not annotated) with the assigned Pfam domain\n",
    "\n",
    "    - Collect the list of proteins matching the assigned Pfam domain\n",
    "\n",
    "    - Collect matching positions of the Pfam domain in the retrieved sequences. Domain positions are available here (large tsv file) or using the InterPro API or align the Pfam domain yourself against SwissProt (HMMSEARCH)\n",
    "\n",
    "3. Compare your model with the assigned Pfam. Calculate the precision, recall, F-score, balanced accuracy, MCC\n",
    "\n",
    "    - Comparison at the protein level. Measure the ability of your model to retrieve the same proteins matched by Pfam\n",
    "\n",
    "    - Comparison at the residue level. Measure the ability of your model to match the same position matched by Pfam\n",
    "\n",
    "4. Consider refining your models to improve their performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Domain family characterization\n",
    "Once the family model is defined (previous step), you will look at functional (and structural) aspects/properties of the entire protein family. The objective is to provide insights about the main function of the family.\n",
    "\n",
    "### Taxonomy\n",
    "\n",
    "1. Collect the taxonomic lineage (tree branch) for each protein of the family_sequences dataset\n",
    "from UniProt (entity/organism/lineage in the UniProt XML)\n",
    "\n",
    "2. Plot the taxonomic tree of the family with nodes size proportional to their relative abundance \n",
    "\n",
    "### Function\n",
    "\n",
    "1. Collect GO annotations for each protein of the family_sequences dataset (entity/dbReference type=\"GO\" in the UniProt XML)\n",
    "\n",
    "2. Calculate the enrichment of each term in the dataset compared to GO annotations available in the SwissProt database (you can download the entire SwissProt XML here). You can use Fisher’ exact test and verify that both two-tails and right-tail P-values (or left-tail depending on how you build the confusion matrix) are close to zero\n",
    "\n",
    "3. Plot enriched terms in a word cloud \n",
    "\n",
    "4. Take into consideration the hierarchical structure of the GO ontology and report most significantly enriched branches, i.e. high level terms\n",
    "\n",
    "5. Always report the full name of the terms and not only the GO ID\n",
    "\n",
    "### Motifs\n",
    "1. Search significantly conserved short motifs inside your family. Use ELM classes and ProSite patterns (for ProSite consider only patterns “PA” lines, not the profiles). Make sure to consider as true matches only those that are found inside disordered regions. Disordered regions for the entire SwissProt (as defined by MobiDB-lite) are available here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
