{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biological Data Project\n",
    "\n",
    "Group members:\n",
    "\n",
    "- Alberto Calabrese\n",
    "\n",
    "- Marlon Helbing\n",
    "\n",
    "- Lorenzo Baietti"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"A protein domain is a conserved part of a given protein sequence and tertiary structure that can evolve, function, and exist independently of the rest of the protein chain. Each domain forms a compact three-dimensional structure and often can be independently stable and folded.\" (Wikipedia)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The project is about the characterization of a single domain. Each group is provided with a representative domain sequence and the corresponding Pfam identifier (see table below). The objective of the project is to build a sequence model starting from the assigned sequence and to provide a functional characterization of the entire domain family (homologous proteins)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input\n",
    "A representative sequence of the domain family. Columns are: group, UniProt accession, organism, Pfam identifier, Pfam name, domain position in the corresponding UniProt protein, domain sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "UniProt : P54315 \n",
    "PfamID : PF00151 \n",
    "Domain Position : 18-353 \n",
    "Organism : Homo sapiens (Human) \n",
    "Pfam Name : Lipase/vitellogenin \n",
    "Domain Sequence : KEVCYEDLGCFSDTEPWGGTAIRPLKILPWSPEKIGTRFLLYTNENPNNFQILLLSDPSTIEASNFQMDRKTRFIIHGFIDKGDESWVTDMCKKLFEVEEVNCICVDWKKGSQATYTQAANNVRVVGAQVAQMLDILLTEYSYPPSKVHLIGHSLGAHVAGEAGSKTPGLSRITGLDPVEASFESTPEEVRLDPSDADFVDVIHTDAAPLIPFLGFGTNQQMGHLDFFPNGGESMPGCKKNALSQIVDLDGIWAGTRDFVACNHLRSYKYYLESILNPDGFAAYPCTSYKSFESDKCFPCPDQGCPQMGHYADKFAGRTSEEQQKFFLNTGEASNF\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Domain model definition\n",
    "The objective of the first part of the project is to build a PSSM and HMM model representing the assigned domain. The two models will be generated starting from the assigned input sequence. The accuracy of the models will be evaluated against Pfam annotations as provided in the SwissProt database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import AlignIO\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from scipy.stats import entropy\n",
    "import math\n",
    "from Bio.Align import MultipleSeqAlignment\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "from Bio.Seq import Seq\n",
    "from Bio import SeqIO\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConservationAnalyzer:\n",
    "    def __init__(self, alignment_file):\n",
    "        \"\"\"\n",
    "        Initialize with an alignment file\n",
    "            alignment_file (str): Path to the alignment file\n",
    "        \"\"\"\n",
    "        self.alignment = AlignIO.read(alignment_file, 'fasta')\n",
    "        self.num_sequences = len(self.alignment)\n",
    "        self.alignment_length = self.alignment.get_alignment_length()\n",
    "        \n",
    "    def get_column(self, pos):\n",
    "        \"\"\"Extract a column from the alignment\"\"\"\n",
    "        return [record.seq[pos] for record in self.alignment]\n",
    "    \n",
    "    def calculate_gap_frequency(self, pos):\n",
    "        \"\"\"Calculate frequency of gaps in a column\"\"\"\n",
    "        column = self.get_column(pos)\n",
    "        return column.count('-') / len(column)\n",
    "    \n",
    "    def calculate_amino_acid_frequencies(self, pos):\n",
    "        \"\"\"Calculate frequencies of each amino acid in a column\"\"\"\n",
    "        column = self.get_column(pos)\n",
    "        total = len(column) - column.count('-')  # Don't count gaps, such that when we calculate conservation scores the gaps don't mess it up \n",
    "        if total == 0:\n",
    "            return {}\n",
    "        \n",
    "        counts = Counter(aa for aa in column if aa != '-')\n",
    "        return {aa: count/total for aa, count in counts.items()}\n",
    "    \n",
    "    def calculate_conservation_score(self, pos):\n",
    "        \"\"\"\n",
    "        Calculate conservation score based on frequency of most common amino acid\n",
    "        Ignores gaps in calculation\n",
    "        \"\"\"\n",
    "        freqs = self.calculate_amino_acid_frequencies(pos)\n",
    "        if not freqs:\n",
    "            return 0\n",
    "        return max(freqs.values())\n",
    "    \n",
    "    def calculate_entropy(self, pos):\n",
    "        \"\"\"\n",
    "        Calculate Shannon entropy for a column\n",
    "        Lower entropy means higher conservation\n",
    "        \"\"\"\n",
    "        freqs = self.calculate_amino_acid_frequencies(pos)\n",
    "        if not freqs:\n",
    "            return float('inf')  \n",
    "        \n",
    "        return -sum(p * math.log2(p) for p in freqs.values())\n",
    "    \n",
    "    def get_amino_acid_groups(self):\n",
    "        \"\"\"Define groups of similar amino acids \n",
    "           Based on : https://en.wikipedia.org/wiki/Conservative_replacement#:~:text=There%20are%2020%20naturally%20occurring,both%20small%2C%20negatively%20charged%20residues.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'aliphatic': set('GAVLI'),\n",
    "            'hydroxyl': set('SCUTM'),\n",
    "            'cyclic': set('P'),\n",
    "            'aromatic': set('FYW'),\n",
    "            'basic': set('HKR'),\n",
    "            'acidic': set('DENQ')\n",
    "        }\n",
    "    \n",
    "    def calculate_group_conservation(self, pos):\n",
    "        \"\"\"\n",
    "        Calculate conservation considering amino acid groups\n",
    "        Basically the same as calculate_conversation_score, just that it calculates based on the groups, not single amino acids !\n",
    "        \"\"\"\n",
    "        column = self.get_column(pos)\n",
    "        groups = self.get_amino_acid_groups()\n",
    "        \n",
    "        # Assign each amino acid to its group\n",
    "        aa_to_group = {}\n",
    "        for group_name, aas in groups.items():\n",
    "            for aa in aas:\n",
    "                aa_to_group[aa] = group_name\n",
    "        \n",
    "        # Count group occurrences\n",
    "        group_counts = Counter(aa_to_group.get(aa, 'other') \n",
    "                             for aa in column if aa != '-')\n",
    "        \n",
    "        if not group_counts:\n",
    "            return 0\n",
    "            \n",
    "        return max(group_counts.values()) / sum(group_counts.values())\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    def find_similar_sequences(self, similarity_threshold):\n",
    "        # TODO : I think using JalView for this is better : JalView --> Edit --> Remove Redundancy \n",
    "        similar_pairs = []\n",
    "        \n",
    "        for i in range(len(self.alignment)):\n",
    "            for j in range(i + 1, len(self.alignment)):\n",
    "                seq1 = str(self.alignment[i].seq)\n",
    "                seq2 = str(self.alignment[j].seq)\n",
    "                \n",
    "                # Calculate similarity (ignoring gaps)\n",
    "                matches = sum(a == b for a, b in zip(seq1, seq2) if a != '-' and b != '-')\n",
    "                total = sum(1 for a, b in zip(seq1, seq2) if a != '-' and b != '-')\n",
    "                \n",
    "                if total > 0:\n",
    "                    similarity = matches / total\n",
    "                    if similarity >= similarity_threshold:\n",
    "                        similar_pairs.append((\n",
    "                            self.alignment[i].id,\n",
    "                            self.alignment[j].id,\n",
    "                            similarity\n",
    "                        ))\n",
    "    \n",
    "        return similar_pairs\n",
    "\n",
    "\n",
    "    def analyze_rows(self, similarity_threshold = 0.95):\n",
    "        similar_pairs = self.find_similar_sequences(similarity_threshold)\n",
    "        print(f\"We have {len(similar_pairs)} many pairs with {similarity_threshold} or more identity (excluding gaps) of a total of {self.num_sequences} sequences\")\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO : I took very strict values now such that the number of residues per sequence is below 100 (right now we have length 77) ; the PSSM creation with \n",
    "    # much higher length did not work, but maybe we should write an email and ask ; nevertheless, we can first try some evaluation based on that PSSM and see our scores\n",
    "    def analyze_columns(self, gap_threshold=0.37, conservation_threshold=0.9):\n",
    "        \"\"\"\n",
    "        Analyze all columns and return comprehensive metrics\n",
    "        Returns DataFrame with various conservation metrics for each position\n",
    "        \"\"\"\n",
    "        data = []\n",
    "        \n",
    "        for i in range(self.alignment_length):\n",
    "            gap_freq = self.calculate_gap_frequency(i)\n",
    "            cons_score = self.calculate_conservation_score(i)\n",
    "            info_content = self.calculate_entropy(i)\n",
    "            group_cons = self.calculate_group_conservation(i)\n",
    "            \n",
    "            data.append({\n",
    "                'position': i + 1,\n",
    "                'gap_frequency': gap_freq,\n",
    "                'single_conservation': cons_score,\n",
    "                'entropy': info_content,\n",
    "                'group_conservation': group_cons,\n",
    "                # Here we should look possibly for better ideas\n",
    "                # Check gap frequency not too high (i.e. not nearly all elements in the columns gaps (-))\n",
    "                # Check that the group conservation is high enough (i.e. the amino acids are not too different\n",
    "                # ; right now we do with groups and not single amino acid sequence since I'd say the groups\n",
    "                # are more representative (if we do single amino acids, we'd delete more stuff))\n",
    "                'suggested_remove': (gap_freq > gap_threshold or       \n",
    "                                   group_cons < conservation_threshold)\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_columns_from_alignment(input_file, output_file, columns_to_remove, format=\"fasta\"):\n",
    "    \"\"\"\n",
    "    Remove specified columns from a multiple sequence alignment and save to new file\n",
    "    \n",
    "    Args:\n",
    "        input_file (str): Path to input alignment file\n",
    "        output_file (str): Path where to save trimmed alignment\n",
    "        columns_to_remove (list): List of column indices to remove (0-based)\n",
    "        format (str): File format (default: \"fasta\")\n",
    "    \"\"\"\n",
    "    # Read the alignment\n",
    "    alignment = AlignIO.read(input_file, format)\n",
    "    \n",
    "    # Sort columns to remove in descending order\n",
    "    # (so removing them doesn't affect the indices of remaining columns)\n",
    "    columns_to_remove = sorted(columns_to_remove, reverse=True)\n",
    "    \n",
    "    # Create new alignment records\n",
    "    new_records = []\n",
    "    \n",
    "    # Process each sequence\n",
    "    for record in alignment:\n",
    "        # Convert sequence to list for easier manipulation\n",
    "        seq_list = list(record.seq)\n",
    "        \n",
    "        # Remove specified columns\n",
    "        for col in columns_to_remove:\n",
    "            del seq_list[col]\n",
    "        \n",
    "        # Create new sequence record\n",
    "        new_seq = Seq(''.join(seq_list)) # Join the list element to a string again (i.e. after removal of amino acids out of sequence represented as list, turn into one string again) and turn into Seq object\n",
    "        new_record = SeqRecord(new_seq,\n",
    "                            id=record.id,\n",
    "                            name=record.name,\n",
    "                            description=record.description)\n",
    "        new_records.append(new_record)\n",
    "    \n",
    "    # Create new alignment\n",
    "    # TODO : Maybe we have to add some variables here (i.e. how to do the MSA)!\n",
    "    new_alignment = MultipleSeqAlignment(new_records)\n",
    "    \n",
    "    # Write to file\n",
    "    AlignIO.write(new_alignment, output_file, format)\n",
    "    \n",
    "    return new_alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize analyzer \n",
    "    analyzer = ConservationAnalyzer(\"clustal_rows_removed_100threshold.fa\")\n",
    "    \n",
    "    # Get comprehensive analysis\n",
    "    analysis = analyzer.analyze_columns()\n",
    "   # analysis_2 = analyzer.analyze_rows()\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\nAlignment Summary:\")\n",
    "    print(f\"Number of sequences: {analyzer.num_sequences}\")\n",
    "    print(f\"Alignment length: {analyzer.alignment_length}\")\n",
    "\n",
    "\n",
    "    # Print number of True/False\n",
    "    counts = analysis['suggested_remove'].value_counts()\n",
    "\n",
    "    counts_true = counts[True]  # To be removed\n",
    "    counts_false = counts[False] # To be kept\n",
    "\n",
    "    print(f\"With the current removal tactic, we would remove {(counts_true / (counts_true + counts_false)):.2f} percent of columns ; we keep {counts_false} of {counts_false + counts_true} columns\")\n",
    "    \n",
    "\n",
    "    # Save detailed analysis to CSV\n",
    "    analysis.to_csv(\"conservation_analysis.csv\", index=False)\n",
    "\n",
    "\n",
    "    # Get indices of columns marked for removal\n",
    "    columns_to_remove = analysis[analysis['suggested_remove']]['position'].values.tolist()\n",
    "    # Convert to 0-based indices (if positions were 1-based)\n",
    "    columns_to_remove = [x-1 for x in columns_to_remove]\n",
    "    \n",
    "    # Remove columns and save new alignment\n",
    "    new_alignment = remove_columns_from_alignment(\n",
    "        \"clustal_rows_removed_100threshold.fa\",\n",
    "        \"trimmed_alignment.fasta\",\n",
    "        columns_to_remove\n",
    "    )\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    print(f\"Original alignment length: {analyzer.alignment_length}\")\n",
    "    print(f\"Number of columns removed: {len(columns_to_remove)}\")\n",
    "    print(f\"New alignment length: {new_alignment.get_alignment_length()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models building\n",
    "\n",
    "1. Retrieve homologous proteins starting from your input sequence performing a BLAST search\n",
    "against UniProt or UniRef50 or UniRef90, or any other database\n",
    "\n",
    "2. Generate a multiple sequence alignment (MSA) starting from retrieved hits using T-coffee or\n",
    "ClustalOmega or MUSCLE\n",
    "\n",
    "3. If necessary, edit the MSA with JalView (or with your custom script or CD-HIT) to remove not\n",
    "conserved positions (columns) and/or redundant information (rows)\n",
    "\n",
    "4. Build a PSSM model starting from the MSA\n",
    "\n",
    "5. Build a HMM model starting from the MSA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models evaluation\n",
    "1. Generate predictions. Run HMM-SEARCH and PSI-BLAST with your models against\n",
    "SwissProt.\n",
    "\n",
    "    - Collect the list of retrieved hits\n",
    "\n",
    "    - Collect matching positions of your models in the retrieved hits\n",
    "\n",
    "2. Define your ground truth. Find all proteins in SwissProt annotated (and not annotated) with the assigned Pfam domain\n",
    "\n",
    "    - Collect the list of proteins matching the assigned Pfam domain\n",
    "\n",
    "    - Collect matching positions of the Pfam domain in the retrieved sequences. Domain positions are available here (large tsv file) or using the InterPro API or align the Pfam domain yourself against SwissProt (HMMSEARCH)\n",
    "\n",
    "3. Compare your model with the assigned Pfam. Calculate the precision, recall, F-score, balanced accuracy, MCC\n",
    "\n",
    "    - Comparison at the protein level. Measure the ability of your model to retrieve the same proteins matched by Pfam\n",
    "\n",
    "    - Comparison at the residue level. Measure the ability of your model to match the same position matched by Pfam\n",
    "\n",
    "4. Consider refining your models to improve their performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Domain family characterization\n",
    "Once the family model is defined (previous step), you will look at functional (and structural) aspects/properties of the entire protein family. The objective is to provide insights about the main function of the family."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taxonomy\n",
    "\n",
    "1. Collect the taxonomic lineage (tree branch) for each protein of the family_sequences dataset\n",
    "from UniProt (entity/organism/lineage in the UniProt XML)\n",
    "\n",
    "2. Plot the taxonomic tree of the family with nodes size proportional to their relative abundance \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MARLON EDIT \n",
    "import pandas as pd \n",
    "\n",
    "# Changed that we take a union of the proteins found by HMM (with e-value thresh of 0.001) and PSIBLAST (all 21) to represent our \"family_sequences\"\n",
    "e_threshold = 0.001\n",
    "psiblast_df = pd.read_csv(\"psiblast_parsed.csv\")\n",
    "hmm_df = pd.read_csv(\"hmmsearch_output.csv\")\n",
    "filtered_hmm_proteins = hmm_df[hmm_df['E-value'] <= e_threshold]['uniprot_id']\n",
    "    \n",
    "psiblast_proteins = set(psiblast_df['uniprot_id'])\n",
    "hmm_proteins = set(filtered_hmm_proteins)\n",
    "\n",
    "\n",
    "family_sequences = list(psiblast_proteins.union(hmm_proteins))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MARLON EDIT \n",
    "import requests\n",
    "import csv\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from scipy.spatial.distance import pdist\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "class TaxonomyAnalyzer:\n",
    "    def __init__(self, max_retries: int = 3, retry_delay: int = 1):\n",
    "        self.max_retries = max_retries\n",
    "        self.retry_delay = retry_delay\n",
    "        self.uniprot_base_url = \"https://rest.uniprot.org/uniprotkb/\"\n",
    "        \n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "            filename=\"Final_taxonomy_analysis.log\"\n",
    "        )\n",
    "\n",
    "\n",
    "    def fetch_taxonomy_info(self, protein_ids: List[str], output_file: str) -> str:\n",
    "        \"\"\"\n",
    "        Fetch taxonomy information \n",
    "        \"\"\"\n",
    "        taxonomy_data = []\n",
    "        error_counts = {\"success\": 0, \"failed\": 0}\n",
    "\n",
    "        for protein_id in tqdm(protein_ids, desc=\"Fetching taxonomy info\"):\n",
    "            for attempt in range(self.max_retries):\n",
    "                try:\n",
    "                    response = requests.get(f\"{self.uniprot_base_url}{protein_id}.json\")\n",
    "                    response.raise_for_status()\n",
    "                    data = response.json()\n",
    "\n",
    "                    taxonomy = data.get(\"organism\", {})\n",
    "                    scientific_name = taxonomy.get(\"scientificName\", \"N/A\")\n",
    "                    lineage = taxonomy.get(\"lineage\", [])\n",
    "                    taxonomy_data.append([protein_id, scientific_name, \" > \".join(lineage)])\n",
    "\n",
    "                    error_counts[\"success\"] += 1\n",
    "                    break\n",
    "                \n",
    "                except requests.exceptions.RequestException as e:\n",
    "                    if attempt == self.max_retries - 1:\n",
    "                        logging.error(f\"Failed to fetch {protein_id} after {self.max_retries} attempts: {str(e)}\")\n",
    "                        taxonomy_data.append([protein_id, \"Error\", \"\"])\n",
    "                        error_counts[\"failed\"] += 1\n",
    "                    else:\n",
    "                        time.sleep(self.retry_delay)\n",
    "                        \n",
    "        with open(output_file, \"w\", newline=\"\") as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\"Protein ID\", \"Scientific Name\", \"Lineage\"])\n",
    "            writer.writerows(taxonomy_data)\n",
    "        logging.info(f\"Taxonomy data saved to {output_file}\")\n",
    "\n",
    "        return output_file\n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MARLON EDIT\n",
    "from Bio import Phylo\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "class PhyloTreeVisualizer:\n",
    "    def create_newick_string(self, taxonomy_df):\n",
    "        \"\"\"Convert taxonomy data to Newick format\"\"\"\n",
    "        lineage_counts = {}\n",
    "        for _, row in taxonomy_df.iterrows():\n",
    "            if isinstance(row['Lineage'], str):\n",
    "                taxa = row['Lineage'].split(' > ')\n",
    "                for i in range(len(taxa)):\n",
    "                    lineage = ' > '.join(taxa[:i+1])\n",
    "                    lineage_counts[lineage] = lineage_counts.get(lineage, 0) + 1\n",
    "\n",
    "        def build_newick(taxa, parent=''):\n",
    "            current = taxa[-1] if taxa else ''\n",
    "            current_path = ' > '.join(taxa)\n",
    "            count = lineage_counts.get(current_path, 1)\n",
    "            \n",
    "            children = []\n",
    "            for lineage in lineage_counts.keys():\n",
    "                if lineage.startswith(current_path + ' > '):\n",
    "                    next_level = lineage.split(' > ')[len(taxa)]\n",
    "                    if next_level not in children:\n",
    "                        children.append(next_level)\n",
    "            \n",
    "            if children:\n",
    "                child_strings = [build_newick(taxa + [child]) for child in children]\n",
    "                return f\"({','.join(child_strings)}){current}:{count}\"\n",
    "            else:\n",
    "                return f\"{current}:{count}\"\n",
    "\n",
    "        root_taxa = set()\n",
    "        for lineage in lineage_counts.keys():\n",
    "            root = lineage.split(' > ')[0]\n",
    "            if root not in root_taxa:\n",
    "                root_taxa.add(root)\n",
    "        \n",
    "        newick = f\"({','.join(build_newick([taxa]) for taxa in root_taxa)});\"\n",
    "        return newick\n",
    "\n",
    "    def create_phylogenetic_tree(self, taxonomy_file, output_file):\n",
    "        \"\"\"Create and save a phylogenetic tree visualization\"\"\"\n",
    "        # Read taxonomy data\n",
    "        df = pd.read_csv(taxonomy_file)\n",
    "        \n",
    "        # Create Newick string\n",
    "        newick_str = self.create_newick_string(df)\n",
    "        \n",
    "        # Parse tree from Newick format\n",
    "        handle = StringIO(newick_str)\n",
    "        tree = Phylo.read(handle, \"newick\")\n",
    "\n",
    "        # Check with ASCII representation\n",
    "        print(\"ASCII representation of the tree:\")\n",
    "        Phylo.draw_ascii(tree)\n",
    "        \n",
    "        # Set up the plot with larger figure size and adjusted dimensions\n",
    "        fig = plt.figure(figsize=(20, 30))  # Increased figure size\n",
    "        \n",
    "        # Draw the tree with customized parameters\n",
    "        axes = fig.add_subplot(1, 1, 1)\n",
    "        Phylo.draw(tree, \n",
    "                  axes=axes,\n",
    "                  do_show=False,\n",
    "                  branch_labels=lambda c: str(int(c.branch_length)) if c.branch_length else '')\n",
    "        \n",
    "        # Adjust the plot\n",
    "        axes.set_title(\"Taxonomic Tree with Branch Lengths Showing Relative Abundance\", pad=20, size=16)\n",
    "        axes.set_xlabel(\"Relative Abundance\", size=12)\n",
    "        \n",
    "        # Increase spacing between taxa\n",
    "        axes.set_xticks(axes.get_xticks())\n",
    "        axes.set_yticks(axes.get_yticks())\n",
    "        \n",
    "        # Adjust label sizes and spacing\n",
    "        plt.setp(axes.get_xticklabels(), fontsize=10)\n",
    "        plt.setp(axes.get_yticklabels(), fontsize=10, style='italic')\n",
    "        \n",
    "        # Adjust layout to prevent label cutoff\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the plot with high resolution\n",
    "        plt.savefig(output_file, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        return output_file\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MARLON EDIT\n",
    "def visualize_phylogenetic_tree(taxonomy_file, output_file):\n",
    "    visualizer = PhyloTreeVisualizer()\n",
    "    return visualizer.create_phylogenetic_tree(taxonomy_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching taxonomy info: 100%|██████████| 57/57 [01:23<00:00,  1.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Files created:\n",
      "Taxonomy data: Final_taxonomy_info.csv\n",
      "Phylogenetic tree: Final_phylogenetic_tree.png\n"
     ]
    }
   ],
   "source": [
    "#MARLON EDIT\n",
    "def main():\n",
    "    # First fetch taxonomy info as before\n",
    "    analyzer = TaxonomyAnalyzer()\n",
    "    taxonomy_file = analyzer.fetch_taxonomy_info(family_sequences, \"Final_taxonomy_info.csv\")\n",
    "    \n",
    "    # Create the phylogenetic tree\n",
    "    tree_file = visualize_phylogenetic_tree(taxonomy_file, \"Final_phylogenetic_tree.png\")\n",
    "    \n",
    "    print(\"\\nFiles created:\")\n",
    "    print(f\"Taxonomy data: {taxonomy_file}\")\n",
    "    print(f\"Phylogenetic tree: {tree_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching taxonomy data:   0%|          | 0/57 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Q64425: 100%|██████████| 57/57 [00:14<00:00,  3.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taxonomy file saved to: taxonomy_info.csv\n",
      "Tree saved to: phylogenetic_tree_freq.png\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from ete3 import Tree, TreeStyle, NodeStyle, TextFace\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# TaxonomyAnalyzer Class for fetching taxonomy information\n",
    "class TaxonomyAnalyzer:\n",
    "    def __init__(self, max_retries: int = 3, retry_delay: int = 1):\n",
    "        self.max_retries = max_retries\n",
    "        self.retry_delay = retry_delay\n",
    "        self.uniprot_base_url = \"https://rest.uniprot.org/uniprotkb/\"\n",
    "\n",
    "    def fetch_taxonomy_info(self, protein_ids: list, output_file: str):\n",
    "        taxonomy_data = []\n",
    "\n",
    "        pbar = tqdm(protein_ids, desc=\"Fetching taxonomy data\")\n",
    "\n",
    "        for protein_id in pbar:\n",
    "            pbar.set_description(f\"Processing {protein_id}\")\n",
    "\n",
    "            for attempt in range(self.max_retries):\n",
    "                try:\n",
    "                    response = requests.get(f\"{self.uniprot_base_url}{protein_id}.json\")\n",
    "                    response.raise_for_status()\n",
    "                    data = response.json()\n",
    "\n",
    "                    taxonomy = data.get(\"organism\", {})\n",
    "                    scientific_name = taxonomy.get(\"scientificName\", \"N/A\")\n",
    "                    lineage = taxonomy.get(\"lineage\", [])\n",
    "\n",
    "                    taxonomy_data.append([protein_id, scientific_name, \" > \".join(lineage)])\n",
    "                    break\n",
    "\n",
    "                except requests.exceptions.RequestException as e:\n",
    "                    print(f\"Error fetching data for {protein_id}: {e}\")\n",
    "                    if attempt == self.max_retries - 1:\n",
    "                        taxonomy_data.append([protein_id, \"Error\", \"\"])\n",
    "                    else:\n",
    "                        time.sleep(self.retry_delay)\n",
    "\n",
    "        taxonomy_df = pd.DataFrame(taxonomy_data, columns=[\"Protein ID\", \"Scientific Name\", \"Lineage\"])\n",
    "        taxonomy_df.to_csv(output_file, index=False)\n",
    "        return taxonomy_df\n",
    "\n",
    "# Load protein IDs from files\n",
    "def load_protein_ids(psiblast_file, hmm_file, e_threshold=0.001):\n",
    "    psiblast_df = pd.read_csv(psiblast_file)\n",
    "    hmm_df = pd.read_csv(hmm_file)\n",
    "\n",
    "    filtered_hmm_proteins = hmm_df[hmm_df['E-value'] <= e_threshold]['uniprot_id']\n",
    "    psiblast_proteins = set(psiblast_df['uniprot_id'])\n",
    "    hmm_proteins = set(filtered_hmm_proteins)\n",
    "\n",
    "    return list(psiblast_proteins.union(hmm_proteins))\n",
    "\n",
    "# Process taxonomy data\n",
    "def process_taxonomy(data, correct_column_name):\n",
    "    taxonomy_dict = {}\n",
    "    frequency_counts = {}\n",
    "    \n",
    "    for _, row in data.iterrows():\n",
    "        lineage = row[correct_column_name].split(\" > \")\n",
    "        current = taxonomy_dict\n",
    "        # Track the full path to maintain hierarchy information\n",
    "        current_path = [] # such that we count occurences of terms in the correct \"level\" where they appear (i.e. always count just in the \"column\" of the linage)\n",
    "        \n",
    "        for level in lineage:\n",
    "            current_path.append(level)\n",
    "            path_key = \" > \".join(current_path)\n",
    "            \n",
    "            # Count frequencies using the full path as key\n",
    "            if path_key not in frequency_counts:\n",
    "                frequency_counts[path_key] = 0\n",
    "            frequency_counts[path_key] += 1\n",
    "            \n",
    "            if level not in current:\n",
    "                current[level] = {}\n",
    "            current = current[level]\n",
    "    \n",
    "    return taxonomy_dict, frequency_counts\n",
    "\n",
    "# Create a Newick string for the taxonomy tree\n",
    "def dict_to_newick(d, parent_abundance=None):\n",
    "    newick = \"\"\n",
    "    for key, sub_dict in d.items():\n",
    "        size = parent_abundance.get(key, 1) if parent_abundance else 1\n",
    "        sub_tree = dict_to_newick(sub_dict, parent_abundance)\n",
    "        newick += f\"({sub_tree}){key}:{size},\" if sub_tree else f\"{key}:{size},\"\n",
    "    return newick.rstrip(\",\")\n",
    "\n",
    "\n",
    "\n",
    "# Fetch taxonomy data\n",
    "def main():\n",
    "    psiblast_file = \"psiblast_parsed.csv\"\n",
    "    hmm_file = \"hmmsearch_output.csv\"\n",
    "    protein_ids = load_protein_ids(psiblast_file, hmm_file)\n",
    "\n",
    "    analyzer = TaxonomyAnalyzer()\n",
    "    taxonomy_data = analyzer.fetch_taxonomy_info(protein_ids, \"taxonomy_info.csv\")\n",
    "\n",
    "    print(\"Taxonomy file saved to: taxonomy_info.csv\")\n",
    "\n",
    "    # Correct column name\n",
    "    correct_column_name = \"Lineage\"  # Use the correct column name\n",
    "\n",
    "    # Create a nested dictionary of taxonomy\n",
    "    taxonomy_dict, frequency_counts = process_taxonomy(taxonomy_data, correct_column_name)\n",
    "\n",
    "    # Count relative abundance (of the different paths ! ; right now we don't really use that)\n",
    "    abundance_counts = taxonomy_data[correct_column_name].value_counts().to_dict()\n",
    "\n",
    "    \n",
    "\n",
    "# TODO : abundance counts used here, but it doesn't show at all in the graph ; we need to ask professor if what we have now is already enough, then we should remove \n",
    "# TODO : this part with abundance counts\n",
    "    newick_tree = f\"({dict_to_newick(taxonomy_dict, abundance_counts)});\"\n",
    "\n",
    "    # Plot using ETE Toolkit\n",
    "    phylo_tree = Tree(newick_tree, format=1)\n",
    "    tree_style = TreeStyle()\n",
    "    tree_style.show_leaf_name = False\n",
    "\n",
    "\n",
    "    # Adjust node sizes (normalize and refine scaling)\n",
    "    max_size = 50  # Increase max size for better differentiation\n",
    "    scaling_factor = 2  # Further refine scaling for visual contrast\n",
    "    for node in phylo_tree.traverse():\n",
    "        # Get the full path from root to this node\n",
    "        path = []\n",
    "        current = node\n",
    "        while current:\n",
    "            if current.name:  # Skip empty names\n",
    "                path.insert(0, current.name)\n",
    "            current = current.up\n",
    "        \n",
    "        path_key = \" > \".join(path)\n",
    "        count = frequency_counts.get(path_key, 1)\n",
    "        nstyle = NodeStyle()\n",
    "        size = abundance_counts.get(node.name, 1)\n",
    "        nstyle[\"size\"] = min(size * scaling_factor, max_size)  # Scale and cap node size\n",
    "        node.set_style(nstyle)\n",
    "        # Add label with name and count\n",
    "        node.add_face(TextFace(f\"{node.name} ({count})\", fsize=10), column=0)\n",
    "\n",
    "    # Improve tree spacing\n",
    "    tree_style.branch_vertical_margin = 30  # Increase spacing for better visibility\n",
    "\n",
    "    # Save the tree to a high-resolution PNG file\n",
    "    output_file = \"phylogenetic_tree_freq.png\"\n",
    "    phylo_tree.render(output_file, w=3000, h=2000, tree_style=tree_style)\n",
    "\n",
    "    print(f\"Tree saved to: {output_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function\n",
    "\n",
    "1. Collect GO annotations for each protein of the family_sequences dataset (entity/dbReference type=\"GO\" in the UniProt XML)\n",
    "\n",
    "2. Calculate the enrichment of each term in the dataset compared to GO annotations available in the SwissProt database (you can download the entire SwissProt XML here). You can use Fisher’ exact test and verify that both two-tails and right-tail P-values (or left-tail depending on how you build the confusion matrix) are close to zero\n",
    "\n",
    "3. Plot enriched terms in a word cloud \n",
    "\n",
    "4. Take into consideration the hierarchical structure of the GO ontology and report most significantly enriched branches, i.e. high level terms\n",
    "\n",
    "5. Always report the full name of the terms and not only the GO ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: obonet in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (1.1.0)\n",
      "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from obonet) (3.3)\n",
      "Requirement already satisfied: statsmodels in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (0.14.4)\n",
      "Requirement already satisfied: numpy<3,>=1.22.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from statsmodels) (2.0.0)\n",
      "Requirement already satisfied: scipy!=1.9.2,>=1.8 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from statsmodels) (1.14.0)\n",
      "Requirement already satisfied: pandas!=2.1.0,>=1.4 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from statsmodels) (2.2.2)\n",
      "Requirement already satisfied: patsy>=0.5.6 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from statsmodels) (1.0.1)\n",
      "Requirement already satisfied: packaging>=21.3 in /Users/albertocalabrese99/Library/Python/3.12/lib/python/site-packages (from statsmodels) (24.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/albertocalabrese99/Library/Python/3.12/lib/python/site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/albertocalabrese99/Library/Python/3.12/lib/python/site-packages (from python-dateutil>=2.8.2->pandas!=2.1.0,>=1.4->statsmodels) (1.16.0)\n",
      "Requirement already satisfied: goatools in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (1.4.12)\n",
      "Requirement already satisfied: docopt in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from goatools) (0.6.2)\n",
      "Requirement already satisfied: ftpretty in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from goatools) (0.4.0)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from goatools) (2.0.0)\n",
      "Requirement already satisfied: openpyxl in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from goatools) (3.1.5)\n",
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from goatools) (2.2.2)\n",
      "Requirement already satisfied: pydot in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from goatools) (3.0.3)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from goatools) (2.32.3)\n",
      "Requirement already satisfied: rich in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from goatools) (13.9.4)\n",
      "Requirement already satisfied: scipy in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from goatools) (1.14.0)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from goatools) (75.6.0)\n",
      "Requirement already satisfied: statsmodels in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from goatools) (0.14.4)\n",
      "Requirement already satisfied: xlsxwriter in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from goatools) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil in /Users/albertocalabrese99/Library/Python/3.12/lib/python/site-packages (from ftpretty->goatools) (2.9.0.post0)\n",
      "Requirement already satisfied: et-xmlfile in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from openpyxl->goatools) (2.0.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pandas->goatools) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pandas->goatools) (2024.1)\n",
      "Requirement already satisfied: pyparsing>=3.0.9 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pydot->goatools) (3.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->goatools) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->goatools) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->goatools) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->goatools) (2024.6.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from rich->goatools) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/albertocalabrese99/Library/Python/3.12/lib/python/site-packages (from rich->goatools) (2.18.0)\n",
      "Requirement already satisfied: patsy>=0.5.6 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from statsmodels->goatools) (1.0.1)\n",
      "Requirement already satisfied: packaging>=21.3 in /Users/albertocalabrese99/Library/Python/3.12/lib/python/site-packages (from statsmodels->goatools) (24.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->goatools) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/albertocalabrese99/Library/Python/3.12/lib/python/site-packages (from python-dateutil->ftpretty->goatools) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install obonet\n",
    "!pip install statsmodels\n",
    "!pip install goatools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONTOLOGY MARLON \n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "from scipy.stats import fisher_exact\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import obonet\n",
    "import networkx as nx\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "from collections import defaultdict\n",
    "\n",
    "# Step 1: Load Protein IDs\n",
    "# TODO : we basically did this above already for taxonomy task and just here neatly written into a function, so we could maybe just do it once in the whole code later on\n",
    "def load_protein_ids(psiblast_file, hmm_file, e_threshold=0.001):\n",
    "    \"\"\"Load protein IDs from PSI-BLAST and HMM search results.\"\"\"\n",
    "    psiblast_df = pd.read_csv(psiblast_file)\n",
    "    hmm_df = pd.read_csv(hmm_file)\n",
    "    \n",
    "    filtered_hmm_proteins = hmm_df[hmm_df['E-value'] <= e_threshold]['uniprot_id']\n",
    "    psiblast_proteins = set(psiblast_df['uniprot_id'])\n",
    "    hmm_proteins = set(filtered_hmm_proteins)\n",
    "    \n",
    "    return list(psiblast_proteins.union(hmm_proteins))\n",
    "\n",
    "'''\n",
    "def fetch_go_annotations(protein_id):\n",
    "    \"\"\"\n",
    "    Fetch and categorize GO annotations for a given protein ID from the UniProt API.\n",
    "    \n",
    "    Args:\n",
    "        protein_id (str): The UniProt ID of the protein\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary containing:\n",
    "            - Categorized GO terms separated by molecular function, biological process, \n",
    "              and cellular component (new format)\n",
    "    \"\"\"\n",
    "    # Define the UniProt API URL for XML data\n",
    "    url = f\"https://rest.uniprot.org/uniprotkb/{protein_id}.xml\"\n",
    "\n",
    "    try:\n",
    "        # Fetch the XML data from UniProt\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Initialize our data structures\n",
    "        go_terms = []  # Original format\n",
    "        categorized_terms = {\n",
    "            'molecular_function': [],\n",
    "            'biological_process': [],\n",
    "            'cellular_component': []\n",
    "        }\n",
    "        \n",
    "        # Set up namespace for XML parsing\n",
    "        namespaces = {'ns': 'http://uniprot.org/uniprot'}\n",
    "        root = ET.fromstring(response.content)\n",
    "        \n",
    "        # Find all GO term references in the XML\n",
    "        for db_ref in root.findall(\".//ns:dbReference[@type='GO']\", namespaces):\n",
    "            go_id = db_ref.attrib.get('id')\n",
    "            term = db_ref.find(\"ns:property[@type='term']\", namespaces)\n",
    "\n",
    "            go_term = term.get('value')\n",
    "            \n",
    "            if go_id and term is not None:\n",
    "                # Store in original format\n",
    "                term_value = term.attrib['value']\n",
    "                \n",
    "                # Categorize based on prefix\n",
    "                if term_value.startswith('F:'):\n",
    "                    categorized_terms['molecular_function'].append({\n",
    "                        'id': go_id,\n",
    "                        'term': term_value[2:]  # Remove 'F:' prefix\n",
    "                    })\n",
    "                elif term_value.startswith('P:'):\n",
    "                    categorized_terms['biological_process'].append({\n",
    "                        'id': go_id,\n",
    "                        'term': term_value[2:]  # Remove 'P:' prefix\n",
    "                    })\n",
    "                elif term_value.startswith('C:'):\n",
    "                    categorized_terms['cellular_component'].append({\n",
    "                        'id': go_id,\n",
    "                        'term': term_value[2:]  # Remove 'C:' prefix\n",
    "                    })\n",
    "        \n",
    "        return {\n",
    "            'categorized': categorized_terms  # New categorized format\n",
    "}\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching GO annotations for {protein_id}: {e}\")\n",
    "        return {\n",
    "            'categorized': {\n",
    "                'molecular_function': [],\n",
    "                'biological_process': [],\n",
    "                'cellular_component': []\n",
    "            }\n",
    "        }\n",
    "    '''\n",
    "\n",
    "# STEP 1 \n",
    "\n",
    " # Define the UniProt API URL for XML data\n",
    "def fetch_go_annotations(protein_id):\n",
    "    \"\"\"\n",
    "    Fetch GO annotations and create GO ID to protein list mapping.\n",
    "    \n",
    "    Args:\n",
    "        protein_ids (list): List of UniProt protein IDs\n",
    "        \n",
    "    Returns:\n",
    "        List : List of the GO ids found for that protein\n",
    "    \"\"\"\n",
    "    go_ids = []\n",
    "    \n",
    "\n",
    "    url = f\"https://rest.uniprot.org/uniprotkb/{protein_id}.xml\"\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        namespaces = {'ns': 'http://uniprot.org/uniprot'}\n",
    "        root = ET.fromstring(response.content)\n",
    "        \n",
    "        for db_ref in root.findall(\".//ns:dbReference[@type='GO']\", namespaces):\n",
    "            go_id = db_ref.attrib.get('id')\n",
    "            \n",
    "            if go_id:\n",
    "                go_ids.append(go_id)\n",
    "                \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching GO annotations for {protein_id}: {e}\")\n",
    "   \n",
    "            \n",
    "    return go_ids\n",
    "\n",
    "\n",
    "\n",
    "def fetch_go_terms(protein_ids):\n",
    "\n",
    "    go_terms = {}\n",
    "\n",
    "    for protein_id in protein_ids:\n",
    "        url = f\"https://rest.uniprot.org/uniprotkb/{protein_id}.xml\"\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            namespaces = {'ns': 'http://uniprot.org/uniprot'}\n",
    "            root = ET.fromstring(response.content)\n",
    "            \n",
    "            for db_ref in root.findall(\".//ns:dbReference[@type='GO']\", namespaces):\n",
    "                go_id = db_ref.attrib.get('id')\n",
    "                term = db_ref.find(\"ns:property[@type='term']\", namespaces)\n",
    "                if go_id and term is not None:\n",
    "                    go_term = term.get('value')\n",
    "                    go_terms[go_id] = go_term\n",
    "                    \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error fetching GO terms for {protein_id}: {e}\")\n",
    "                \n",
    "    return go_terms\n",
    "\n",
    "# Let's add some debugging to help understand what's happening\n",
    "# here we see that the big .xml file has the same structure as the small ones \n",
    "# we already analyzed ; thus,we can use the same parsing structure, but this time directly\n",
    "# just collect the counts of GO terms, because that is all we need (no diff. categories, would just make our code slower)\n",
    "def print_swissprot_file(swissprot_xml_path, length = 50):\n",
    "    \"\"\"\n",
    "    Just to look at the first few lines to see the structure\n",
    "    \"\"\"\n",
    "\n",
    "    with open(swissprot_xml_path, 'r') as f:\n",
    "        print(\"First length lines of the file:\")\n",
    "        for i, line in enumerate(f):\n",
    "            if i < length:\n",
    "                print(line.strip())\n",
    "            else:\n",
    "                break\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "def parse_swissprot_go_terms(swissprot_xml_path, family_proteins, skip_proteins):\n",
    "    \"\"\"\n",
    "    Parse GO terms from SwissProt XML file, excluding proteins from our family.\n",
    "    \n",
    "    Args:\n",
    "        swissprot_xml_path (str): Path to the SwissProt XML file\n",
    "        family_proteins (set): Set of UniProt IDs in our protein family\n",
    "        skip_proteins (bool): Whether to skip proteins in our family\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (go_term_counts dictionary, total proteins processed)\n",
    "    \"\"\"\n",
    "    # Initialize counters\n",
    "    go_term_counts = defaultdict(int)\n",
    "    total_proteins = 0\n",
    "    skipped_proteins = 0\n",
    "    \n",
    "    # Set up namespace for XML parsing\n",
    "    namespaces = {'ns': 'http://uniprot.org/uniprot'}\n",
    "    \n",
    "    # Use iterparse for memory-efficient parsing\n",
    "    context = ET.iterparse(swissprot_xml_path, events=('end',))\n",
    "    \n",
    "    print(\"Starting to parse SwissProt XML...\")\n",
    "    \n",
    "    for event, elem in context:\n",
    "        if elem.tag.endswith('entry'):\n",
    "            # Get the UniProt ID for this protein\n",
    "            accession = elem.find(\".//ns:accession\", namespaces)\n",
    "            if accession is not None:\n",
    "                uniprot_id = accession.text\n",
    "                \n",
    "                # Skip if this protein is in our family (we need this for the enrichment task to create the contigency table later on)\n",
    "    \n",
    "                if uniprot_id in family_proteins and skip_proteins:\n",
    "                        skipped_proteins += 1\n",
    "                else:\n",
    "                    # Process GO terms for non-family proteins\n",
    "                    for db_ref in elem.findall(\".//ns:dbReference[@type='GO']\", namespaces):\n",
    "                        go_id = db_ref.attrib.get('id')\n",
    "                        if go_id:\n",
    "                            go_term_counts[go_id] += 1\n",
    "                    total_proteins += 1\n",
    "        \n",
    "\n",
    "            \n",
    "            # Clear the element to save memory\n",
    "            elem.clear()\n",
    "            \n",
    "            # Print progress periodically\n",
    "            if (total_proteins + skipped_proteins) % 10000 == 0:\n",
    "                print(f\"Processed {total_proteins} proteins \"\n",
    "                      f\"(skipped {skipped_proteins} family proteins)...\")\n",
    "              #  break\n",
    "        \n",
    "\n",
    "    \n",
    "    return go_term_counts, total_proteins\n",
    "    '''\n",
    "\n",
    "def parse_swissprot_go_terms(swissprot_xml_path, family_proteins):\n",
    "   \"\"\"\n",
    "   Parse GO terms from SwissProt XML file for each protein.\n",
    "   \n",
    "   Args:\n",
    "       swissprot_xml_path (str): Path to SwissProt XML file\n",
    "       family_proteins (set): UniProt IDs in protein family\n",
    "   \n",
    "   Returns:\n",
    "       dict: protein ID -> list of GO IDs for that protein\n",
    "   \"\"\"\n",
    "   protein_to_go = defaultdict(list)\n",
    "   total_proteins = 0\n",
    "   skipped_proteins = 0\n",
    "   \n",
    "   namespaces = {'ns': 'http://uniprot.org/uniprot'}\n",
    "   context = ET.iterparse(swissprot_xml_path, events=('end',))\n",
    "   \n",
    "   print(\"Starting to parse SwissProt XML...\")\n",
    "   \n",
    "   for event, elem in context:\n",
    "       if elem.tag.endswith('entry'):\n",
    "           accession = elem.find(\".//ns:accession\", namespaces)\n",
    "           if accession is not None:\n",
    "               uniprot_id = accession.text\n",
    "               \n",
    "               if uniprot_id in family_proteins:\n",
    "                   skipped_proteins += 1\n",
    "               else:\n",
    "                   for db_ref in elem.findall(\".//ns:dbReference[@type='GO']\", namespaces):\n",
    "                       go_id = db_ref.attrib.get('id')\n",
    "                       if go_id:\n",
    "                           protein_to_go[uniprot_id].append(go_id)\n",
    "                   total_proteins += 1\n",
    "\n",
    "           elem.clear()\n",
    "           \n",
    "           if (total_proteins + skipped_proteins) % 10000 == 0:\n",
    "               print(f\"Processed {total_proteins} proteins \"\n",
    "                     f\"(skipped {skipped_proteins} family proteins)...\")\n",
    "               \n",
    "                    \n",
    "               \n",
    "   return protein_to_go\n",
    "\n",
    "def calculate_go_enrichment(go_to_proteins_family, go_to_proteins_swissprot, total_proteins_family, total_proteins_swissprot, go_id_to_go_term):\n",
    "    results = []\n",
    "    \n",
    "    \n",
    "    for go_id in go_to_proteins_family.keys():\n",
    "   \n",
    "        # Create the 2x2 contingency table for Fisher's exact test\n",
    "        # The table looks like this:\n",
    "        #                   Protein in family    Protein not in family (i.e. all in SwissProt - family proteins)\n",
    "        # Has GO term            a                    b\n",
    "        # No GO term             c                    d\n",
    "        \n",
    "        # Contingency table calculations:\n",
    "        a = len(go_to_proteins_family[go_id])  # Proteins with this GO term in family\n",
    "        \n",
    "        # For b, we need to make sure we don't subtract more than what's in SwissProt\n",
    "        b = len(go_to_proteins_swissprot.get(go_id, []))  # Proteins with GO term in rest of SwissProt \n",
    "        \n",
    "        c = total_proteins_family - a  # Proteins without GO term in family\n",
    "        \n",
    "        # For d, ensure we don't get negative values by using max\n",
    "        d = total_proteins_swissprot - b\n",
    "        \n",
    "        # Verify all values are non-negative before creating contingency table\n",
    "        if all(x >= 0 for x in [a, b, c, d]):\n",
    "            contingency_table = [[a, b], [c, d]]\n",
    "            \n",
    "            # Perform Fisher's exact test\n",
    "            # We ask : is the GO term appearing more often in our family than we would expect by random chance ?\n",
    "            # The null hypothesis (H0) is: \"The proportion of proteins with this GO term in our family \n",
    "            # is the same as the proportion in the SwissProt dataset (without the protein in the family).\" \n",
    "            # In other words, under H0, getting the GO term is independent of being in our family (so it doesn't represent the family)\n",
    "            # Alternative Hypothesis (H1) depends on what tail to use \n",
    "            #Right-tail (greater): Our family has a higher proportion of this GO term than SwissProt\n",
    "            #Left-tail (less): Our family has a lower proportion of this GO term than SwissProt\n",
    "            #Two-tail (two-sided): The proportion is different (either higher or lower)\n",
    "            #Fisher's exact test calculates the probability of seeing our observed data (or more extreme) under the null hypothesis.\n",
    "            #A very small p-value (like < 0.05) tells us:\n",
    "            #Two-tail: This GO term's frequency is significantly different from SwissProt\n",
    "            #Right-tail: This GO term is significantly enriched in our family(overrepresented)\n",
    "            #Left-tail: This GO term is significantly depleted in our family(underrepresented)\n",
    "\n",
    "            odds_ratio, pvalue_two_tail = fisher_exact(contingency_table, alternative='two-sided')\n",
    "            # TODO : including both the p-values for now, we have to understand when to use what (like asked in the task), \n",
    "            # TODO : i.e. how we ordered the confusion matrix (contingency table)\n",
    "            _, pvalue_greater = fisher_exact(contingency_table, alternative='greater')\n",
    "          #  _, pvalue_less = fisher_exact(contingency_table, alternative='less')\n",
    "            \n",
    "            # Calculate fold enrichment safely\n",
    "            my_proportion = a / total_proteins_family \n",
    "            swissprot_proportion = (a+b) / (total_proteins_swissprot + total_proteins_family)\n",
    "     \n",
    "            # Not needed anymore when we do a+b for the swissprot proportion : So we calculate contingency table and Fishers Test\n",
    "            # on the right values now, but the proportion on ALL swissprot, so also the ones that are in family TODO : ask prof if correct\n",
    "            '''\n",
    "            # Fold Enrichment\n",
    "            # TODO : see if the argumentation in the next comment makes sense (send email to prof)\n",
    "            if b == 0: # When the swissprot count is 0, it means that : \n",
    "                                     # When collecting the GO terms of SwissProt, we skipped over the proteins in our family\n",
    "                                     # Thus, if no protein in SwissProt has this GO term, ONLY the protein in the family itself \n",
    "                                     # has that GO term (compared to ALL of SwissProt), thus in the WordCloud later on\n",
    "                                     # we want to especially show the term of this GO id and will thus give it\n",
    "                                     # 'inf' amount (infinite) for now\n",
    "                if my_proportion > 0:\n",
    "                    fold_enrichment = float('inf')\n",
    "                else:\n",
    "                    fold_enrichment = 0\n",
    "            else:\n",
    "                fold_enrichment = my_proportion/swissprot_proportion\n",
    "            '''\n",
    "       \n",
    "     \n",
    "            \n",
    "            results.append({\n",
    "                'GO_ID': go_id,\n",
    "                'GO_Term': go_id_to_go_term.get(go_id, 'N/A'),\n",
    "                'Count_Prot_Dataset': a,\n",
    "                'Count_Prot_SwissProt': b,\n",
    "                'Count_Prot_SwissProt_Actual': a+b,\n",
    "                'Percentage_Dataset': round(my_proportion * 100, 2),\n",
    "                'Percentage_SwissProt': round(swissprot_proportion * 100, 10),\n",
    "                'Fold_Enrichment': round(my_proportion/swissprot_proportion,2),\n",
    "                'P_Value_Two_Tail': pvalue_two_tail,\n",
    "                'P_Value_Greater': pvalue_greater,\n",
    "            })\n",
    "    \n",
    "    # Convert to DataFrame and sort by p-value\n",
    "    df_results = pd.DataFrame(results)\n",
    "    if not df_results.empty:\n",
    "        df_results = df_results.sort_values('P_Value_Two_Tail')\n",
    "\n",
    "    df_results.to_csv(\"enrichment_results.csv\")\n",
    "    \n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to turn a Protein_ID : [GO_terms] into a GO_term : [Protein_IDs] dictionary\n",
    "\n",
    "def reverse_protein_go_dict(protein_to_go):\n",
    "   \"\"\"\n",
    "   Convert protein->GO dict to GO->proteins dict.\n",
    "   \"\"\"\n",
    "   go_to_proteins = defaultdict(list)\n",
    "   for protein, go_terms in protein_to_go.items():\n",
    "       for go_term in go_terms:\n",
    "           go_to_proteins[go_term].append(protein)\n",
    "   return go_to_proteins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MARLON \n",
    "# Hierarchical Structure\n",
    "\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from goatools import obo_parser\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def analyze_go_hierarchy():\n",
    "    # First, we downloaded the go.obo file so we can parse it \n",
    "    go_obo = obo_parser.GODag('go.obo')\n",
    "    \n",
    "    # Read our enrichment results\n",
    "    df = pd.read_csv(\"enrichment_results.csv\")\n",
    "    \n",
    "    # Filter for significantly enriched terms\n",
    "    enriched_terms = df[\n",
    "        (df['P_Value_Two_Tail'] < 0.05) &\n",
    "        (df['P_Value_Greater'] < 0.05)\n",
    "    ]\n",
    "    \n",
    "    # Create a dictionary to store branch information\n",
    "    branch_info = {}\n",
    "    \n",
    "    # For each enriched term, traverse up its ancestry\n",
    "    for _, row in enriched_terms.iterrows():\n",
    "        go_id = row['GO_ID']\n",
    "        if go_id in go_obo:\n",
    "            term = go_obo[go_id]\n",
    "            \n",
    "            # Get all ancestors (parents) up to the root of the DAG (since we use get_all_parents we do that here! get_parents would just get the direct parents)\n",
    "            ancestors = term.get_all_parents()\n",
    "            \n",
    "            # Add information about this term to all its ancestor branches\n",
    "            for ancestor_id in ancestors:\n",
    "                if ancestor_id not in branch_info:\n",
    "                    branch_info[ancestor_id] = {\n",
    "                        'term_name': go_obo[ancestor_id].name,\n",
    "                        'enriched_children': [],\n",
    "                        'total_significance': 0,\n",
    "                        'depth': go_obo[ancestor_id].depth,\n",
    "                    }\n",
    "\n",
    "                # TODO : correct ????\n",
    "                # Our go_id is a child to the current ancestors (note that this is not necessarily a direct child, but maybe also much more down in the tree somewhere)\n",
    "                branch_info[ancestor_id]['enriched_children'].append({\n",
    "                    'id': go_id,\n",
    "                    'name': term.name,\n",
    "                    'p_value': row['P_Value_Two_Tail']\n",
    "                })\n",
    "                # Measure significance based on -log value of the p value of all the childs of the ancestor (lower p values have higher -log scores!)\n",
    "                branch_info[ancestor_id]['total_significance'] += -np.log10(row['P_Value_Two_Tail'])\n",
    "    \n",
    "    # Filter for high-level terms (lower depth) with multiple enriched children\n",
    "    significant_branches = {\n",
    "        go_id: info for go_id, info in branch_info.items() # take each key,value of the branch_info dictionary\n",
    "        if len(info['enriched_children']) >= 2  # At least 2 enriched children\n",
    "        and info['depth'] <= 3  # High-level term (adjust this threshold as needed)\n",
    "    }\n",
    "    \n",
    "    # Sort branches by their total significance\n",
    "    sorted_branches = sorted(\n",
    "        significant_branches.items(),\n",
    "        key=lambda x: x[1]['total_significance'],\n",
    "        reverse=True\n",
    "    )\n",
    "    \n",
    "    # Create a list to store the branch information\n",
    "    branch_data = []\n",
    "\n",
    "    # Convert the branch information into a format suitable for a DataFrame\n",
    "    for go_id, info in sorted_branches[:20]:  # Top 20 branches\n",
    "        branch_data.append({\n",
    "            'GO_ID': go_id,\n",
    "            'Branch_Name': info['term_name'],\n",
    "            'Hierarchy_Depth': info['depth'],\n",
    "            'Number_Enriched_Terms': len(info['enriched_children']),\n",
    "            'Total_Significance_Score': info['total_significance']\n",
    "        })\n",
    "\n",
    "    # Create a DataFrame and save to CSV\n",
    "    branches_df = pd.DataFrame(branch_data)\n",
    "    branches_df.to_csv('enriched_branches.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go.obo: fmt(1.2) rel(2024-11-03) 43,983 Terms\n"
     ]
    }
   ],
   "source": [
    "# MARLON\n",
    "analyze_go_hierarchy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching GO annotations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching GO annotations: 100%|██████████| 57/57 [00:14<00:00,  3.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to parse SwissProt XML...\n",
      "Processed 10000 proteins (skipped 0 family proteins)...\n",
      "Processed 20000 proteins (skipped 0 family proteins)...\n",
      "Processed 29985 proteins (skipped 15 family proteins)...\n",
      "Processed 39985 proteins (skipped 15 family proteins)...\n",
      "Processed 49985 proteins (skipped 15 family proteins)...\n",
      "Processed 59985 proteins (skipped 15 family proteins)...\n",
      "Processed 69985 proteins (skipped 15 family proteins)...\n",
      "Processed 79985 proteins (skipped 15 family proteins)...\n",
      "Processed 89974 proteins (skipped 26 family proteins)...\n",
      "Processed 99974 proteins (skipped 26 family proteins)...\n",
      "Processed 109974 proteins (skipped 26 family proteins)...\n",
      "Processed 119974 proteins (skipped 26 family proteins)...\n",
      "Processed 129974 proteins (skipped 26 family proteins)...\n",
      "Processed 139974 proteins (skipped 26 family proteins)...\n",
      "Processed 149974 proteins (skipped 26 family proteins)...\n",
      "Processed 159974 proteins (skipped 26 family proteins)...\n",
      "Processed 169974 proteins (skipped 26 family proteins)...\n",
      "Processed 179974 proteins (skipped 26 family proteins)...\n",
      "Processed 189974 proteins (skipped 26 family proteins)...\n",
      "Processed 199969 proteins (skipped 31 family proteins)...\n",
      "Processed 209969 proteins (skipped 31 family proteins)...\n",
      "Processed 219969 proteins (skipped 31 family proteins)...\n",
      "Processed 229968 proteins (skipped 32 family proteins)...\n",
      "Processed 239968 proteins (skipped 32 family proteins)...\n",
      "Processed 249968 proteins (skipped 32 family proteins)...\n",
      "Processed 259958 proteins (skipped 42 family proteins)...\n",
      "Processed 269955 proteins (skipped 45 family proteins)...\n",
      "Processed 279955 proteins (skipped 45 family proteins)...\n",
      "Processed 289955 proteins (skipped 45 family proteins)...\n",
      "Processed 299955 proteins (skipped 45 family proteins)...\n",
      "Processed 309949 proteins (skipped 51 family proteins)...\n",
      "Processed 319949 proteins (skipped 51 family proteins)...\n",
      "Processed 329949 proteins (skipped 51 family proteins)...\n",
      "Processed 339949 proteins (skipped 51 family proteins)...\n",
      "Processed 349949 proteins (skipped 51 family proteins)...\n",
      "Processed 359949 proteins (skipped 51 family proteins)...\n",
      "Processed 369949 proteins (skipped 51 family proteins)...\n",
      "Processed 379949 proteins (skipped 51 family proteins)...\n",
      "Processed 389949 proteins (skipped 51 family proteins)...\n",
      "Processed 399949 proteins (skipped 51 family proteins)...\n",
      "Processed 409949 proteins (skipped 51 family proteins)...\n",
      "Processed 419949 proteins (skipped 51 family proteins)...\n",
      "Processed 429949 proteins (skipped 51 family proteins)...\n",
      "Processed 439949 proteins (skipped 51 family proteins)...\n",
      "Processed 449949 proteins (skipped 51 family proteins)...\n",
      "Processed 459949 proteins (skipped 51 family proteins)...\n",
      "Processed 469949 proteins (skipped 51 family proteins)...\n",
      "Processed 479949 proteins (skipped 51 family proteins)...\n",
      "Processed 489946 proteins (skipped 54 family proteins)...\n",
      "Processed 499946 proteins (skipped 54 family proteins)...\n",
      "Processed 509946 proteins (skipped 54 family proteins)...\n",
      "Processed 519946 proteins (skipped 54 family proteins)...\n",
      "Processed 529946 proteins (skipped 54 family proteins)...\n",
      "Processed 539946 proteins (skipped 54 family proteins)...\n",
      "Processed 549943 proteins (skipped 57 family proteins)...\n",
      "Processed 559943 proteins (skipped 57 family proteins)...\n",
      "Processed 569943 proteins (skipped 57 family proteins)...\n",
      "\n",
      "Top enriched GO terms:\n",
      "\n",
      "Term: P:galactolipid catabolic process\n",
      "Weight in word cloud: 9677.81\n",
      "\n",
      "Term: P:positive regulation of triglyceride lipase activity\n",
      "Weight in word cloud: 9677.81\n",
      "\n",
      "Term: P:intermediate-density lipoprotein particle remodeling\n",
      "Weight in word cloud: 9677.81\n",
      "\n",
      "Term: P:chylomicron remodeling\n",
      "Weight in word cloud: 8188.91\n",
      "\n",
      "Term: P:intestinal lipid catabolic process\n",
      "Weight in word cloud: 7258.36\n",
      "\n",
      "Term: F:lipoprotein lipase activity\n",
      "Weight in word cloud: 7168.75\n",
      "\n",
      "Term: F:1-18:1-2-16:0-monogalactosyldiacylglycerol lipase activity\n",
      "Weight in word cloud: 6158.60\n",
      "\n",
      "Term: P:low-density lipoprotein particle mediated signaling\n",
      "Weight in word cloud: 5278.80\n",
      "\n",
      "Term: P:regulation of lipoprotein metabolic process\n",
      "Weight in word cloud: 4838.90\n",
      "\n",
      "Term: F:galactolipase activity\n",
      "Weight in word cloud: 3984.98\n"
     ]
    }
   ],
   "source": [
    "#MARLON \n",
    "def main():\n",
    "\n",
    "    psiblast_file = \"psiblast_parsed.csv\"\n",
    "    hmm_file = \"hmmsearch_output.csv\"\n",
    "    protein_ids = load_protein_ids(psiblast_file, hmm_file)\n",
    "    \n",
    "\n",
    "    \n",
    "    # Proteins_to_GO terms for our family \n",
    "    print(\"Fetching GO annotations...\")\n",
    "    family_annotations = {}\n",
    "    for pid in tqdm(protein_ids, desc=\"Fetching GO annotations\"):\n",
    "        family_annotations[pid] = fetch_go_annotations(pid)\n",
    "\n",
    "    total_proteins_family = len(family_annotations)\n",
    "\n",
    "    go_id_to_go_term = fetch_go_terms(protein_ids)\n",
    "\n",
    "    \n",
    "    # Proteins_to_GO terms for SwissProt\n",
    "    swissprot_annotations = parse_swissprot_go_terms(\"uniprot_sprot.xml\", protein_ids) #go_counts_swissprot, num_proteins_swissprot\n",
    "    \n",
    "    total_proteins_swissprot = len(swissprot_annotations)\n",
    "\n",
    "    # Now Map the GO terms to the proteins ; for the enrichment task, we need to know how many proteins have a certain GO term\n",
    "    go_to_proteins_swissprot = reverse_protein_go_dict(swissprot_annotations)\n",
    "    go_to_proteins_family = reverse_protein_go_dict(family_annotations)\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    # Calculate GO enrichments for both with skipped proteins and without \n",
    "    _ = calculate_go_enrichment(go_to_proteins_family, go_to_proteins_swissprot, total_proteins_family, total_proteins_swissprot, go_id_to_go_term)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Read the enrichment results\n",
    "    df = pd.read_csv(\"enrichment_results.csv\")\n",
    "\n",
    "    # Get the terms to the GO ids from the family data\n",
    "  #  go_id_to_term = create_go_id_to_term_mapping(family_annotations)\n",
    "\n",
    "    # Filter for significantly enriched terms\n",
    "    enriched_terms = df[\n",
    "    (df['P_Value_Two_Tail'] < 0.05) &\n",
    "    (df['P_Value_Greater'] < 0.05)\n",
    "    ]\n",
    "\n",
    "\n",
    "    # Create word frequencies using the actual GO terms instead of IDs\n",
    "    word_frequencies = {}\n",
    "    for _, row in enriched_terms.iterrows():\n",
    "        go_id = row['GO_ID']\n",
    "        if go_id in go_id_to_go_term:  # Make sure we have the term for this ID\n",
    "            term = go_id_to_go_term[go_id]\n",
    "            # Use fold enrichment as weight\n",
    "            weight = row['Fold_Enrichment']\n",
    "            word_frequencies[term] = weight\n",
    "\n",
    "    # Create and display the word cloud\n",
    "    wordcloud = WordCloud(\n",
    "        width=1200, \n",
    "        height=800,\n",
    "        background_color='white',\n",
    "        prefer_horizontal=0.7,\n",
    "        max_words=50,  # Limit to top 50 terms for better readability\n",
    "        min_font_size=10,\n",
    "        max_font_size=60\n",
    "    ).generate_from_frequencies(word_frequencies)\n",
    "\n",
    "    # Plot and save the word cloud\n",
    "    plt.figure(figsize=(20, 12))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title('GO Term Enrichment Word Cloud', fontsize=16, pad=20)\n",
    "    plt.savefig('go_enrichment_wordcloud.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    # Print out the enriched terms for verification\n",
    "    print(\"\\nTop enriched GO terms:\")\n",
    "    sorted_terms = sorted(word_frequencies.items(), key=lambda x: x[1], reverse=True)\n",
    "    for term, weight in sorted_terms[:10]:\n",
    "        print(f\"\\nTerm: {term}\")\n",
    "        print(f\"Weight in word cloud: {weight:.2f}\")\n",
    "\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GO ontology...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 312\u001b[0m\n\u001b[1;32m    309\u001b[0m     write_summary_and_results(enrichment_results, family_annotations, go_graph)\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 312\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[48], line 258\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m():\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading GO ontology...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 258\u001b[0m     go_graph \u001b[38;5;241m=\u001b[39m \u001b[43mload_go_ontology\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    260\u001b[0m     psiblast_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpsiblast_parsed.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    261\u001b[0m     hmm_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhmmsearch_output.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[0;32mIn[48], line 126\u001b[0m, in \u001b[0;36mload_go_ontology\u001b[0;34m()\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load the GO ontology from OBO file.\"\"\"\u001b[39;00m\n\u001b[1;32m    125\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp://purl.obolibrary.org/obo/go/go-basic.obo\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 126\u001b[0m graph \u001b[38;5;241m=\u001b[39m \u001b[43mobonet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_obo\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m graph\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/obonet/read.py:37\u001b[0m, in \u001b[0;36mread_obo\u001b[0;34m(path_or_file, ignore_obsolete, encoding)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_obo\u001b[39m(\n\u001b[1;32m     16\u001b[0m     path_or_file: PathType, ignore_obsolete: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, encoding: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     17\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m networkx\u001b[38;5;241m.\u001b[39mMultiDiGraph[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m     18\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;03m    Return a networkx.MultiDiGraph of the ontology serialized by the\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124;03m    specified path or file.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;124;03m        is a path/URL. Set to None for platform-dependent locale default.\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mopen_read_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m obo_file:\n\u001b[1;32m     38\u001b[0m         typedefs, terms, instances, header \u001b[38;5;241m=\u001b[39m get_sections(obo_file)\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124montology\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m header:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/obonet/io.py:37\u001b[0m, in \u001b[0;36mopen_read_file\u001b[0;34m(path, encoding)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m re\u001b[38;5;241m.\u001b[39mmatch(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m^(http|ftp)s?://\u001b[39m\u001b[38;5;124m\"\u001b[39m, path):\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m urlopen(path) \u001b[38;5;28;01mas\u001b[39;00m response:\n\u001b[0;32m---> 37\u001b[0m         content \u001b[38;5;241m=\u001b[39m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m opener \u001b[38;5;241m==\u001b[39m io\u001b[38;5;241m.\u001b[39mopen:\n\u001b[1;32m     39\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m encoding:\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.8/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py:489\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    488\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 489\u001b[0m         s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_safe_read\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlength\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    490\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m IncompleteRead:\n\u001b[1;32m    491\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.8/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py:638\u001b[0m, in \u001b[0;36mHTTPResponse._safe_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_safe_read\u001b[39m(\u001b[38;5;28mself\u001b[39m, amt):\n\u001b[1;32m    632\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Read the number of bytes requested.\u001b[39;00m\n\u001b[1;32m    633\u001b[0m \n\u001b[1;32m    634\u001b[0m \u001b[38;5;124;03m    This function should be used when <amt> bytes \"should\" be present for\u001b[39;00m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;124;03m    reading. If the bytes are truly not available (due to EOF), then the\u001b[39;00m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;124;03m    IncompleteRead exception can be used to detect the problem.\u001b[39;00m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 638\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    639\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m<\u001b[39m amt:\n\u001b[1;32m    640\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m IncompleteRead(data, amt\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mlen\u001b[39m(data))\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.8/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "from scipy.stats import fisher_exact\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import obonet\n",
    "import networkx as nx\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "# Step 1: Load Protein IDs\n",
    "# TODO : we basically did this above already for taxonomy task and just here neatly written into a function, so we could maybe just do it once in the whole code later on\n",
    "def load_protein_ids(psiblast_file, hmm_file, e_threshold=0.001):\n",
    "    \"\"\"Load protein IDs from PSI-BLAST and HMM search results.\"\"\"\n",
    "    psiblast_df = pd.read_csv(psiblast_file)\n",
    "    hmm_df = pd.read_csv(hmm_file)\n",
    "    \n",
    "    filtered_hmm_proteins = hmm_df[hmm_df['E-value'] <= e_threshold]['uniprot_id']\n",
    "    psiblast_proteins = set(psiblast_df['uniprot_id'])\n",
    "    hmm_proteins = set(filtered_hmm_proteins)\n",
    "    \n",
    "    return list(psiblast_proteins.union(hmm_proteins))\n",
    "\n",
    "\n",
    "def fetch_go_annotations(protein_id):\n",
    "    \"\"\"\n",
    "    Fetch and categorize GO annotations for a given protein ID from the UniProt API.\n",
    "    \n",
    "    Args:\n",
    "        protein_id (str): The UniProt ID of the protein\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary containing:\n",
    "            - Categorized GO terms separated by molecular function, biological process, \n",
    "              and cellular component (new format)\n",
    "    \"\"\"\n",
    "    # Define the UniProt API URL for XML data\n",
    "    url = f\"https://rest.uniprot.org/uniprotkb/{protein_id}.xml\"\n",
    "    \n",
    "    try:\n",
    "        # Fetch the XML data from UniProt\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Initialize our data structures\n",
    "        go_terms = []  # Original format\n",
    "        categorized_terms = {\n",
    "            'molecular_function': [],\n",
    "            'biological_process': [],\n",
    "            'cellular_component': []\n",
    "        }\n",
    "        \n",
    "        # Set up namespace for XML parsing\n",
    "        namespaces = {'ns': 'http://uniprot.org/uniprot'}\n",
    "        root = ET.fromstring(response.content)\n",
    "        \n",
    "        # Find all GO term references in the XML\n",
    "        for db_ref in root.findall(\".//ns:dbReference[@type='GO']\", namespaces):\n",
    "            go_id = db_ref.attrib.get('id')\n",
    "            term = db_ref.find(\"ns:property[@type='term']\", namespaces)\n",
    "            category = db_ref.find(\"ns:property[@type='category']\", namespaces)\n",
    "            \n",
    "            if go_id and term is not None:\n",
    "                # Store in original format\n",
    "                term_value = term.attrib['value']\n",
    "\n",
    "                \n",
    "                # Categorize based on prefix\n",
    "                if term_value.startswith('F:'):\n",
    "                    categorized_terms['molecular_function'].append({\n",
    "                        'id': go_id,\n",
    "                        'term': term_value[2:]  # Remove 'F:' prefix\n",
    "                    })\n",
    "                elif term_value.startswith('P:'):\n",
    "                    categorized_terms['biological_process'].append({\n",
    "                        'id': go_id,\n",
    "                        'term': term_value[2:]  # Remove 'P:' prefix\n",
    "                    })\n",
    "                elif term_value.startswith('C:'):\n",
    "                    categorized_terms['cellular_component'].append({\n",
    "                        'id': go_id,\n",
    "                        'term': term_value[2:]  # Remove 'C:' prefix\n",
    "                    })\n",
    "        \n",
    "        return {\n",
    "            'categorized': categorized_terms  # New categorized format\n",
    "        }\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching GO annotations for {protein_id}: {e}\")\n",
    "        return {\n",
    "            'categorized': {\n",
    "                'molecular_function': [],\n",
    "                'biological_process': [],\n",
    "                'cellular_component': []\n",
    "            }\n",
    "        }\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Step 3: Fetch Random Proteins\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def fetch_random_proteins(batch_size=100, total_proteins=500):\n",
    "    \"\"\"Fetch a list of random reviewed UniProt protein IDs.\"\"\"\n",
    "    url = \"https://rest.uniprot.org/uniprotkb/stream?query=reviewed:true&format=list\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        all_proteins = response.text.splitlines()\n",
    "        selected_proteins = random.sample(all_proteins, min(total_proteins, len(all_proteins)))\n",
    "        return [selected_proteins[i:i + batch_size] for i in range(0, len(selected_proteins), batch_size)]\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching random proteins: {e}\")\n",
    "        return []\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Step 4: Load GO Ontology\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def load_go_ontology():\n",
    "    \"\"\"Load the GO ontology from OBO file.\"\"\"\n",
    "    url = \"http://purl.obolibrary.org/obo/go/go-basic.obo\"\n",
    "    graph = obonet.read_obo(url)\n",
    "    return graph\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Step 5: Flatten Annotations for Analysis\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def flatten_annotations(annotation_dict):\n",
    "    \"\"\"Flatten GO annotations into a list of GO terms.\"\"\"\n",
    "    flat_terms = []\n",
    "    for annotations in annotation_dict.values():\n",
    "        flat_terms.extend([a[\"GO_ID\"] for a in annotations])\n",
    "    return flat_terms\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Step 6: Enrichment Analysis\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def calculate_enrichment(go_term, family_terms, background_terms):\n",
    "    \"\"\"Calculate enrichment of a GO term using Fisher's exact test.\"\"\"\n",
    "    family_count = family_terms.count(go_term)\n",
    "    family_not = len(family_terms) - family_count\n",
    "    background_count = background_terms.count(go_term)\n",
    "    background_not = len(background_terms) - background_count\n",
    "\n",
    "    contingency_table = [[family_count, background_count],\n",
    "                         [family_not, background_not]]\n",
    "    _, p_value = fisher_exact(contingency_table, alternative='greater')\n",
    "    return p_value\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Step 7: Visualize Enriched Terms\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def plot_wordcloud(enrichment_results, annotations):\n",
    "    \"\"\"Generate and save a word cloud of enriched GO terms.\"\"\"\n",
    "    term_names = {a[\"GO_ID\"]: a[\"Term\"] for ann_list in annotations.values() for a in ann_list}\n",
    "    enriched_with_names = {term_names[go_id]: -np.log10(p) for go_id, p in enrichment_results.items() if go_id in term_names}\n",
    "\n",
    "    if not enriched_with_names:\n",
    "        print(\"No enriched terms found. Word cloud will not be generated.\")\n",
    "        return\n",
    "\n",
    "    wordcloud = WordCloud(width=1000, height=600, background_color=\"white\", colormap=\"viridis\").generate_from_frequencies(enriched_with_names)\n",
    "    wordcloud.to_file(\"enriched_terms_wordcloud.png\")\n",
    "    print(\"Word cloud saved as enriched_terms_wordcloud.png\")\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Word Cloud of Enriched GO Terms\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_branch_enrichment(enrichment_results, go_graph):\n",
    "    \"\"\"Plot GO branch enrichment.\"\"\"\n",
    "    branch_scores = {}\n",
    "    for go_id, p_value in enrichment_results.items():\n",
    "        try:\n",
    "            parents = nx.ancestors(go_graph, go_id)\n",
    "            for parent in parents:\n",
    "                if parent not in branch_scores:\n",
    "                    branch_scores[parent] = []\n",
    "                branch_scores[parent].append(p_value)\n",
    "        except nx.NetworkXError:\n",
    "            continue\n",
    "\n",
    "    significant_branches = {branch: np.mean(scores)\n",
    "                            for branch, scores in branch_scores.items() if len(scores) >= 3}\n",
    "\n",
    "    # Limit to top 50 branches by score\n",
    "    sorted_branches = sorted(significant_branches.items(), key=lambda x: x[1], reverse=True)[:50]\n",
    "    branches, scores = zip(*sorted_branches)\n",
    "\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    plt.barh(range(len(branches)), scores, color=\"steelblue\")\n",
    "    plt.yticks(range(len(branches)), [go_graph.nodes[branch]['name'] for branch in branches], fontsize=8)\n",
    "    plt.xlabel('Mean p-value')\n",
    "    plt.title('Top 50 GO Branch Enrichments')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"go_enrichment_branches.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.legend([\"Top 50 Branch Enrichments\"], loc=\"lower right\")\n",
    "    print(\"GO branch enrichment plot saved as go_enrichment_branches.png\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Step 8: Write Summary and Results to File\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def write_summary_and_results(enrichment_results, family_annotations, go_graph):\n",
    "    \"\"\"Write a summary and detailed results to a text file.\"\"\"\n",
    "    with open(\"enrichment_results.txt\", \"w\") as f:\n",
    "        # Write summary\n",
    "        f.write(\"SUMMARY\\n\")\n",
    "        f.write(\"========\\n\")\n",
    "        f.write(f\"Number of enriched GO terms: {len(enrichment_results)}\\n\")\n",
    "        f.write(f\"Top enriched term: {max(enrichment_results, key=enrichment_results.get, default='None')}\\n\")\n",
    "        f.write(\"\\n\\n\")\n",
    "\n",
    "        # Write detailed results\n",
    "        f.write(\"DETAILED RESULTS\\n\")\n",
    "        f.write(\"================\\n\")\n",
    "        for go_id, p_value in enrichment_results.items():\n",
    "            term_name = next((a[\"Term\"] for ann_list in family_annotations.values() \n",
    "                             for a in ann_list if a[\"GO_ID\"] == go_id), go_id)\n",
    "            f.write(f\"{go_id}: {term_name} (p-value: {p_value:.2e})\\n\")\n",
    "\n",
    "        # Write branch scores\n",
    "        f.write(\"\\n\\nSIGNIFICANT GO BRANCHES\\n\")\n",
    "        f.write(\"========================\\n\")\n",
    "        branch_scores = {}\n",
    "        for go_id, p_value in enrichment_results.items():\n",
    "            try:\n",
    "                parents = nx.ancestors(go_graph, go_id)\n",
    "                for parent in parents:\n",
    "                    if parent not in branch_scores:\n",
    "                        branch_scores[parent] = []\n",
    "                    branch_scores[parent].append(p_value)\n",
    "            except nx.NetworkXError:\n",
    "                continue\n",
    "\n",
    "        significant_branches = {branch: np.mean(scores) \n",
    "                                for branch, scores in branch_scores.items() if len(scores) >= 3}\n",
    "        for branch, score in sorted(significant_branches.items(), key=lambda x: x[1]):\n",
    "            branch_name = go_graph.nodes[branch].get('name', branch)\n",
    "            f.write(f\"{branch}: {branch_name} (mean p-value: {score:.2e})\\n\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Main Script\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def main():\n",
    "    print(\"Loading GO ontology...\")\n",
    "    go_graph = load_go_ontology()\n",
    "\n",
    "    psiblast_file = \"psiblast_parsed.csv\"\n",
    "    hmm_file = \"hmmsearch_output.csv\"\n",
    "    protein_ids = load_protein_ids(psiblast_file, hmm_file)\n",
    "\n",
    "    print(\"Fetching GO annotations...\")\n",
    "    family_annotations = {}\n",
    "    for pid in tqdm(protein_ids, desc=\"Fetching GO annotations\"):\n",
    "        family_annotations[pid] = fetch_go_annotations(pid)\n",
    "\n",
    "\n",
    "    print(family_annotations)\n",
    "\n",
    "\n",
    "    print(\"Fetching background annotations...\")\n",
    "    background_annotations = {}\n",
    "    background_batches = fetch_random_proteins(batch_size=50, total_proteins=500)\n",
    "    for batch in tqdm(background_batches, desc=\"Processing background proteins\"):\n",
    "        for pid in batch:\n",
    "            background_annotations[pid] = fetch_go_annotations(pid)\n",
    "\n",
    "    print(\"Calculating enrichment...\")\n",
    "    family_terms = flatten_annotations(family_annotations)\n",
    "    background_terms = flatten_annotations(background_annotations)\n",
    "\n",
    "    unique_go_terms = set(family_terms)\n",
    "    enrichment_results = {}\n",
    "    pvalues = []\n",
    "    terms = []\n",
    "\n",
    "    for term in unique_go_terms:\n",
    "        _, p_value = fisher_exact([\n",
    "            [family_terms.count(term), len(family_terms) - family_terms.count(term)],\n",
    "            [background_terms.count(term), len(background_terms) - background_terms.count(term)]\n",
    "        ], alternative='greater')\n",
    "\n",
    "        pvalues.append(p_value)\n",
    "        terms.append(term)\n",
    "\n",
    "    rejected, p_corrected, _, _ = multipletests(pvalues, method='fdr_bh')\n",
    "\n",
    "    for term, p_value, significant in zip(terms, p_corrected, rejected):\n",
    "        if significant:\n",
    "            enrichment_results[term] = p_value\n",
    "\n",
    "    print(\"Generating visualizations...\")\n",
    "    plot_wordcloud(enrichment_results, family_annotations)\n",
    "    plot_branch_enrichment(enrichment_results, go_graph)\n",
    "\n",
    "    print(\"Writing results to file...\")\n",
    "    write_summary_and_results(enrichment_results, family_annotations, go_graph)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motifs\n",
    "1. Search significantly conserved short motifs inside your family. Use ELM classes and ProSite patterns (for ProSite consider only patterns “PA” lines, not the profiles). Make sure to consider as true matches only those that are found inside disordered regions. Disordered regions for the entire SwissProt (as defined by MobiDB-lite) are available here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
