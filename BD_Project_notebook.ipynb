{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biological Data Project\n",
    "\n",
    "Group members:\n",
    "\n",
    "- Alberto Calabrese\n",
    "\n",
    "- Marlon Helbing\n",
    "\n",
    "- Lorenzo Baietti"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"A protein domain is a conserved part of a given protein sequence and tertiary structure that can evolve, function, and exist independently of the rest of the protein chain. Each domain forms a compact three-dimensional structure and often can be independently stable and folded.\" (Wikipedia)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The project is about the characterization of a single domain. Each group is provided with a representative domain sequence and the corresponding Pfam identifier (see table below). The objective of the project is to build a sequence model starting from the assigned sequence and to provide a functional characterization of the entire domain family (homologous proteins)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input\n",
    "A representative sequence of the domain family. Columns are: group, UniProt accession, organism, Pfam identifier, Pfam name, domain position in the corresponding UniProt protein, domain sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "UniProt : P54315 \n",
    "PfamID : PF00151 \n",
    "Domain Position : 18-353 \n",
    "Organism : Homo sapiens (Human) \n",
    "Pfam Name : Lipase/vitellogenin \n",
    "Domain Sequence : KEVCYEDLGCFSDTEPWGGTAIRPLKILPWSPEKIGTRFLLYTNENPNNFQILLLSDPSTIEASNFQMDRKTRFIIHGFIDKGDESWVTDMCKKLFEVEEVNCICVDWKKGSQATYTQAANNVRVVGAQVAQMLDILLTEYSYPPSKVHLIGHSLGAHVAGEAGSKTPGLSRITGLDPVEASFESTPEEVRLDPSDADFVDVIHTDAAPLIPFLGFGTNQQMGHLDFFPNGGESMPGCKKNALSQIVDLDGIWAGTRDFVACNHLRSYKYYLESILNPDGFAAYPCTSYKSFESDKCFPCPDQGCPQMGHYADKFAGRTSEEQQKFFLNTGEASNF\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Domain model definition\n",
    "The objective of the first part of the project is to build a PSSM and HMM model representing the assigned domain. The two models will be generated starting from the assigned input sequence. The accuracy of the models will be evaluated against Pfam annotations as provided in the SwissProt database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Building \n",
    "1. Retrieve homologous proteins starting from your input sequence performing a BLAST search\n",
    "against UniProt or UniRef50 or UniRef90, or any other database\n",
    "\n",
    "- We use https://www.uniprot.org/blast \n",
    "    - use the Domain Sequence as Input\n",
    "    Parameters : \n",
    "    - against UniProtKB\n",
    "    - e-value thresh : 0.0001\n",
    "    - 1000 hits\n",
    "\n",
    "- results in 1000 hits \n",
    "- do ID matching to UniProtKB (we need to do that to download the .fasta file)\n",
    "-  results in 'UNIPROTKB_INITIAL_ORIGINAL.FASTA' \n",
    "\n",
    "2. Generate a multiple sequence alignment (MSA) starting from retrieved hits using T-coffee or\n",
    "ClustalOmega or MUSCLE\n",
    "    - We use https://www.ebi.ac.uk/jdispatcher/msa/clustalo\n",
    "    Parameters :\n",
    "    - Output Format : FASTA\n",
    "\n",
    "- results in ClustalOmegaUniPortAlignment_ORIGINAL.fasta\n",
    "\n",
    "3. If necessary, edit the MSA with JalView (or with your custom script or CD-HIT) to remove not\n",
    "conserved positions (columns) and/or redundant information (rows)\n",
    "    - We first used JalView at a 100% threshold to check for redundant rows\n",
    "\n",
    "- results in ClustalOmegaUniPortAlignment.fasta\n",
    "    - With this we remove down to 155 sequences (check again if correct number)\n",
    "    - Then we utilize 'conservation.py' to remove columns based on gap frequency (right now only that, lets check if the model is good enough and then we can clean up the code)\n",
    "    - Right now gap_threshold at 0.90, which removes around 70% of the initial columns (again, need to check actual numbers) ; we should experiment with conservation_threshold and name it in the paper, I think it adds a nice touch that he sees we tried it\n",
    "\n",
    "- results in trimmed_alignment.fasta\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import AlignIO\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from scipy.stats import entropy\n",
    "import math\n",
    "from Bio.Align import MultipleSeqAlignment\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "from Bio.Seq import Seq\n",
    "from Bio import SeqIO\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConservationAnalyzer:\n",
    "    def __init__(self, alignment_file):\n",
    "        \"\"\"\n",
    "        Initialize with an alignment file\n",
    "            alignment_file (str): Path to the alignment file\n",
    "        \"\"\"\n",
    "        self.alignment = AlignIO.read(alignment_file, 'fasta')\n",
    "        self.num_sequences = len(self.alignment)\n",
    "        self.alignment_length = self.alignment.get_alignment_length()\n",
    "        \n",
    "    def get_column(self, pos):\n",
    "        \"\"\"Extract a column from the alignment\"\"\"\n",
    "        return [record.seq[pos] for record in self.alignment]\n",
    "    \n",
    "    def calculate_gap_frequency(self, pos):\n",
    "        \"\"\"Calculate frequency of gaps in a column\"\"\"\n",
    "        column = self.get_column(pos)\n",
    "        return column.count('-') / len(column)\n",
    "    \n",
    "    def calculate_amino_acid_frequencies(self, pos):\n",
    "        \"\"\"Calculate frequencies of each amino acid in a column\"\"\"\n",
    "        column = self.get_column(pos)\n",
    "        total = len(column) - column.count('-')  # Don't count gaps, such that when we calculate conservation scores the gaps don't mess it up \n",
    "        if total == 0:\n",
    "            return {}\n",
    "        \n",
    "        counts = Counter(aa for aa in column if aa != '-')\n",
    "        return {aa: count/total for aa, count in counts.items()}\n",
    "    \n",
    "    def calculate_conservation_score(self, pos):\n",
    "        \"\"\"\n",
    "        Calculate conservation score based on frequency of most common amino acid\n",
    "        Ignores gaps in calculation\n",
    "        \"\"\"\n",
    "        freqs = self.calculate_amino_acid_frequencies(pos)\n",
    "        if not freqs:\n",
    "            return 0\n",
    "        return max(freqs.values())\n",
    "    \n",
    "    def calculate_entropy(self, pos):\n",
    "        \"\"\"\n",
    "        Calculate Shannon entropy for a column\n",
    "        Lower entropy means higher conservation\n",
    "        \"\"\"\n",
    "        freqs = self.calculate_amino_acid_frequencies(pos)\n",
    "        if not freqs:\n",
    "            return float('inf')  \n",
    "        \n",
    "        return -sum(p * math.log2(p) for p in freqs.values())\n",
    "    \n",
    "    def get_amino_acid_groups(self):\n",
    "        \"\"\"Define groups of similar amino acids \n",
    "           Based on : https://en.wikipedia.org/wiki/Conservative_replacement#:~:text=There%20are%2020%20naturally%20occurring,both%20small%2C%20negatively%20charged%20residues.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'aliphatic': set('GAVLI'),\n",
    "            'hydroxyl': set('SCUTM'),\n",
    "            'cyclic': set('P'),\n",
    "            'aromatic': set('FYW'),\n",
    "            'basic': set('HKR'),\n",
    "            'acidic': set('DENQ')\n",
    "        }\n",
    "    \n",
    "    def calculate_group_conservation(self, pos):\n",
    "        \"\"\"\n",
    "        Calculate conservation considering amino acid groups\n",
    "        Basically the same as calculate_conversation_score, just that it calculates based on the groups, not single amino acids !\n",
    "        \"\"\"\n",
    "        column = self.get_column(pos)\n",
    "        groups = self.get_amino_acid_groups()\n",
    "        \n",
    "        # Assign each amino acid to its group\n",
    "        aa_to_group = {}\n",
    "        for group_name, aas in groups.items():\n",
    "            for aa in aas:\n",
    "                aa_to_group[aa] = group_name\n",
    "        \n",
    "        # Count group occurrences\n",
    "        group_counts = Counter(aa_to_group.get(aa, 'other') \n",
    "                             for aa in column if aa != '-')\n",
    "        \n",
    "        if not group_counts:\n",
    "            return 0\n",
    "            \n",
    "        return max(group_counts.values()) / sum(group_counts.values())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # TODO : I took very strict values now such that the number of residues per sequence is below 100 (right now we have length 77) ; the PSSM creation with \n",
    "    # much higher length did not work, but maybe we should write an email and ask ; nevertheless, we can first try some evaluation based on that PSSM and see our scores\n",
    "\n",
    "    # TODO : diff gap_thresh/conservation threshold for different number of columns in output (OPTIMIZE)\n",
    "    def analyze_columns(self, gap_threshold=0.90, conservation_threshold=0.9):\n",
    "        \"\"\"\n",
    "        Analyze all columns and return comprehensive metrics\n",
    "        Returns DataFrame with various conservation metrics for each position\n",
    "        \"\"\"\n",
    "        data = []\n",
    "        \n",
    "        for i in range(self.alignment_length):\n",
    "            gap_freq = self.calculate_gap_frequency(i)\n",
    "            cons_score = self.calculate_conservation_score(i)\n",
    "            info_content = self.calculate_entropy(i)\n",
    "            group_cons = self.calculate_group_conservation(i)\n",
    "            \n",
    "            data.append({\n",
    "                'position': i + 1,\n",
    "                'gap_frequency': gap_freq,\n",
    "                'single_conservation': cons_score,\n",
    "                'entropy': info_content,\n",
    "                'group_conservation': group_cons,\n",
    "                # Here we should look possibly for better ideas\n",
    "                # Check gap frequency not too high (i.e. not nearly all elements in the columns gaps (-))\n",
    "                # Check that the group conservation is high enough (i.e. the amino acids are not too different\n",
    "                # ; right now we do with groups and not single amino acid sequence since I'd say the groups\n",
    "                # are more representative (if we do single amino acids, we'd delete more stuff))\n",
    "                'suggested_remove': (gap_freq > gap_threshold) #or       \n",
    "                                 #  group_cons < conservation_threshold) # TODO : OPTIMIZE WHEN TO REMOVE\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "def remove_columns_from_alignment(input_file, output_file, columns_to_remove, format=\"fasta\"):\n",
    "    \"\"\"\n",
    "    Remove specified columns from a multiple sequence alignment and save to new file\n",
    "    \n",
    "    Args:\n",
    "        input_file (str): Path to input alignment file\n",
    "        output_file (str): Path where to save trimmed alignment\n",
    "        columns_to_remove (list): List of column indices to remove (0-based)\n",
    "        format (str): File format (default: \"fasta\")\n",
    "    \"\"\"\n",
    "    # Read the alignment\n",
    "    alignment = AlignIO.read(input_file, format)\n",
    "    \n",
    "    # Sort columns to remove in descending order\n",
    "    # (so removing them doesn't affect the indices of remaining columns)\n",
    "    columns_to_remove = sorted(columns_to_remove, reverse=True)\n",
    "    \n",
    "    # Create new alignment records\n",
    "    new_records = []\n",
    "    \n",
    "    # Process each sequence\n",
    "    for record in alignment:\n",
    "        # Convert sequence to list for easier manipulation\n",
    "        seq_list = list(record.seq)\n",
    "        \n",
    "        # Remove specified columns\n",
    "        for col in columns_to_remove:\n",
    "            del seq_list[col]\n",
    "        \n",
    "        # Create new sequence record\n",
    "        new_seq = Seq(''.join(seq_list)) # Join the list element to a string again (i.e. after removal of amino acids out of sequence represented as list, turn into one string again) and turn into Seq object\n",
    "        new_record = SeqRecord(new_seq,\n",
    "                            id=record.id,\n",
    "                            name=record.name,\n",
    "                            description=record.description)\n",
    "        new_records.append(new_record)\n",
    "    \n",
    "    # Create new alignment\n",
    "    # TODO : Maybe we have to add some variables here (i.e. how to do the MSA)!\n",
    "    new_alignment = MultipleSeqAlignment(new_records)\n",
    "    \n",
    "    # Write to file\n",
    "    AlignIO.write(new_alignment, output_file, format)\n",
    "    \n",
    "    return new_alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Initialize analyzer \n",
    "    analyzer = ConservationAnalyzer(\"ClustalOmegaUniProtAlignment.fasta\")\n",
    "    \n",
    "    # Get comprehensive analysis\n",
    "    analysis = analyzer.analyze_columns(gap_threshold=0.90)\n",
    "   # analysis_2 = analyzer.analyze_rows()\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\nAlignment Summary:\")\n",
    "    print(f\"Number of sequences: {analyzer.num_sequences}\")\n",
    "    print(f\"Alignment length: {analyzer.alignment_length}\")\n",
    "\n",
    "\n",
    "    # Print number of True/False\n",
    "    counts = analysis['suggested_remove'].value_counts()\n",
    "\n",
    "    counts_true = counts[True]  # To be removed\n",
    "    counts_false = counts[False] # To be kept\n",
    "\n",
    "    print(f\"With the current removal tactic, we would remove {(counts_true / (counts_true + counts_false)):.2f} percent of columns ; we keep {counts_false} of {counts_false + counts_true} columns\")\n",
    "    \n",
    "\n",
    "    # Save detailed analysis to CSV\n",
    "    analysis.to_csv(\"conservation_analysis.csv\", index=False)\n",
    "\n",
    "\n",
    "    # Get indices of columns marked for removal\n",
    "    columns_to_remove = analysis[analysis['suggested_remove']]['position'].values.tolist()\n",
    "    # Convert to 0-based indices (if positions were 1-based)\n",
    "    columns_to_remove = [x-1 for x in columns_to_remove]\n",
    "    \n",
    "    # Remove columns and save new alignment\n",
    "    new_alignment = remove_columns_from_alignment(\n",
    "        \"ClustalOmegaUniProtAlignment.fasta\",\n",
    "        \"trimmed_alignment.fasta\",\n",
    "        columns_to_remove\n",
    "    )\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    print(f\"Original alignment length: {analyzer.alignment_length}\")\n",
    "    print(f\"Number of columns removed: {len(columns_to_remove)}\")\n",
    "    print(f\"New alignment length: {new_alignment.get_alignment_length()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Build a PSSM model starting from the MSA\n",
    "\n",
    "- First direct in the folder where ncbi-blast-2.16.0+ was installed\n",
    "\n",
    "- Then use this terminal command : \n",
    "\n",
    "ncbi-blast-2.16.0+/bin/psiblast -subject data/protein_family/trimmed_alignment.fasta -in_msa data/protein_family/trimmed_alignment.fasta -out_ascii_pssm data/protein_family/trimmed_alignment.pssm_ascii -out_pssm data/protein_family/trimmed_alignment.pssm\n",
    "\n",
    "    - Note that the trimmed_alignment.fasta needs to be in the described folder (data/protein_family/)\n",
    "\n",
    "\n",
    "5. Build a HMM model starting from the MSA\n",
    "\n",
    "- First direct in the folder where hmmer-3.4 was installed\n",
    "\n",
    "- Then use this terminal command :\n",
    "\n",
    "hmmer-3.4/src/hmmbuild data/protein_family/trimmed_alignment.hmm data/protein_family/trimmed_alignment.fasta\n",
    "\n",
    "    - Note that the trimmed_alignment.fasta needs to be in the described folder (data/protein_family/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models evaluation\n",
    "1. Generate predictions. Run HMM-SEARCH and PSI-BLAST with your models against\n",
    "SwissProt.\n",
    "\n",
    "    - Collect the list of retrieved hits\n",
    "\n",
    "    - Collect matching positions of your models in the retrieved hits\n",
    "\n",
    "PSI-BLAST :\n",
    "-\tWhen working on MAC : First have to change settings so we have access to use psiblast & makeblastdb (in terminal, go to folder where psiblast/makeblastdb located and run this)\n",
    "    xattr -c psiblast\n",
    "    chmod +x psiblast\n",
    "    xattr -c makeblastdb\n",
    "    chmod +x makeblastdb\n",
    "\n",
    "- Then use this command to create a \"formatted swissprot database\" (we need to check what that exactly means)\n",
    "      \n",
    "      ./ncbi-blast-2.16.0+/bin/makeblastdb -in uniprot_sprot.fasta -dbtype prot -out swissprot\n",
    "\n",
    "        - Notice that uniprot_sprot.fasta needs to be installed and be located in the current folder that we are in (in the terminal)\n",
    "\n",
    "\n",
    "- Finally, the actual predictions can be obtained with the following command\n",
    "\n",
    "        ./ncbi-blast-2.16.0+/bin/psiblast -in_pssm trimmed_alignment.pssm \\\n",
    "            -db swissprot \\\n",
    "            -out psiblast_search_output.txt \\\n",
    "            -outfmt \"6 qseqid sseqid qstart qend sstart send pident evalue\" \\\n",
    "\n",
    "    - where \n",
    "\n",
    "            qseqid: Query sequence identifier (your domain)\n",
    "            sseqid: Subject sequence identifier (matched protein)\n",
    "            qstart: Start position in your query domain\n",
    "            qend: End position in your query domain\n",
    "            sstart: Start position in the matched sequence\n",
    "            send: End position in the matched sequence\n",
    "            pident: Percentage of identical matches\n",
    "            evalue: Expectation value (statistical significance)\n",
    "    \n",
    "\n",
    "HMMER :\n",
    "\n",
    "- Simply use \n",
    "\n",
    "    ./hmmer-3.4/src/hmmsearch trimmed_alignment.hmm uniprot_sprot.fasta > hmmsearch_output.txt\n",
    "\n",
    "\n",
    "- To make both of the output files more accessible, we parsed them by writing two scripts \n",
    "    - in that way we create two .csv files that are easier to be compared, also directly by eye\n",
    "    - these .csv files then contained the matching positions of our two models in the retrieved hits\n",
    "\n",
    "Returns psiblast_parsed.csv\n",
    "Returns hmmsearch_output.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"FOR PSIBLAST\"\"\"\n",
    "def parse_psiblast_output(input_file):\n",
    "    results = []\n",
    "    \n",
    "    with open(input_file, 'r') as f:\n",
    "        for line in f:\n",
    "            # Skip empty lines\n",
    "            if not line.strip():\n",
    "                continue\n",
    "                \n",
    "            # Split the line by tabs or multiple spaces\n",
    "            parts = re.split(r'\\s+', line.strip())\n",
    "            \n",
    "            if len(parts) >= 8:  # Make sure we have all required fields\n",
    "                query_id = parts[0]\n",
    "                subject_id = parts[1]\n",
    "                \n",
    "                # Extract UniProt ID and organism from subject_id\n",
    "                # Format is usually sp|UniprotID|Name\n",
    "                subject_parts = subject_id.split('|')\n",
    "                if len(subject_parts) >= 2:\n",
    "                    uniprot_id = subject_parts[1]\n",
    "                    \n",
    "                    # Create result dictionary\n",
    "                    result = {\n",
    "                        'protein_name': subject_id,\n",
    "                        'uniprot_id': uniprot_id,\n",
    "                        'organism': 'N/A',  # PSIBLAST output doesn't include organism\n",
    "                        'domain_start': int(parts[4]),  # sstart\n",
    "                        'domain_end': int(parts[5]),    # send\n",
    "                        'domain_length': int(parts[5]) - int(parts[4]) + 1,\n",
    "                        'E-value': float(parts[7])  \n",
    "                    }\n",
    "                    results.append(result)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def write_csv(results, output_file):\n",
    "    if not results:\n",
    "        return\n",
    "    # Notice that we skip the start & end positions in the query domain (i.e. the PSSM here), as we are only interested in where we found matches in the sequence of SwissProt we looked through\n",
    "    fieldnames = ['protein_name', 'uniprot_id', 'organism', 'domain_start', \n",
    "                 'domain_end', 'domain_length', 'E-value']\n",
    "    \n",
    "    with open(output_file, 'w', newline='') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(results)\n",
    "\n",
    "\n",
    "input_file = 'psiblast_search_output.txt'\n",
    "output_file = 'psiblast_parsed.csv'\n",
    "\n",
    "results = parse_psiblast_output(input_file)\n",
    "write_csv(results, output_file)\n",
    "print(f\"Processed {len(results)} PSIBLAST matches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"FOR HMM\"\"\"\n",
    "# File paths\n",
    "input_file_path = \"hmmsearch_output.txt\"\n",
    "output_file_path = \"hmmsearch_output.csv\"\n",
    "\n",
    "# Initialize storage for parsed data\n",
    "parsed_data = []\n",
    "\n",
    "# Regular expressions to capture key information\n",
    "header_regex = r\">> ([^\\s]+)\"\n",
    "domain_regex = r\"\\s+(\\d+) [!?]\\s+[\\d\\.]+\\s+[\\d\\.]+\\s+[\\de\\.\\+\\-]+\\s+([\\de\\.\\+\\-]+)\\s+\\d+\\s+\\d+\\s+(?:\\[\\.|\\.\\.)+\\s+(\\d+)\\s+(\\d+)\"\n",
    "\n",
    "with open(input_file_path, \"r\") as infile:\n",
    "    current_protein = None\n",
    "\n",
    "    for line in infile:\n",
    "        # Match protein header line\n",
    "        header_match = re.match(header_regex, line)\n",
    "        if header_match:\n",
    "            # If we already captured a protein, save its data\n",
    "            if current_protein:\n",
    "                parsed_data.append(current_protein)\n",
    "\n",
    "            # Start a new protein record\n",
    "            protein_id = header_match.groups()[0]\n",
    "            current_protein = {\n",
    "                \"protein_name\": protein_id.split(\"|\")[2],\n",
    "                \"uniprot_id\": protein_id.split(\"|\")[1],\n",
    "                \"domains\": []\n",
    "            }\n",
    "\n",
    "        # Match domain annotation (including both `!` and `?` lines)\n",
    "        domain_match = re.match(domain_regex, line)\n",
    "        if domain_match and current_protein:\n",
    "            _, score, start, end = domain_match.groups()\n",
    "            start, end, score = int(start), int(end), float(score)\n",
    "            length = end - start + 1\n",
    "            current_protein[\"domains\"].append((score, start, end, length))\n",
    "\n",
    "    # Handle the last protein record\n",
    "    if current_protein:\n",
    "        parsed_data.append(current_protein)\n",
    "\n",
    "# Prepare fieldnames dynamically\n",
    "fieldnames = [\"protein_name\", \"uniprot_id\"]\n",
    "max_domains = max(len(protein[\"domains\"]) for protein in parsed_data)\n",
    "for i in range(1, max_domains + 1):\n",
    "    if i == 1:\n",
    "        fieldnames.extend([\n",
    "            f\"E-value\", f\"domain_start\", f\"domain_end\", f\"domain_length\"\n",
    "        ])\n",
    "    else:\n",
    "        fieldnames.extend([\n",
    "        f\"domain_{i}_E-value\", f\"domain_{i}_start\", f\"domain_{i}_end\", f\"domain_{i}_length\"\n",
    "    ])\n",
    "\n",
    "# Write to CSV\n",
    "with open(output_file_path, \"w\", newline=\"\") as outfile:\n",
    "    writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for protein in parsed_data:\n",
    "        row = {\n",
    "            \"protein_name\": protein[\"protein_name\"],\n",
    "            \"uniprot_id\": protein[\"uniprot_id\"]\n",
    "        }\n",
    "        for i, domain in enumerate(protein[\"domains\"], start=1):\n",
    "            if i == 1:\n",
    "                row[f\"E-value\"] = domain[0]\n",
    "                row[f\"domain_start\"] = domain[1]\n",
    "                row[f\"domain_end\"] = domain[2]\n",
    "                row[f\"domain_length\"] = domain[3] \n",
    "            else:\n",
    "                row[f\"domain_{i}_E-value\"] = domain[0]\n",
    "                row[f\"domain_{i}_start\"] = domain[1]\n",
    "                row[f\"domain_{i}_end\"] = domain[2]\n",
    "                row[f\"domain_{i}_length\"] = domain[3]\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(f\"CSV file generated: {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Define your ground truth. Find all proteins in SwissProt annotated (and not annotated) with the assigned Pfam domain\n",
    "\n",
    "    - Collect the list of proteins matching the assigned Pfam domain\n",
    "\n",
    "    - Collect matching positions of the Pfam domain in the retrieved sequences. Domain positions are available here (large tsv file) or using the InterPro API or align the Pfam domain yourself against SwissProt (HMMSEARCH)\n",
    "\n",
    "        - For this, we decided to use the InterPro API : \n",
    "\n",
    "        https://www.ebi.ac.uk/interpro/api/protein/reviewed/entry/pfam/PF00151/\n",
    "\n",
    "        which contains the reviewed proteins that match our assigned PFAM domain, PF00151\n",
    "\n",
    "        - to effectively scrape this website, we wrote a parser\n",
    "\n",
    "    Returns pfam_domain_positions.json \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import time\n",
    "from typing import Dict, List, Optional\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InterProAPIFetcher:\n",
    "    def __init__(self, base_url: str):\n",
    "        self.base_url = base_url\n",
    "        self.processed_count = 0\n",
    "        self.all_results = []\n",
    "        self.seen_accessions = set()  # Track unique protein accessions\n",
    "        self.duplicate_count = 0\n",
    "\n",
    "    def fetch_page(self, url: str) -> Optional[Dict]:\n",
    "        max_retries = 3\n",
    "        retry_delay = 2\n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                response = requests.get(url)\n",
    "                response.raise_for_status()\n",
    "                return response.json()\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"Attempt {attempt + 1} failed: {str(e)}\")\n",
    "                if attempt < max_retries - 1:\n",
    "                    print(f\"Waiting {retry_delay} seconds before retrying...\")\n",
    "                    time.sleep(retry_delay)\n",
    "                    retry_delay *= 2\n",
    "                else:\n",
    "                    print(\"Max retries reached. Moving on...\")\n",
    "                    return None\n",
    "\n",
    "    def fetch_all_pages(self) -> List[Dict]:\n",
    "        next_url = self.base_url\n",
    "        total_count = None\n",
    "        page_number = 1\n",
    "        \n",
    "        while next_url:\n",
    "            print(f\"\\nFetching page {page_number}...\")\n",
    "            print(f\"URL: {next_url}\")\n",
    "            \n",
    "            page_data = self.fetch_page(next_url)\n",
    "            \n",
    "            if page_data is None:\n",
    "                print(\"Failed to fetch page. Stopping pagination.\")\n",
    "                break\n",
    "            \n",
    "            if total_count is None:\n",
    "                total_count = page_data['count']\n",
    "                print(f\"API reports total count: {total_count}\")\n",
    "            \n",
    "            # Check for duplicates in this page\n",
    "            new_proteins = []\n",
    "            page_duplicates = 0\n",
    "            \n",
    "            for protein in page_data['results']:\n",
    "                accession = protein['metadata']['accession']\n",
    "                if accession in self.seen_accessions:\n",
    "                    page_duplicates += 1\n",
    "                    self.duplicate_count += 1\n",
    "                else:\n",
    "                    self.seen_accessions.add(accession)\n",
    "                    new_proteins.append(protein)\n",
    "            \n",
    "            print(f\"Page {page_number} stats:\")\n",
    "            print(f\"- Proteins in response: {len(page_data['results'])}\")\n",
    "            print(f\"- New unique proteins: {len(new_proteins)}\")\n",
    "            print(f\"- Duplicates found: {page_duplicates}\")\n",
    "            \n",
    "            self.all_results.extend(new_proteins)\n",
    "            self.processed_count = len(self.all_results)\n",
    "            \n",
    "            next_url = page_data.get('next')\n",
    "            page_number += 1\n",
    "            \n",
    "            time.sleep(1)\n",
    "        \n",
    "        print(f\"\\nFinal Statistics:\")\n",
    "        print(f\"Total unique proteins: {len(self.all_results)}\")\n",
    "        print(f\"Total duplicates found: {self.duplicate_count}\")\n",
    "        print(f\"Total processed entries: {self.processed_count + self.duplicate_count}\")\n",
    "        \n",
    "        return self.all_results\n",
    "\n",
    "    def save_results(self, filename: str):\n",
    "        output_data = {\n",
    "            'count': len(self.all_results),\n",
    "            'results': self.all_results\n",
    "        }\n",
    "        \n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(output_data, f, indent=2)\n",
    "        print(f\"\\nResults saved to {filename}\")\n",
    "        print(f\"File contains {len(self.all_results)} unique proteins\")\n",
    "\n",
    "def main():\n",
    "    base_url = \"https://www.ebi.ac.uk/interpro/api/protein/reviewed/entry/pfam/PF00151/\"\n",
    "    \n",
    "    fetcher = InterProAPIFetcher(base_url)\n",
    "    fetcher.fetch_all_pages()\n",
    "    fetcher.save_results('pfam_domain_positions.json')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- After the successful parsing, we also turn the .json file into .csv for further processing (and again, easier interpretation by eye)\n",
    "\n",
    "For this we wrote another script \n",
    "\n",
    "Returns pfam_domain_positions.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pfam_info(json_file):\n",
    "    # Read the JSON file\n",
    "    with open(json_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # List to store the extracted information\n",
    "    pfam_matches = []\n",
    "    \n",
    "    # Iterate through each result in the JSON data\n",
    "    for result in data['results']:\n",
    "        # Extract protein metadata\n",
    "        protein_info = {\n",
    "            'protein_name': result['metadata']['name'],\n",
    "            'uniprot_id': result['metadata']['accession'],\n",
    "            'organism': result['metadata']['source_organism']['scientificName']\n",
    "        }\n",
    "        \n",
    "        # Extract PFAM domain information\n",
    "        # We know there's only one entry because we queried for a specific PFAM domain\n",
    "        pfam_entry = result['entries'][0]\n",
    "        \n",
    "        # Get the domain fragments (start and end positions)\n",
    "        for location in pfam_entry['entry_protein_locations']:\n",
    "            for fragment in location['fragments']:\n",
    "                domain_info = {\n",
    "                    **protein_info,  # Include all protein information\n",
    "                    'domain_start': fragment['start'],\n",
    "                    'domain_end': fragment['end'],\n",
    "                    'domain_length': fragment['end'] - fragment['start'] + 1,\n",
    "                    'protein_length': pfam_entry['protein_length'],\n",
    "                    'score': location['score']\n",
    "                }\n",
    "                pfam_matches.append(domain_info)\n",
    "    \n",
    "    return pfam_matches\n",
    "\n",
    "# Use the function to extract information\n",
    "json_file = 'pfam_domain_positions.json'\n",
    "matches = extract_pfam_info(json_file)\n",
    "\n",
    "# Print the results in a formatted way\n",
    "print(\"\\nPFAM Domain Matches:\")\n",
    "print(\"-\" * 80)\n",
    "for match in matches:\n",
    "    print(f\"Protein: {match['protein_name']} ({match['uniprot_id']})\")\n",
    "    print(f\"Organism: {match['organism']}\")\n",
    "    print(f\"Domain position: {match['domain_start']}-{match['domain_end']} \"\n",
    "          f\"(length: {match['domain_length']} aa)\")\n",
    "    print(f\"Total protein length: {match['protein_length']} aa\")\n",
    "    print(f\"Score: {match['score']}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "# Optional: Save to a more structured format like CSV for further analysis\n",
    "import pandas as pd\n",
    "\n",
    "# Convert the matches to a DataFrame\n",
    "df = pd.DataFrame(matches)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv('pfam_domain_positions.csv', index=False)\n",
    "print(\"\\nResults have been saved to 'pfam_domain_positions.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Finally, we now have 3 similar .csv files , 1 depicting the Ground Truth and 2 depicting the predictions from both the PSSM and HMM model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "3. Compare your model with the assigned Pfam. Calculate the precision, recall, F-score, balanced accuracy, MCC\n",
    "\n",
    "    - Comparison at the protein level. Measure the ability of your model to retrieve the same proteins matched by Pfam\n",
    "\n",
    "    - Comparison at the residue level. Measure the ability of your model to match the same position matched by Pfam\n",
    "\n",
    "\n",
    "- To this extent , we created a script for evaluation purposes\n",
    "- Here also explain the code more (e.g. the one hot encoding for residue levels, that for the HMM we always just took the first hit with the lowest e-value and didn't look at the other hits because they never were significant etc.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, balanced_accuracy_score, matthews_corrcoef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Based on the .csv files that closely resemble each other, do the calculations\"\"\"\n",
    "\n",
    "\n",
    "def create_residue_vectors(pred_df, pfam_df, protein_id):\n",
    "    \"\"\"\n",
    "    For a single protein (based on its protein_id), create residue vectors\n",
    "    \"\"\"\n",
    "\n",
    "    # Note that since the ground truth (i.e. the PFAM domains) are all just a single sequence hit, we only include the strongest\n",
    "    # (lowest e-value) hit from the HMM search (also we noticed that basically all 2nd, 3rd domain hits have much higher e-values)\n",
    " \n",
    "    # Get domain positions from both predictions\n",
    "    pred_matches = pred_df[pred_df['uniprot_id'] == protein_id]\n",
    "    pfam_matches = pfam_df[pfam_df['uniprot_id'] == protein_id]\n",
    "    \n",
    "    # Check if for the protein at hand, we even find it in both of the .csv's \n",
    "    if len(pred_matches) == 0 or len(pfam_matches) == 0:\n",
    "        return None\n",
    "    \n",
    "\n",
    "    assert (len(pred_df == 1) , \"not length 1 \")\n",
    "    # Get the max of the max lengths for each hit for the current protein\n",
    "    # Notice that we can have multiple hits per protein (i.e. multiple alignments that were found)\n",
    "    # So we have to account for all of them\n",
    "    max_length = max(\n",
    "        pred_matches['domain_end'].max(),\n",
    "        pfam_matches['domain_end'].max()\n",
    "    )\n",
    "    \n",
    "    # With that, we can create vectors of the same size\n",
    "    # Create binary vectors for each position\n",
    "    true_positions = np.zeros(int(max_length))\n",
    "    pred_positions = np.zeros(int(max_length))\n",
    "    \n",
    "\n",
    "    # Now, iterate through all the found alignments (for the current protein)\n",
    "\n",
    "    # Fill in Pfam (true) positions\n",
    "    for _, row in pfam_matches.iterrows():\n",
    "        start = row['domain_start'] - 1  # Convert to 0-based indexing\n",
    "        end = row['domain_end']\n",
    "        true_positions[start:end] = 1\n",
    "        \n",
    "    # Fill in PSSM/HMM (predicted) positions\n",
    "    for _, row in pred_matches.iterrows():\n",
    "        start = int(row['domain_start'] - 1)  # Convert to 0-based indexing\n",
    "        end = int(row['domain_end'])\n",
    "        pred_positions[start:end] = 1\n",
    "    \n",
    "    return true_positions, pred_positions\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_model(psiblast_file, hmm_file, pfam_file, only_found=False, e_threshold = 0.0001):\n",
    "    \"\"\"\n",
    "    Evaluate PSIBLAST model performance against Pfam annotations\n",
    "    \n",
    "    Parameters:\n",
    "    - psiblast_file: Path to PSIBLAST results CSV\n",
    "    - hmm_file : Path to HMM results CSV\n",
    "    - pfam_file: Path to Pfam ground truth CSV\n",
    "    - only_found: If True, only evaluate proteins found by PSIBLAST\n",
    "    \"\"\"\n",
    "    # Step 1: Load both CSV files\n",
    "    psiblast_df = pd.read_csv(psiblast_file)\n",
    "    hmm_df = pd.read_csv(hmm_file)\n",
    "    pfam_df = pd.read_csv(pfam_file)\n",
    "\n",
    "    # HMM finds a lot of hits, a lot with extremely high e-values : Take only the ones that are above some threshold (score for now, later e-value)\n",
    "    # We filter based on the first domain hit (TODO : i.e. the best one ? check if first found domain always strongest)\n",
    "    filtered_hmm_proteins = hmm_df[hmm_df['E-value'] <= e_threshold]['uniprot_id']\n",
    "    \n",
    "    # Step 2: Get unique list of proteins from both files\n",
    "    psiblast_proteins = set(psiblast_df['uniprot_id'])\n",
    "    hmm_proteins = set(filtered_hmm_proteins)\n",
    "    pfam_proteins = set(pfam_df['uniprot_id'])\n",
    "\n",
    "    \n",
    "    if only_found:\n",
    "        # Only consider proteins that PSIBLAST/HMM found\n",
    "        all_proteins_psiblast = psiblast_proteins\n",
    "        all_proteins_hmm = hmm_proteins\n",
    "        print(\"\\nEvaluating only PSIBLAST-found proteins:\")\n",
    "    else:\n",
    "        # Consider all proteins from both sets\n",
    "        all_proteins_psiblast = psiblast_proteins.union(pfam_proteins)\n",
    "        all_proteins_hmm = hmm_proteins.union(pfam_proteins)\n",
    "        print(\"\\nEvaluating all proteins:\")\n",
    "    \n",
    "    print(f\"Number of proteins predicted by PSIBLAST: {len(psiblast_proteins)}\")\n",
    "    print(f\"Number of proteins predicted by HMM: {len(hmm_proteins)}\")\n",
    "    print(f\"Number of proteins in Pfam ground truth: {len(pfam_proteins)}\")\n",
    "    print(f\"Number of proteins being evaluated for PSIBLAST: {len(all_proteins_psiblast)}\")\n",
    "    print(f\"Number of proteins being evaluated for HMM: {len(all_proteins_hmm)}\")\n",
    "    \n",
    "\n",
    "    print(\"\\n=== Protein-Level Evaluation ===\")\n",
    "    # Step 3: Create binary vectors for true and predicted labels\n",
    "    y_true_psiblast = []  # Ground truth from Pfam\n",
    "    y_pred_psiblast = []  # Predictions from PSIBLAST\n",
    "    y_true_hmm = []\n",
    "    y_pred_hmm = []\n",
    "\n",
    "\n",
    "    \n",
    "    for protein in all_proteins_psiblast:\n",
    "        y_true_psiblast.append(1 if protein in pfam_proteins else 0)\n",
    "        y_pred_psiblast.append(1 if protein in psiblast_proteins else 0)\n",
    "\n",
    "\n",
    "    for protein in all_proteins_hmm:\n",
    "        y_true_hmm.append(1 if protein in pfam_proteins else 0)\n",
    "        y_pred_hmm.append(1 if protein in hmm_proteins else 0)\n",
    "\n",
    "    # So we have something like\n",
    "    # y_true_psiblast 0 0 1 0 1 ...\n",
    "    # y_pred_psiblast 0 1 1 0 1 ...\n",
    "    \n",
    "    # Step 4: Calculate performance metrics\n",
    "    protein_results_psiblast = {\n",
    "        'Precision': precision_score(y_true_psiblast, y_pred_psiblast),\n",
    "        'Recall': recall_score(y_true_psiblast, y_pred_psiblast),\n",
    "        'F-score': f1_score(y_true_psiblast, y_pred_psiblast),\n",
    "        'Balanced Accuracy': balanced_accuracy_score(y_true_psiblast, y_pred_psiblast),\n",
    "        'MCC': matthews_corrcoef(y_true_psiblast, y_pred_psiblast)\n",
    "    }\n",
    "\n",
    "\n",
    "    protein_results_hmm = {\n",
    "        'Precision': precision_score(y_true_hmm, y_pred_hmm),\n",
    "        'Recall': recall_score(y_true_hmm, y_pred_hmm),\n",
    "        'F-score': f1_score(y_true_hmm, y_pred_hmm),\n",
    "        'Balanced Accuracy': balanced_accuracy_score(y_true_hmm, y_pred_hmm),\n",
    "        'MCC': matthews_corrcoef(y_true_hmm, y_pred_hmm)\n",
    "    }\n",
    "\n",
    "\n",
    "    # Step 5: Calculate confusion matrix components\n",
    "    tp_psiblast = sum(1 for t, p in zip(y_true_psiblast, y_pred_psiblast) if t == 1 and p == 1)\n",
    "    fp_psiblast = sum(1 for t, p in zip(y_true_psiblast, y_pred_psiblast) if t == 0 and p == 1)\n",
    "    fn_psiblast = sum(1 for t, p in zip(y_true_psiblast, y_pred_psiblast) if t == 1 and p == 0)\n",
    "    tn_psiblast = sum(1 for t, p in zip(y_true_psiblast, y_pred_psiblast) if t == 0 and p == 0)\n",
    "\n",
    "\n",
    "    tp_hmm = sum(1 for t, p in zip(y_true_hmm, y_pred_hmm) if t == 1 and p == 1)\n",
    "    fp_hmm = sum(1 for t, p in zip(y_true_hmm, y_pred_hmm) if t == 0 and p == 1)\n",
    "    fn_hmm = sum(1 for t, p in zip(y_true_hmm, y_pred_hmm) if t == 1 and p == 0)\n",
    "    tn_hmm = sum(1 for t, p in zip(y_true_hmm, y_pred_hmm) if t == 0 and p == 0)\n",
    "    \n",
    "    # Print detailed results\n",
    "    print(\"\\nProtein-Level Confusion Matrix PSIBLAST:\")\n",
    "    print(f\"True Positives: {tp_psiblast}\")\n",
    "    print(f\"False Positives: {fp_psiblast}\")\n",
    "    print(f\"False Negatives: {fn_psiblast}\")\n",
    "    print(f\"True Negatives: {tn_psiblast}\")\n",
    "\n",
    "\n",
    "\n",
    "    print(\"\\nProtein-Level Confusion Matrix HMM:\")\n",
    "    print(f\"True Positives: {tp_hmm}\")\n",
    "    print(f\"False Positives: {fp_hmm}\")\n",
    "    print(f\"False Negatives: {fn_hmm}\")\n",
    "    print(f\"True Negatives: {tn_hmm}\")\n",
    "\n",
    "\n",
    "    print(\"\\nProtein-Level Metrics PSIBLAST:\")\n",
    "    for metric, value in protein_results_psiblast.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "\n",
    "    print(\"\\nProtein-Level Metrics HMM:\")\n",
    "    for metric, value in protein_results_hmm.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "    # Residue-level evaluation\n",
    "   \n",
    "    print(\"\\n=== Residue-Level Evaluation ===\")\n",
    "    # Only evaluate residues for proteins found in both sets \n",
    "    common_proteins_psiblast = psiblast_proteins.intersection(pfam_proteins)\n",
    "    common_proteins_hmm = hmm_proteins.intersection(pfam_proteins)\n",
    "    print(f\"Number of proteins for residue-level evaluation PSIBLAST: {len(common_proteins_psiblast)}\")\n",
    "    print(f\"Number of proteins for residue-level evaluation HMM: {len(common_proteins_hmm)}\")\n",
    "    \n",
    "    # Collect all residue-level predictions\n",
    "    all_true_residues_psiblast = []\n",
    "    all_pred_residues_psiblast = []\n",
    "\n",
    "    all_true_residues_hmm = []\n",
    "    all_pred_residues_hmm = []\n",
    "\n",
    "    \n",
    "    for protein in common_proteins_psiblast:\n",
    "        result = create_residue_vectors(psiblast_df, pfam_df, protein)\n",
    "        if result is not None:\n",
    "            true_pos, pred_pos = result\n",
    "            all_true_residues_psiblast.extend(true_pos)\n",
    "            all_pred_residues_psiblast.extend(pred_pos)\n",
    "\n",
    "\n",
    "    for protein in common_proteins_hmm:\n",
    "        result = create_residue_vectors(hmm_df, pfam_df, protein)\n",
    "        if result is not None:\n",
    "            true_pos, pred_pos = result\n",
    "            all_true_residues_hmm.extend(true_pos)\n",
    "            all_pred_residues_hmm.extend(pred_pos)\n",
    "    \n",
    "    # Calculate residue-level metrics\n",
    "    residue_results_psiblast = {\n",
    "        'Precision': precision_score(all_true_residues_psiblast, all_pred_residues_psiblast),\n",
    "        'Recall': recall_score(all_true_residues_psiblast, all_pred_residues_psiblast),\n",
    "        'F-score': f1_score(all_true_residues_psiblast, all_pred_residues_psiblast),\n",
    "        'Balanced Accuracy': balanced_accuracy_score(all_true_residues_psiblast, all_pred_residues_psiblast),\n",
    "        'MCC': matthews_corrcoef(all_true_residues_psiblast, all_pred_residues_psiblast)\n",
    "    }\n",
    "    \n",
    "    # Calculate residue-level confusion matrix\n",
    "    tp = sum(1 for t, p in zip(all_true_residues_psiblast, all_pred_residues_psiblast) if t == 1 and p == 1)\n",
    "    fp = sum(1 for t, p in zip(all_true_residues_psiblast, all_pred_residues_psiblast) if t == 0 and p == 1)\n",
    "    fn = sum(1 for t, p in zip(all_true_residues_psiblast, all_pred_residues_psiblast) if t == 1 and p == 0)\n",
    "    tn = sum(1 for t, p in zip(all_true_residues_psiblast, all_pred_residues_psiblast) if t == 0 and p == 0)\n",
    "    \n",
    "    print(\"\\nResidue-Level Confusion Matrix PSIBLAST:\")\n",
    "    print(f\"True Positives: {tp}\")\n",
    "    print(f\"False Positives: {fp}\")\n",
    "    print(f\"False Negatives: {fn}\")\n",
    "    print(f\"True Negatives: {tn}\")\n",
    "    \n",
    "    print(\"\\nResidue-Level Metrics PSIBLAST:\")\n",
    "    for metric, value in residue_results_psiblast.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "    # Calculate residue-level metrics\n",
    "    residue_results_hmm = {\n",
    "        'Precision': precision_score(all_true_residues_hmm, all_pred_residues_hmm),\n",
    "        'Recall': recall_score(all_true_residues_hmm, all_pred_residues_hmm),\n",
    "        'F-score': f1_score(all_true_residues_hmm, all_pred_residues_hmm),\n",
    "        'Balanced Accuracy': balanced_accuracy_score(all_true_residues_hmm, all_pred_residues_hmm),\n",
    "        'MCC': matthews_corrcoef(all_true_residues_hmm, all_pred_residues_hmm)\n",
    "    }\n",
    "    \n",
    "    # Calculate residue-level confusion matrix\n",
    "    tp = sum(1 for t, p in zip(all_true_residues_hmm, all_pred_residues_hmm) if t == 1 and p == 1)\n",
    "    fp = sum(1 for t, p in zip(all_true_residues_hmm, all_pred_residues_hmm) if t == 0 and p == 1)\n",
    "    fn = sum(1 for t, p in zip(all_true_residues_hmm, all_pred_residues_hmm) if t == 1 and p == 0)\n",
    "    tn = sum(1 for t, p in zip(all_true_residues_hmm, all_pred_residues_hmm) if t == 0 and p == 0)\n",
    "\n",
    "\n",
    "    print(\"\\nResidue-Level Confusion Matrix HMM:\")\n",
    "    print(f\"True Positives: {tp}\")\n",
    "    print(f\"False Positives: {fp}\")\n",
    "    print(f\"False Negatives: {fn}\")\n",
    "    print(f\"True Negatives: {tn}\")\n",
    "    \n",
    "    print(\"\\nResidue-Level Metrics HMM:\")\n",
    "    for metric, value in residue_results_hmm.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "psiblast_file = 'psiblast_parsed.csv'\n",
    "hmm_file = 'hmmsearch_output.csv'\n",
    "pfam_file = 'pfam_domain_positions.csv'\n",
    "\n",
    "#print(\"===============================\")\n",
    "#print(\"Evaluation on all proteins:\")\n",
    "#results_all = evaluate_model_protein_level(psiblast_file, pfam_file, only_found=False)\n",
    "\n",
    "print(\"\\n===============================\")\n",
    "print(\"Evaluation only on found proteins in both PSSM/HMM:\")\n",
    "evaluate_model(psiblast_file,hmm_file, pfam_file, only_found=False, e_threshold= 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Consider refining your models to improve their performance\n",
    "\n",
    "- Multiple Steps that we took\n",
    "\n",
    "    - remove redundancy at different thresholds in JalView\n",
    "    - use other starting database\n",
    "    - use in first search (Step 1) different e-values for broader searches\n",
    "    - when removing columns, tweak the two parameters that we had differently to see what gives better results\n",
    "    - and here then say we found our best model using ...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Domain family characterization\n",
    "Once the family model is defined (previous step), you will look at functional (and structural) aspects/properties of the entire protein family. The objective is to provide insights about the main function of the family."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taxonomy\n",
    "\n",
    "1. Collect the taxonomic lineage (tree branch) for each protein of the family_sequences dataset\n",
    "from UniProt (entity/organism/lineage in the UniProt XML)\n",
    "\n",
    "2. Plot the taxonomic tree of the family with nodes size proportional to their relative abundance \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from scipy.spatial.distance import pdist\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "from Bio import Phylo\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "from ete3 import Tree, TreeStyle, NodeStyle, TextFace\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a union of the proteins found by HMM (with e-value thresh of 0.001) and PSIBLAST (all 21) to represent our \"family_sequences\"\n",
    "e_threshold = 0.001\n",
    "psiblast_df = pd.read_csv(\"psiblast_parsed.csv\")\n",
    "hmm_df = pd.read_csv(\"hmmsearch_output.csv\")\n",
    "filtered_hmm_proteins = hmm_df[hmm_df['E-value'] <= e_threshold]['uniprot_id']\n",
    "    \n",
    "psiblast_proteins = set(psiblast_df['uniprot_id'])\n",
    "hmm_proteins = set(filtered_hmm_proteins)\n",
    "\n",
    "\n",
    "family_sequences = list(psiblast_proteins.union(hmm_proteins))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing P54315: 100%|██████████| 83/83 [00:17<00:00,  4.86it/s]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taxonomy file saved to: taxonomy_info.csv\n",
      "Tree saved to: phylogenetic_tree_freq.png\n"
     ]
    }
   ],
   "source": [
    "# TaxonomyAnalyzer Class for fetching taxonomy information\n",
    "class TaxonomyAnalyzer:\n",
    "    def __init__(self, max_retries: int = 3, retry_delay: int = 1):\n",
    "        self.max_retries = max_retries\n",
    "        self.retry_delay = retry_delay\n",
    "        self.uniprot_base_url = \"https://rest.uniprot.org/uniprotkb/\"\n",
    "\n",
    "    def fetch_taxonomy_info(self, protein_ids: list, output_file: str):\n",
    "        taxonomy_data = []\n",
    "\n",
    "        pbar = tqdm(protein_ids, desc=\"Fetching taxonomy data\")\n",
    "\n",
    "        for protein_id in pbar:\n",
    "            pbar.set_description(f\"Processing {protein_id}\")\n",
    "\n",
    "            for attempt in range(self.max_retries):\n",
    "                try:\n",
    "                    response = requests.get(f\"{self.uniprot_base_url}{protein_id}.json\")\n",
    "                    response.raise_for_status()\n",
    "                    data = response.json()\n",
    "\n",
    "                    taxonomy = data.get(\"organism\", {})\n",
    "                    scientific_name = taxonomy.get(\"scientificName\", \"N/A\")\n",
    "                    lineage = taxonomy.get(\"lineage\", [])\n",
    "\n",
    "                    taxonomy_data.append([protein_id, scientific_name, \" > \".join(lineage)])\n",
    "                    break\n",
    "\n",
    "                except requests.exceptions.RequestException as e:\n",
    "                    print(f\"Error fetching data for {protein_id}: {e}\")\n",
    "                    if attempt == self.max_retries - 1:\n",
    "                        taxonomy_data.append([protein_id, \"Error\", \"\"])\n",
    "                    else:\n",
    "                        time.sleep(self.retry_delay)\n",
    "\n",
    "        taxonomy_df = pd.DataFrame(taxonomy_data, columns=[\"Protein ID\", \"Scientific Name\", \"Lineage\"])\n",
    "        taxonomy_df.to_csv(output_file, index=False)\n",
    "        return taxonomy_df\n",
    "\n",
    "# Load protein IDs from files\n",
    "def load_protein_ids(psiblast_file, hmm_file, e_threshold=0.001):\n",
    "    psiblast_df = pd.read_csv(psiblast_file)\n",
    "    hmm_df = pd.read_csv(hmm_file)\n",
    "\n",
    "    filtered_hmm_proteins = hmm_df[hmm_df['E-value'] <= e_threshold]['uniprot_id']\n",
    "    psiblast_proteins = set(psiblast_df['uniprot_id'])\n",
    "    hmm_proteins = set(filtered_hmm_proteins)\n",
    "\n",
    "    return list(psiblast_proteins.union(hmm_proteins))\n",
    "\n",
    "# Process taxonomy data\n",
    "def process_taxonomy(data, correct_column_name):\n",
    "    taxonomy_dict = {}\n",
    "    frequency_counts = {}\n",
    "    \n",
    "    for _, row in data.iterrows():\n",
    "        lineage = row[correct_column_name].split(\" > \")\n",
    "        current = taxonomy_dict\n",
    "        # Track the full path to maintain hierarchy information\n",
    "        current_path = [] # such that we count occurences of terms in the correct \"level\" where they appear (i.e. always count just in the \"column\" of the linage)\n",
    "        \n",
    "        for level in lineage:\n",
    "            current_path.append(level)\n",
    "            path_key = \" > \".join(current_path)\n",
    "            \n",
    "            # Count frequencies using the full path as key\n",
    "            if path_key not in frequency_counts:\n",
    "                frequency_counts[path_key] = 0\n",
    "            frequency_counts[path_key] += 1\n",
    "            \n",
    "            if level not in current:\n",
    "                current[level] = {}\n",
    "            current = current[level]\n",
    "    \n",
    "    return taxonomy_dict, frequency_counts\n",
    "\n",
    "# Create a Newick string for the taxonomy tree\n",
    "def dict_to_newick(d, parent_abundance=None):\n",
    "    newick = \"\"\n",
    "    for key, sub_dict in d.items():\n",
    "        size = parent_abundance.get(key, 1) if parent_abundance else 1\n",
    "        sub_tree = dict_to_newick(sub_dict, parent_abundance)\n",
    "        newick += f\"({sub_tree}){key}:{size},\" if sub_tree else f\"{key}:{size},\"\n",
    "    return newick.rstrip(\",\")\n",
    "\n",
    "\n",
    "\n",
    "# Fetch taxonomy data\n",
    "def main():\n",
    "    psiblast_file = \"psiblast_parsed.csv\"\n",
    "    hmm_file = \"hmmsearch_output.csv\"\n",
    "    protein_ids = load_protein_ids(psiblast_file, hmm_file)\n",
    "\n",
    "    analyzer = TaxonomyAnalyzer()\n",
    "    taxonomy_data = analyzer.fetch_taxonomy_info(protein_ids, \"taxonomy_info.csv\")\n",
    "\n",
    "    print(\"Taxonomy file saved to: taxonomy_info.csv\")\n",
    "\n",
    "    # Correct column name\n",
    "    correct_column_name = \"Lineage\"  # Use the correct column name\n",
    "\n",
    "    # Create a nested dictionary of taxonomy\n",
    "    taxonomy_dict, frequency_counts = process_taxonomy(taxonomy_data, correct_column_name)\n",
    "\n",
    "    # Count relative abundance (of the different paths ! ; right now we don't really use that)\n",
    "    abundance_counts = taxonomy_data[correct_column_name].value_counts().to_dict()\n",
    "\n",
    "\n",
    "    newick_tree = f\"({dict_to_newick(taxonomy_dict, abundance_counts)});\"\n",
    "\n",
    "    # Plot using ETE Toolkit\n",
    "    phylo_tree = Tree(newick_tree, format=1)\n",
    "    tree_style = TreeStyle()\n",
    "    tree_style.show_leaf_name = False\n",
    "\n",
    "\n",
    "    # Adjust node sizes (normalize and refine scaling)\n",
    "    max_size = 50  # Increase max size for better differentiation\n",
    "    scaling_factor = 2  # Further refine scaling for visual contrast\n",
    "    for node in phylo_tree.traverse():\n",
    "        # Get the full path from root to this node\n",
    "        path = []\n",
    "        current = node\n",
    "        while current:\n",
    "            if current.name:  # Skip empty names\n",
    "                path.insert(0, current.name)\n",
    "            current = current.up\n",
    "        \n",
    "        path_key = \" > \".join(path)\n",
    "        count = frequency_counts.get(path_key, 1)\n",
    "        nstyle = NodeStyle()\n",
    "        size = abundance_counts.get(node.name, 1)\n",
    "        nstyle[\"size\"] = min(size * scaling_factor, max_size)  # Scale and cap node size\n",
    "        node.set_style(nstyle)\n",
    "        # Add label with name and count\n",
    "        node.add_face(TextFace(f\"{node.name} ({count})\", fsize=10), column=0)\n",
    "\n",
    "    # Improve tree spacing\n",
    "    tree_style.branch_vertical_margin = 30  # Increase spacing for better visibility\n",
    "\n",
    "    # Save the tree to a high-resolution PNG file\n",
    "    output_file = \"phylogenetic_tree_freq.png\"\n",
    "    phylo_tree.render(output_file, w=3000, h=2000, tree_style=tree_style)\n",
    "\n",
    "    print(f\"Tree saved to: {output_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function\n",
    "\n",
    "1. Collect GO annotations for each protein of the family_sequences dataset (entity/dbReference type=\"GO\" in the UniProt XML)\n",
    "\n",
    "2. Calculate the enrichment of each term in the dataset compared to GO annotations available in the SwissProt database (you can download the entire SwissProt XML here). You can use Fisher’ exact test and verify that both two-tails and right-tail P-values (or left-tail depending on how you build the confusion matrix) are close to zero\n",
    "\n",
    "3. Plot enriched terms in a word cloud \n",
    "\n",
    "4. Take into consideration the hierarchical structure of the GO ontology and report most significantly enriched branches, i.e. high level terms\n",
    "\n",
    "5. Always report the full name of the terms and not only the GO ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: obonet in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (1.1.0)\n",
      "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from obonet) (3.3)\n",
      "Requirement already satisfied: statsmodels in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (0.14.4)\n",
      "Requirement already satisfied: numpy<3,>=1.22.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from statsmodels) (2.0.0)\n",
      "Requirement already satisfied: scipy!=1.9.2,>=1.8 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from statsmodels) (1.14.0)\n",
      "Requirement already satisfied: pandas!=2.1.0,>=1.4 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from statsmodels) (2.2.2)\n",
      "Requirement already satisfied: patsy>=0.5.6 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from statsmodels) (1.0.1)\n",
      "Requirement already satisfied: packaging>=21.3 in /Users/albertocalabrese99/Library/Python/3.12/lib/python/site-packages (from statsmodels) (24.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/albertocalabrese99/Library/Python/3.12/lib/python/site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/albertocalabrese99/Library/Python/3.12/lib/python/site-packages (from python-dateutil>=2.8.2->pandas!=2.1.0,>=1.4->statsmodels) (1.16.0)\n",
      "Requirement already satisfied: goatools in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (1.4.12)\n",
      "Requirement already satisfied: docopt in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from goatools) (0.6.2)\n",
      "Requirement already satisfied: ftpretty in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from goatools) (0.4.0)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from goatools) (2.0.0)\n",
      "Requirement already satisfied: openpyxl in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from goatools) (3.1.5)\n",
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from goatools) (2.2.2)\n",
      "Requirement already satisfied: pydot in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from goatools) (3.0.3)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from goatools) (2.32.3)\n",
      "Requirement already satisfied: rich in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from goatools) (13.9.4)\n",
      "Requirement already satisfied: scipy in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from goatools) (1.14.0)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from goatools) (75.6.0)\n",
      "Requirement already satisfied: statsmodels in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from goatools) (0.14.4)\n",
      "Requirement already satisfied: xlsxwriter in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from goatools) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil in /Users/albertocalabrese99/Library/Python/3.12/lib/python/site-packages (from ftpretty->goatools) (2.9.0.post0)\n",
      "Requirement already satisfied: et-xmlfile in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from openpyxl->goatools) (2.0.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pandas->goatools) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pandas->goatools) (2024.1)\n",
      "Requirement already satisfied: pyparsing>=3.0.9 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pydot->goatools) (3.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->goatools) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->goatools) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->goatools) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->goatools) (2024.6.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from rich->goatools) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/albertocalabrese99/Library/Python/3.12/lib/python/site-packages (from rich->goatools) (2.18.0)\n",
      "Requirement already satisfied: patsy>=0.5.6 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from statsmodels->goatools) (1.0.1)\n",
      "Requirement already satisfied: packaging>=21.3 in /Users/albertocalabrese99/Library/Python/3.12/lib/python/site-packages (from statsmodels->goatools) (24.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->goatools) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/albertocalabrese99/Library/Python/3.12/lib/python/site-packages (from python-dateutil->ftpretty->goatools) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install obonet\n",
    "!pip install statsmodels\n",
    "!pip install goatools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "from scipy.stats import fisher_exact\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import obonet\n",
    "import networkx as nx\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "from collections import defaultdict\n",
    "from goatools import obo_parser\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to turn a Protein_ID : [GO_terms] into a GO_term : [Protein_IDs] dictionary\n",
    "\n",
    "def reverse_protein_go_dict(protein_to_go):\n",
    "   \"\"\"\n",
    "   Convert protein->GO dict to GO->proteins dict.\n",
    "   \"\"\"\n",
    "   go_to_proteins = defaultdict(list)\n",
    "   for protein, go_terms in protein_to_go.items():\n",
    "       for go_term in go_terms:\n",
    "           go_to_proteins[go_term].append(protein)\n",
    "   return go_to_proteins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 1: Load Protein IDs\n",
    "# TODO : we basically did this above already for taxonomy task and just here neatly written into a function, so we could maybe just do it once in the whole code later on\n",
    "def load_protein_ids(psiblast_file, hmm_file, e_threshold=0.001):\n",
    "    \"\"\"Load protein IDs from PSI-BLAST and HMM search results.\"\"\"\n",
    "    psiblast_df = pd.read_csv(psiblast_file)\n",
    "    hmm_df = pd.read_csv(hmm_file)\n",
    "    \n",
    "    filtered_hmm_proteins = hmm_df[hmm_df['E-value'] <= e_threshold]['uniprot_id']\n",
    "    psiblast_proteins = set(psiblast_df['uniprot_id'])\n",
    "    hmm_proteins = set(filtered_hmm_proteins)\n",
    "    \n",
    "    return list(psiblast_proteins.union(hmm_proteins))\n",
    "\n",
    "'''\n",
    "def fetch_go_annotations(protein_id):\n",
    "    \"\"\"\n",
    "    Fetch and categorize GO annotations for a given protein ID from the UniProt API.\n",
    "    \n",
    "    Args:\n",
    "        protein_id (str): The UniProt ID of the protein\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary containing:\n",
    "            - Categorized GO terms separated by molecular function, biological process, \n",
    "              and cellular component (new format)\n",
    "    \"\"\"\n",
    "    # Define the UniProt API URL for XML data\n",
    "    url = f\"https://rest.uniprot.org/uniprotkb/{protein_id}.xml\"\n",
    "\n",
    "    try:\n",
    "        # Fetch the XML data from UniProt\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Initialize our data structures\n",
    "        go_terms = []  # Original format\n",
    "        categorized_terms = {\n",
    "            'molecular_function': [],\n",
    "            'biological_process': [],\n",
    "            'cellular_component': []\n",
    "        }\n",
    "        \n",
    "        # Set up namespace for XML parsing\n",
    "        namespaces = {'ns': 'http://uniprot.org/uniprot'}\n",
    "        root = ET.fromstring(response.content)\n",
    "        \n",
    "        # Find all GO term references in the XML\n",
    "        for db_ref in root.findall(\".//ns:dbReference[@type='GO']\", namespaces):\n",
    "            go_id = db_ref.attrib.get('id')\n",
    "            term = db_ref.find(\"ns:property[@type='term']\", namespaces)\n",
    "\n",
    "            go_term = term.get('value')\n",
    "            \n",
    "            if go_id and term is not None:\n",
    "                # Store in original format\n",
    "                term_value = term.attrib['value']\n",
    "                \n",
    "                # Categorize based on prefix\n",
    "                if term_value.startswith('F:'):\n",
    "                    categorized_terms['molecular_function'].append({\n",
    "                        'id': go_id,\n",
    "                        'term': term_value[2:]  # Remove 'F:' prefix\n",
    "                    })\n",
    "                elif term_value.startswith('P:'):\n",
    "                    categorized_terms['biological_process'].append({\n",
    "                        'id': go_id,\n",
    "                        'term': term_value[2:]  # Remove 'P:' prefix\n",
    "                    })\n",
    "                elif term_value.startswith('C:'):\n",
    "                    categorized_terms['cellular_component'].append({\n",
    "                        'id': go_id,\n",
    "                        'term': term_value[2:]  # Remove 'C:' prefix\n",
    "                    })\n",
    "        \n",
    "        return {\n",
    "            'categorized': categorized_terms  # New categorized format\n",
    "}\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching GO annotations for {protein_id}: {e}\")\n",
    "        return {\n",
    "            'categorized': {\n",
    "                'molecular_function': [],\n",
    "                'biological_process': [],\n",
    "                'cellular_component': []\n",
    "            }\n",
    "        }\n",
    "    '''\n",
    "\n",
    "# STEP 1 \n",
    "\n",
    " # Define the UniProt API URL for XML data\n",
    "def fetch_go_annotations(protein_id):\n",
    "    \"\"\"\n",
    "    Fetch GO annotations and create GO ID to protein list mapping.\n",
    "    \n",
    "    Args:\n",
    "        protein_ids (list): List of UniProt protein IDs\n",
    "        \n",
    "    Returns:\n",
    "        List : List of the GO ids found for that protein\n",
    "    \"\"\"\n",
    "    go_ids = []\n",
    "    \n",
    "\n",
    "    url = f\"https://rest.uniprot.org/uniprotkb/{protein_id}.xml\"\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        namespaces = {'ns': 'http://uniprot.org/uniprot'}\n",
    "        root = ET.fromstring(response.content)\n",
    "        \n",
    "        for db_ref in root.findall(\".//ns:dbReference[@type='GO']\", namespaces):\n",
    "            go_id = db_ref.attrib.get('id')\n",
    "            \n",
    "            if go_id:\n",
    "                go_ids.append(go_id)\n",
    "                \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching GO annotations for {protein_id}: {e}\")\n",
    "   \n",
    "            \n",
    "    return go_ids\n",
    "\n",
    "\n",
    "\n",
    "def fetch_go_terms(protein_ids):\n",
    "\n",
    "    go_terms = {}\n",
    "\n",
    "    for protein_id in protein_ids:\n",
    "        url = f\"https://rest.uniprot.org/uniprotkb/{protein_id}.xml\"\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            namespaces = {'ns': 'http://uniprot.org/uniprot'}\n",
    "            root = ET.fromstring(response.content)\n",
    "            \n",
    "            for db_ref in root.findall(\".//ns:dbReference[@type='GO']\", namespaces):\n",
    "                go_id = db_ref.attrib.get('id')\n",
    "                term = db_ref.find(\"ns:property[@type='term']\", namespaces)\n",
    "                if go_id and term is not None:\n",
    "                    go_term = term.get('value')\n",
    "                    go_terms[go_id] = go_term\n",
    "                    \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error fetching GO terms for {protein_id}: {e}\")\n",
    "                \n",
    "    return go_terms\n",
    "\n",
    "# Let's add some debugging to help understand what's happening\n",
    "# here we see that the big .xml file has the same structure as the small ones \n",
    "# we already analyzed ; thus,we can use the same parsing structure, but this time directly\n",
    "# just collect the counts of GO terms, because that is all we need (no diff. categories, would just make our code slower)\n",
    "def print_swissprot_file(swissprot_xml_path, length = 50):\n",
    "    \"\"\"\n",
    "    Just to look at the first few lines to see the structure\n",
    "    \"\"\"\n",
    "\n",
    "    with open(swissprot_xml_path, 'r') as f:\n",
    "        print(\"First length lines of the file:\")\n",
    "        for i, line in enumerate(f):\n",
    "            if i < length:\n",
    "                print(line.strip())\n",
    "            else:\n",
    "                break\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "def parse_swissprot_go_terms(swissprot_xml_path, family_proteins, skip_proteins):\n",
    "    \"\"\"\n",
    "    Parse GO terms from SwissProt XML file, excluding proteins from our family.\n",
    "    \n",
    "    Args:\n",
    "        swissprot_xml_path (str): Path to the SwissProt XML file\n",
    "        family_proteins (set): Set of UniProt IDs in our protein family\n",
    "        skip_proteins (bool): Whether to skip proteins in our family\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (go_term_counts dictionary, total proteins processed)\n",
    "    \"\"\"\n",
    "    # Initialize counters\n",
    "    go_term_counts = defaultdict(int)\n",
    "    total_proteins = 0\n",
    "    skipped_proteins = 0\n",
    "    \n",
    "    # Set up namespace for XML parsing\n",
    "    namespaces = {'ns': 'http://uniprot.org/uniprot'}\n",
    "    \n",
    "    # Use iterparse for memory-efficient parsing\n",
    "    context = ET.iterparse(swissprot_xml_path, events=('end',))\n",
    "    \n",
    "    print(\"Starting to parse SwissProt XML...\")\n",
    "    \n",
    "    for event, elem in context:\n",
    "        if elem.tag.endswith('entry'):\n",
    "            # Get the UniProt ID for this protein\n",
    "            accession = elem.find(\".//ns:accession\", namespaces)\n",
    "            if accession is not None:\n",
    "                uniprot_id = accession.text\n",
    "                \n",
    "                # Skip if this protein is in our family (we need this for the enrichment task to create the contigency table later on)\n",
    "    \n",
    "                if uniprot_id in family_proteins and skip_proteins:\n",
    "                        skipped_proteins += 1\n",
    "                else:\n",
    "                    # Process GO terms for non-family proteins\n",
    "                    for db_ref in elem.findall(\".//ns:dbReference[@type='GO']\", namespaces):\n",
    "                        go_id = db_ref.attrib.get('id')\n",
    "                        if go_id:\n",
    "                            go_term_counts[go_id] += 1\n",
    "                    total_proteins += 1\n",
    "        \n",
    "\n",
    "            \n",
    "            # Clear the element to save memory\n",
    "            elem.clear()\n",
    "            \n",
    "            # Print progress periodically\n",
    "            if (total_proteins + skipped_proteins) % 10000 == 0:\n",
    "                print(f\"Processed {total_proteins} proteins \"\n",
    "                      f\"(skipped {skipped_proteins} family proteins)...\")\n",
    "              #  break\n",
    "        \n",
    "\n",
    "    \n",
    "    return go_term_counts, total_proteins\n",
    "    '''\n",
    "\n",
    "def parse_swissprot_go_terms(swissprot_xml_path, family_proteins):\n",
    "   \"\"\"\n",
    "   Parse GO terms from SwissProt XML file for each protein.\n",
    "   \n",
    "   Args:\n",
    "       swissprot_xml_path (str): Path to SwissProt XML file\n",
    "       family_proteins (set): UniProt IDs in protein family\n",
    "   \n",
    "   Returns:\n",
    "       dict: protein ID -> list of GO IDs for that protein\n",
    "   \"\"\"\n",
    "   protein_to_go = defaultdict(list)\n",
    "   total_proteins = 0\n",
    "   skipped_proteins = 0\n",
    "   \n",
    "   namespaces = {'ns': 'http://uniprot.org/uniprot'}\n",
    "   context = ET.iterparse(swissprot_xml_path, events=('end',))\n",
    "   \n",
    "   print(\"Starting to parse SwissProt XML...\")\n",
    "   \n",
    "   for event, elem in context:\n",
    "       if elem.tag.endswith('entry'):\n",
    "           accession = elem.find(\".//ns:accession\", namespaces)\n",
    "           if accession is not None:\n",
    "               uniprot_id = accession.text\n",
    "               \n",
    "               if uniprot_id in family_proteins:\n",
    "                   skipped_proteins += 1\n",
    "               else:\n",
    "                   for db_ref in elem.findall(\".//ns:dbReference[@type='GO']\", namespaces):\n",
    "                       go_id = db_ref.attrib.get('id')\n",
    "                       if go_id:\n",
    "                           protein_to_go[uniprot_id].append(go_id)\n",
    "                   total_proteins += 1\n",
    "\n",
    "           elem.clear()\n",
    "           \n",
    "           if (total_proteins + skipped_proteins) % 10000 == 0:\n",
    "               print(f\"Processed {total_proteins} proteins \"\n",
    "                     f\"(skipped {skipped_proteins} family proteins)...\")\n",
    "               \n",
    "                    \n",
    "               \n",
    "   return protein_to_go\n",
    "\n",
    "def calculate_go_enrichment(go_to_proteins_family, go_to_proteins_swissprot, total_proteins_family, total_proteins_swissprot, go_id_to_go_term):\n",
    "    results = []\n",
    "    \n",
    "    \n",
    "    for go_id in go_to_proteins_family.keys():\n",
    "   \n",
    "        # Create the 2x2 contingency table for Fisher's exact test\n",
    "        # The table looks like this:\n",
    "        #                   Protein in family    Protein not in family (i.e. all in SwissProt - family proteins)\n",
    "        # Has GO term            a                    b\n",
    "        # No GO term             c                    d\n",
    "        \n",
    "        # Contingency table calculations:\n",
    "        a = len(go_to_proteins_family[go_id])  # Proteins with this GO term in family\n",
    "        \n",
    "        # For b, we need to make sure we don't subtract more than what's in SwissProt\n",
    "        b = len(go_to_proteins_swissprot.get(go_id, []))  # Proteins with GO term in rest of SwissProt \n",
    "        \n",
    "        c = total_proteins_family - a  # Proteins without GO term in family\n",
    "        \n",
    "        # For d, ensure we don't get negative values by using max\n",
    "        d = total_proteins_swissprot - b\n",
    "        \n",
    "        # Verify all values are non-negative before creating contingency table\n",
    "        if all(x >= 0 for x in [a, b, c, d]):\n",
    "            contingency_table = [[a, b], [c, d]]\n",
    "            \n",
    "            # Perform Fisher's exact test\n",
    "            # We ask : is the GO term appearing more often in our family than we would expect by random chance ?\n",
    "            # The null hypothesis (H0) is: \"The proportion of proteins with this GO term in our family \n",
    "            # is the same as the proportion in the SwissProt dataset (without the protein in the family).\" \n",
    "            # In other words, under H0, getting the GO term is independent of being in our family (so it doesn't represent the family)\n",
    "            # Alternative Hypothesis (H1) depends on what tail to use \n",
    "            #Right-tail (greater): Our family has a higher proportion of this GO term than SwissProt\n",
    "            #Left-tail (less): Our family has a lower proportion of this GO term than SwissProt\n",
    "            #Two-tail (two-sided): The proportion is different (either higher or lower)\n",
    "            #Fisher's exact test calculates the probability of seeing our observed data (or more extreme) under the null hypothesis.\n",
    "            #A very small p-value (like < 0.05) tells us:\n",
    "            #Two-tail: This GO term's frequency is significantly different from SwissProt\n",
    "            #Right-tail: This GO term is significantly enriched in our family(overrepresented)\n",
    "            #Left-tail: This GO term is significantly depleted in our family(underrepresented)\n",
    "\n",
    "            odds_ratio, pvalue_two_tail = fisher_exact(contingency_table, alternative='two-sided')\n",
    "            # TODO : including both the p-values for now, we have to understand when to use what (like asked in the task), \n",
    "            # TODO : i.e. how we ordered the confusion matrix (contingency table)\n",
    "            _, pvalue_greater = fisher_exact(contingency_table, alternative='greater')\n",
    "          #  _, pvalue_less = fisher_exact(contingency_table, alternative='less')\n",
    "            \n",
    "            # Calculate fold enrichment safely\n",
    "            my_proportion = a / total_proteins_family \n",
    "            swissprot_proportion = (a+b) / (total_proteins_swissprot + total_proteins_family)\n",
    "     \n",
    "            # Not needed anymore when we do a+b for the swissprot proportion : So we calculate contingency table and Fishers Test\n",
    "            # on the right values now, but the proportion on ALL swissprot, so also the ones that are in family TODO : ask prof if correct\n",
    "            '''\n",
    "            # Fold Enrichment\n",
    "            # TODO : see if the argumentation in the next comment makes sense (send email to prof)\n",
    "            if b == 0: # When the swissprot count is 0, it means that : \n",
    "                                     # When collecting the GO terms of SwissProt, we skipped over the proteins in our family\n",
    "                                     # Thus, if no protein in SwissProt has this GO term, ONLY the protein in the family itself \n",
    "                                     # has that GO term (compared to ALL of SwissProt), thus in the WordCloud later on\n",
    "                                     # we want to especially show the term of this GO id and will thus give it\n",
    "                                     # 'inf' amount (infinite) for now\n",
    "                if my_proportion > 0:\n",
    "                    fold_enrichment = float('inf')\n",
    "                else:\n",
    "                    fold_enrichment = 0\n",
    "            else:\n",
    "                fold_enrichment = my_proportion/swissprot_proportion\n",
    "            '''\n",
    "       \n",
    "     \n",
    "            \n",
    "            results.append({\n",
    "                'GO_ID': go_id,\n",
    "                'GO_Term': go_id_to_go_term.get(go_id, 'N/A'),\n",
    "                'Count_Prot_Dataset': a,\n",
    "                'Count_Prot_SwissProt': b,\n",
    "                'Count_Prot_SwissProt_Actual': a+b,\n",
    "                'Percentage_Dataset': round(my_proportion * 100, 2),\n",
    "                'Percentage_SwissProt': round(swissprot_proportion * 100, 10),\n",
    "                'Fold_Enrichment': round(my_proportion/swissprot_proportion,2),\n",
    "                'P_Value_Two_Tail': pvalue_two_tail,\n",
    "                'P_Value_Greater': pvalue_greater,\n",
    "            })\n",
    "    \n",
    "    # Convert to DataFrame and sort by p-value\n",
    "    df_results = pd.DataFrame(results)\n",
    "    if not df_results.empty:\n",
    "        df_results = df_results.sort_values('P_Value_Two_Tail')\n",
    "\n",
    "    df_results.to_csv(\"enrichment_results.csv\")\n",
    "    \n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hierarchical Structure\n",
    "def analyze_go_hierarchy():\n",
    "    # First, we downloaded the go.obo file so we can parse it \n",
    "    go_obo = obo_parser.GODag('go.obo')\n",
    "    \n",
    "    # Read our enrichment results\n",
    "    df = pd.read_csv(\"enrichment_results.csv\")\n",
    "    \n",
    "    # Filter for significantly enriched terms\n",
    "    enriched_terms = df[\n",
    "        (df['P_Value_Two_Tail'] < 0.05) &\n",
    "        (df['P_Value_Greater'] < 0.05)\n",
    "    ]\n",
    "    \n",
    "    # Create a dictionary to store branch information\n",
    "    branch_info = {}\n",
    "    \n",
    "    # For each enriched term, traverse up its ancestry\n",
    "    for _, row in enriched_terms.iterrows():\n",
    "        go_id = row['GO_ID']\n",
    "        if go_id in go_obo:\n",
    "            term = go_obo[go_id]\n",
    "            \n",
    "            # Get all ancestors (parents) up to the root of the DAG (since we use get_all_parents we do that here! get_parents would just get the direct parents)\n",
    "            ancestors = term.get_all_parents()\n",
    "            \n",
    "            # Add information about this term to all its ancestor branches\n",
    "            for ancestor_id in ancestors:\n",
    "                if ancestor_id not in branch_info:\n",
    "                    branch_info[ancestor_id] = {\n",
    "                        'term_name': go_obo[ancestor_id].name,\n",
    "                        'enriched_children': [],\n",
    "                        'total_significance': 0,\n",
    "                        'depth': go_obo[ancestor_id].depth,\n",
    "                    }\n",
    "\n",
    "                # TODO : correct ????\n",
    "                # Our go_id is a child to the current ancestors (note that this is not necessarily a direct child, but maybe also much more down in the tree somewhere)\n",
    "                branch_info[ancestor_id]['enriched_children'].append({\n",
    "                    'id': go_id,\n",
    "                    'name': term.name,\n",
    "                    'p_value': row['P_Value_Two_Tail']\n",
    "                })\n",
    "                # Measure significance based on -log value of the p value of all the childs of the ancestor (lower p values have higher -log scores!)\n",
    "                branch_info[ancestor_id]['total_significance'] += -np.log10(row['P_Value_Two_Tail'])\n",
    "    \n",
    "    # Filter for high-level terms (lower depth) with multiple enriched children\n",
    "    significant_branches = {\n",
    "        go_id: info for go_id, info in branch_info.items() # take each key,value of the branch_info dictionary\n",
    "        if len(info['enriched_children']) >= 2  # At least 2 enriched children\n",
    "        and info['depth'] <= 3  # High-level term (adjust this threshold as needed)\n",
    "    }\n",
    "    \n",
    "    # Sort branches by their total significance\n",
    "    sorted_branches = sorted(\n",
    "        significant_branches.items(),\n",
    "        key=lambda x: x[1]['total_significance'],\n",
    "        reverse=True\n",
    "    )\n",
    "    \n",
    "    # Create a list to store the branch information\n",
    "    branch_data = []\n",
    "\n",
    "    # Convert the branch information into a format suitable for a DataFrame\n",
    "    for go_id, info in sorted_branches[:20]:  # Top 20 branches\n",
    "        branch_data.append({\n",
    "            'GO_ID': go_id,\n",
    "            'Branch_Name': info['term_name'],\n",
    "            'Hierarchy_Depth': info['depth'],\n",
    "            'Number_Enriched_Terms': len(info['enriched_children']),\n",
    "            'Total_Significance_Score': info['total_significance']\n",
    "        })\n",
    "\n",
    "    # Create a DataFrame and save to CSV\n",
    "    branches_df = pd.DataFrame(branch_data)\n",
    "    branches_df.to_csv('enriched_branches.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    psiblast_file = \"psiblast_parsed.csv\"\n",
    "    hmm_file = \"hmmsearch_output.csv\"\n",
    "    protein_ids = load_protein_ids(psiblast_file, hmm_file)\n",
    "    \n",
    "\n",
    "    \n",
    "    # Proteins_to_GO terms for our family \n",
    "    print(\"Fetching GO annotations...\")\n",
    "    family_annotations = {}\n",
    "    for pid in tqdm(protein_ids, desc=\"Fetching GO annotations\"):\n",
    "        family_annotations[pid] = fetch_go_annotations(pid)\n",
    "\n",
    "    total_proteins_family = len(family_annotations)\n",
    "\n",
    "    go_id_to_go_term = fetch_go_terms(protein_ids)\n",
    "\n",
    "    \n",
    "    # Proteins_to_GO terms for SwissProt\n",
    "    swissprot_annotations = parse_swissprot_go_terms(\"uniprot_sprot.xml\", protein_ids) #go_counts_swissprot, num_proteins_swissprot\n",
    "    \n",
    "    total_proteins_swissprot = len(swissprot_annotations)\n",
    "\n",
    "    # Now Map the GO terms to the proteins ; for the enrichment task, we need to know how many proteins have a certain GO term\n",
    "    go_to_proteins_swissprot = reverse_protein_go_dict(swissprot_annotations)\n",
    "    go_to_proteins_family = reverse_protein_go_dict(family_annotations)\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    # Calculate GO enrichments for both with skipped proteins and without \n",
    "    _ = calculate_go_enrichment(go_to_proteins_family, go_to_proteins_swissprot, total_proteins_family, total_proteins_swissprot, go_id_to_go_term)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Read the enrichment results\n",
    "    df = pd.read_csv(\"enrichment_results.csv\")\n",
    "\n",
    "    # Get the terms to the GO ids from the family data\n",
    "  #  go_id_to_term = create_go_id_to_term_mapping(family_annotations)\n",
    "\n",
    "    # Filter for significantly enriched terms\n",
    "    enriched_terms = df[\n",
    "    (df['P_Value_Two_Tail'] < 0.05) &\n",
    "    (df['P_Value_Greater'] < 0.05)\n",
    "    ]\n",
    "\n",
    "\n",
    "    # Create word frequencies using the actual GO terms instead of IDs\n",
    "    word_frequencies = {}\n",
    "    for _, row in enriched_terms.iterrows():\n",
    "        go_id = row['GO_ID']\n",
    "        if go_id in go_id_to_go_term:  # Make sure we have the term for this ID\n",
    "            term = go_id_to_go_term[go_id]\n",
    "            # Use fold enrichment as weight\n",
    "            weight = row['Fold_Enrichment']\n",
    "            word_frequencies[term] = weight\n",
    "\n",
    "    # Create and display the word cloud\n",
    "    wordcloud = WordCloud(\n",
    "        width=1200, \n",
    "        height=800,\n",
    "        background_color='white',\n",
    "        prefer_horizontal=0.7,\n",
    "        max_words=50,  # Limit to top 50 terms for better readability\n",
    "        min_font_size=10,\n",
    "        max_font_size=60\n",
    "    ).generate_from_frequencies(word_frequencies)\n",
    "\n",
    "    # Plot and save the word cloud\n",
    "    plt.figure(figsize=(20, 12))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title('GO Term Enrichment Word Cloud', fontsize=16, pad=20)\n",
    "    plt.savefig('go_enrichment_wordcloud.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    # Print out the enriched terms for verification\n",
    "    print(\"\\nTop enriched GO terms:\")\n",
    "    sorted_terms = sorted(word_frequencies.items(), key=lambda x: x[1], reverse=True)\n",
    "    for term, weight in sorted_terms[:10]:\n",
    "        print(f\"\\nTerm: {term}\")\n",
    "        print(f\"Weight in word cloud: {weight:.2f}\")\n",
    "\n",
    "    \n",
    "    # Hierarchy\n",
    "\n",
    "    analyze_go_hierarchy()\n",
    "\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motifs\n",
    "1. Search significantly conserved short motifs inside your family. Use ELM classes and ProSite patterns (for ProSite consider only patterns “PA” lines, not the profiles). Make sure to consider as true matches only those that are found inside disordered regions. Disordered regions for the entire SwissProt (as defined by MobiDB-lite) are available here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
